<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Bool_tbb" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="序言自 Transformer 模型的文章《Attention is all you need》这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕">
<meta property="og:type" content="article">
<meta property="og:title" content="Bertology:说一说那些不为人知的细节">
<meta property="og:url" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/index.html">
<meta property="og:site_name" content="Bool_tbb">
<meta property="og:description" content="序言自 Transformer 模型的文章《Attention is all you need》这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig1.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig2.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig3.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig4.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig5.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig6.jpg">
<meta property="article:published_time" content="2022-05-01T08:16:22.000Z">
<meta property="article:modified_time" content="2022-05-08T09:31:00.301Z">
<meta property="article:author" content="bool_tbb">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/fig1.jpg">

<link rel="canonical" href="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Bertology:说一说那些不为人知的细节 | Bool_tbb</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body itemscope itemtype="http://schema.org/WebPage">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bool_tbb</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">一枚NLPer小菜鸡</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Bertology:说一说那些不为人知的细节
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-01 16:16:22" itemprop="dateCreated datePublished" datetime="2022-05-01T16:16:22+08:00">2022-05-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-08 17:31:00" itemprop="dateModified" datetime="2022-05-08T17:31:00+08:00">2022-05-08</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>自 Transformer 模型的文章<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">《Attention is all you need》</a>这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕获能力，基于Transformer系列的backbone预训练模型在下游任务微调成一种常用的方法。</p>
<p>虽然每年有大量的前沿研究基于Bert这样的模型，但是很多文章—-表面是对模型进行了创新获得了不错的效果，究其本质，无非是扩大了模型的参数量，提高训练数据的质量和数量。因此，取得了更为理想的效果。关于为什么模型的效果好，以及每一层的神经网络到底是取得了如何的效果则没有系统的阐释。今天读到了一篇<a href="https://watermark.silverchair.com/tacl_a_00349.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsowggLGBgkqhkiG9w0BBwagggK3MIICswIBADCCAqwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM356qe6dtueuLMY59AgEQgIICfeMIeHCgVGdSvJu8D2XfjC5ElP1yXiIVEdIbGODaHM9mO1WisSSW5owwhoZNils1hOJUETd_3bZvcHmLZBMUwDPl90BJI5W5psceyCep9D5F3qW-dy31dxzM2bWVKBUjHL7WE7vcbad4Cvrzq_AlR36jArY0mhSc5ekJJRdbTNMHtskqingzrFuLz8eR1yUpWpq8kDtceTZV1zBJCM_NVinFFwwt1eh9cn7n8PVEm4em5fj912Oo77yMTPnr_6nPORwVjl9ZuboiKWTzV0XJz3TO-PxXcUD_mg-6DdbsS9hXibOZ3oz9bVpXVGKW6XYFOs7BzHT6A6M9lDDKDtAp2R9iBqajMyhukiB1bB6VA0ePRke64wdJ17CF9xibbH8FqlDlaYhieRLmc7DTWbIFA15hsM7Aq1iwB9mYQ49eur0aCzZSZJWRrxugqSs3MuNSnjzJvClzEQwKRBToWUFfQ0FlxG0vpOfswjZz2PnDBDhOZD6VvVF0Gs1I19RJsoaMAxfI5lMNk3tSsvxcsFbRms7EOTXG6O0qlkCsMF4GfaGp6wQq9rsAmgQsGHu1wecwwRrMveNbF7dd-0crKukSCUwZOCXM0_qn5BiOOflI4LXAjJZ6T5A1kseghSFTFHLNM1no4Vxa6tdLY-njxhK50ib5Cytrunc58Kc1qwQbahB3x4UVlKm8ww14pFjnLi2e1nJJi-_YWdu1JNuI6RbhsM_GS1iSxu6KmsH49DTy0urourShCVC-23Y6KM91pry926KPk2r_gGzmoIwC0FSGZ-FCIlMzUING1XRyVlPR9P414Ifv1_hSJKldyzljyMyte-HJhKunk9uuuWOa6f8" target="_blank" rel="noopener">《A Primer in BERTology: What We Know About How BERTWorks》</a>发表于Transactions of the Association for Computational Linguistics。这篇文章对于bert模型的一些细节进行了系统的论述，很全面，因此，读了一下，把它分享给大家。</p>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>众所周知，BERT已经在nlp领域大杀四方，但是对于“why it works well”相对来说没有那么多的研究。如此一来，想要更进一步做出更加solid的研究，需要对该模型有更多的了解。 相比较一些经典的backbone，比如说卷积神经网络(CNN)，在感性上的动机则没有特别明显。我们希望对其中的一些结构进行分析，使得模型中具体所做的事情更容易被人们所认识，但是，对于模型过于大的尺寸所造成的预训练不好做（大部分研究人员没有超多的显卡，以及预训练所耗费的经费十分多）且想要做消融实验并不容易。因此对于近些年来对解释bert的文章做一个综述，方便大家对于该模型的认识。</p>
<a id="more"></a>
<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>这篇文章主要介绍了一些已经被人们所理解的模型其中的部分结构，并且也同样强调了一些仍旧需要探索的地方。</p>
<p>首先，将要介绍一些对模型“linguistic”上的探索。比如说：目前关于 BERT 学习的语言和世界知识类型的证据，以及这些知识可能存储在模型中的位置和方式。<br>之后将介绍模型的技术方面，并提供当前方案的概述，改进 BERT 的架构、预训练和微调。<br>最后，讨论了过度参数化的问题、压缩 BERT 的方法以及作为模型分析技术的修剪的新生领域。</p>
<h2 id="BERT-模型概述"><a href="#BERT-模型概述" class="headerlink" title="BERT 模型概述"></a>BERT 模型概述</h2><p>先简要说一下BERT模型的基本结构，BERT是由相同的Transformer的编码器堆叠而成，其中编码器中最重要的成分是“多头注意力机制”，对于每一个“头”来说，它由三个矩阵进行矩阵乘法完成对于注意力的计算，这三个矩阵分别称为，“key”矩阵，“query”矩阵，“value”矩阵，对于输入文本中的每一个词（这里说词是指一个文本的在输入端所能分解成的最小组成单元，对于英文来说可以是一个词组，单词，或者亚词，或者是字母）通过三个矩阵的线性变换求得每个词的表示，然后对于每一个头的输出进行组合，最后通过全连接层得到最终的文本表示，当然在输入端到多头注意力端有一个残差连接结构。并且最后的输出是经过归一化处理操作所得到(按照一定的均值和方差得到的分布)。</p>
<p>通常情况下，BERT的工作流程分为以下两个阶段：预训练阶段和微调阶段，在预训练阶段，运用MLM（Masked Language Model）任务和NSP(Next sentence prediction)任务,在微调阶段，会在BERT输出的基础上添加一层或多层全连接神经网络。</p>
<p>上文提到，对于模型的输入表示来说，可以分解成词组，单词，亚词，或者是字母，BERT模型使用了亚词的表示方式，通过wordpiece算法将一个文本中的每个英文单词切分成亚词的大小，通过这样的方法既可以保留更多的语意信息，相比较切分成单词来说，可以缩小此表的大小。同时，还需要增加另外两个表示层，分别是表示单词在一句话中位置关系的postion编码，另外一个段编码，表示的是一种句子级别的相对位置关系。这最终的输入表示为这三种表示方式相加。另外在一段文本开始时在word编码层加入一个特殊的字段[CLS]位。在两句话间隔处加入[SEP]字段。</p>
<p>谷歌和抱抱脸提供了多种版本的BERT源代码（看源码是搞清模型细节的最有效方式），其中包括了“base”和“large”版本的模型（区别在于模型的大小）。</p>
<h2 id="BERT-到底学了个啥？"><a href="#BERT-到底学了个啥？" class="headerlink" title="BERT 到底学了个啥？"></a>BERT 到底学了个啥？</h2><p>介绍完一些基础的概念后，下面要进入正题了，BERT中的网络到底都学了什么知识呢？</p>
<p>许多研究着眼于编码在BERT权重中的知识。流行的方法包括 MLM 的填充式探测、自我注意权重分析以及使用不同BERT表示作为输入的探测分类器。</p>
<h3 id="句法知识"><a href="#句法知识" class="headerlink" title="句法知识"></a>句法知识</h3><p>BERT的表示实际上是分层的而不是线性的<a href="https://arxiv.org/pdf/1906.01698.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(lin et al(2019))</a>，举一个例子，除了词序信息以外，还有一些句法树结构的东西，<br><a href="https://arxiv.org/pdf/1905.06316" target="_blank" rel="noopener">Tenney et al. (2019b)</a> and <a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">Liu et al. (2019a)</a>，BERT 嵌入对有关词性、句法块和角色的信息进行编码。 （<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6446/6302" target="_blank" rel="noopener">Vilares et al., 2020;</a> <a href="https://arxiv.org/pdf/2002.00737" target="_blank" rel="noopener">Kim et al., 2020;</a> <a href="https://arxiv.org/pdf/1906.11511" target="_blank" rel="noopener">Rosa and Mareˇcek, 2019</a>)这篇文章表明，有足够的句法知识似乎在词嵌入的部分被表示从而恢复句法树。尽管探测分类器无法恢复句法树中远距离父节点的标签。<br><a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">(Liu et al., 2019a)</a>. Warstadt and Bowman (2020)表明在四分之三的探测任务中存在层次结构的证据。</p>
<p>至于句法是如何表示的，句法结构似乎没有直接编码在自注意力权重中。<a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">Htut et al. (2019)</a> 表明即使有根的标注，也无法从 BERT 头中提取完整的解析树。<a href="https://hal.inria.fr/hal-02131630/document" target="_blank" rel="noopener">Jawahar et al. (2019)</a>直接从自注意力权重中提取的依赖树的简要说明，但不提供定量评估的方法。</p>
<p>然而，句法信息可以从BERT的token 表示中恢复。<a href="https://aclanthology.org/N19-1419/" target="_blank" rel="noopener">Hewitt and Manning (2019)</a> 的工作可以从bert的token嵌入中学习到一个转换矩阵，使得他们在PennTreebank data 上恢复句法依存关系<a href="https://www.pnas.org/content/pnas/117/48/30046.full.pdf" target="_blank" rel="noopener">(Manning et al., 2020)</a>。<a href="https://hal.inria.fr/hal-02131630/document" target="_blank" rel="noopener">Jawahar et al. (2019)</a>使用张量积分解网络（Tensor Product Decomposition Networks<a href="https://arxiv.org/pdf/1812.08718" target="_blank" rel="noopener">(McCoy et al., 2019a)</a>）对 [CLS] token进行转换实验，得出的结论是依赖树是 5 种分解方案中的最佳匹配（尽管报告的 MSE 差异非常小）。<a href="https://aclanthology.org/2020.repl4nlp-1.15/" target="_blank" rel="noopener">Miaschi and Dell’Orletta (2020)</a>使用连接的标记表示作为输入执行一系列句法探测实验。</p>
<p>请注意，所有这些方法都在寻找标准语言结构的证据，并为探索添加一些额外的知识。<a href="https://arxiv.org/pdf/2004.14786" target="_blank" rel="noopener">Wu et al. (2020)</a> 在 MLM 任务中，基于测量一个词对预测序列中另一个词的影响，提出了一种无参数方法。如图所示:</p>
<p><img src="fig1.jpg" alt="avtar"></p>
<center>句法知识的无参数探测：共享句法子树的词在MLM预测中相互影响较大(Wu et al., 2020)</center>

<p>他们得出的结论是，BERT“自然地”学习了一些句法信息，尽管它与语言注释资源不太相似。</p>
<p>MLM 的填充式探测表明，BERT 在执行完形填空任务时会考虑主谓一致(<a href="https://arxiv.org/pdf/1901.05287" target="_blank" rel="noopener">Goldberg, 2019;</a> <a href="https://arxiv.org/pdf/1909.00111" target="_blank" rel="noopener">van Schijndel et al., 2019</a>),即使对于无意义的句子和在主语和动词之间有干扰从句的句子<a href="https://arxiv.org/pdf/1901.05287" target="_blank" rel="noopener">(Goldberg, 2019)</a>. <a href="https://arxiv.org/pdf/1909.02597.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Warstadt et al. (2019)</a>对负极性项目 (NPI) 的研究表明，与范围违规相比，BERT 能够更好地检测 NPI 的存在（例如“ever”）和允许使用它们的单词（例如“whether”）。</p>
<p>上述关于句法知识的主张被 BERT 不“理解”否定并且对格式错误的输入不敏感的证据所掩盖。特别是，即使是打乱词序、截断句子、删除主语和宾语，它的预测也没有改变。后者似乎更有可能，因为 <a href="https://arxiv.org/pdf/2008.06788" target="_blank" rel="noopener">Glavaš 和 Vuli´c (2020)</a> 报告说，带有监督解析的中间微调步骤对下游任务的性能没有太大影响。</p>
<h3 id="语义知识"><a href="#语义知识" class="headerlink" title="语义知识"></a>语义知识</h3><p>迄今为止，更多的研究致力于 BERT 的句法知识而非语义现象。但是，我们确实有来自 MLM 探索性研究的证据表明 BERT 对语义角色有一些了解<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf" target="_blank" rel="noopener">(Ettinger, 2019)</a>.BERT 甚至对与正确角色语义相关的语义角色的错误填充物表现出一些偏好，而不是那些不相关的语义角色（例如，给厨师小费比给知更鸟小费更好，但比给服务员小费更糟糕）。</p>
<p><a href="https://arxiv.org/pdf/1905.06316" target="_blank" rel="noopener">Tenney et al. (2019b)</a> 表明，BERT 对有关实体类型、关系、语义角色和原型角色的信息进行编码，因为可以使用探测分类器检测到这些信息。<br><a href="https://arxiv.org/pdf/1909.07940.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Wallace et al., 2019b)</a>表明BERT对于数字信息表示十分吃力。加法和数字解码任务表明，BERT 不能很好地表示浮点数，并且无法从训练数据中泛化。问题的一部分是 BERT 的 wordpiece 标记化，因为相似值的数量可以划分为截然不同的词块。</p>
<p>开箱即用的BERT模型对于命名实体的替换是十分敏感的，例如，在共同引用任务中替换名称会改变85%的预测<a href="https://arxiv.org/pdf/2007.06897" target="_blank" rel="noopener">(Balasubramanian et al., 2020)</a>。 这表明该模型实际上并没有形成命名实体的一般概念，尽管它在NER探测任务中的F1分数很高。<a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Tenney et al., 2019a)</a>. <a href="https://arxiv.org/pdf/2003.05473" target="_blank" rel="noopener">Broscheit (2019)</a> 发现在维基百科实体链接上微调 BERT “教”它额外的实体知识，这表明它在维基百科的预训练期间没有吸收所有相关的实体信息。</p>
<h3 id="世界知识"><a href="#世界知识" class="headerlink" title="世界知识"></a>世界知识</h3><p>BERT 中捕获的有关常识知识的大部分证据来自使用它来提取此类知识的从业者。一项对 BERT 的直接探索性研究报告称，BERT 在语用推理和基于角色的事件知识方面存在困难<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf" target="_blank" rel="noopener">(Ettinger, 2019)</a>。<br>BERT 还挣扎在对象的抽象属性以及可能被假设而不是提及的视觉和感知属性这些问题上<a href="https://arxiv.org/pdf/1910.01157" target="_blank" rel="noopener">(Da and Kasai, 2019)</a>。</p>
<p>BERT 的 MLM 组件通过填空很容易适应知识归纳（例如“猫喜欢追逐 [<em>_</em>]”）。<a href="https://arxiv.org/pdf/1909.01066.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Petroni et al. (2019)</a> 表明，对于某些关系类型，普通 BERT 与依赖知识库的方法相比具有竞争力。如下图所示：</p>
<p><img src="fig2.jpg" alt="avatar"></p>
<center>Bert的世界知识</center>

<p><a href="https://arxiv.org/pdf/2002.08910" target="_blank" rel="noopener">Roberts et al. (2020)</a>对使用 T5 模型的开放域 QA 显示乐相似的效果。<br><a href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" target="_blank" rel="noopener">(Raffel et al., 2019)</a>. <a href="https://aclanthology.org/D19-1109/" target="_blank" rel="noopener">Davison et al. (2019)</a>表明它可以更好地推广到看不见的数据。为了检索 BERT 的知识，我们需要好的模板句子，并且有一些有关于它们的自动提取和扩充的工作(<a href="https://ojs.aaai.org/index.php/AAAI/article/download/6242/6098" target="_blank" rel="noopener">Bouraoui et al., 2019;</a> <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460" target="_blank" rel="noopener">Jiang et al., 2019b</a>)。</p>
<p>但是，BERT 无法根据其世界知识进行推理。<a href="https://arxiv.org/pdf/1908.02899.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Forbes et al. (2019)</a>表明 BERT 可以“猜测”许多对象的可供性和属性，但不能推理属性和可供性之间的关系。例如，它“知道”人可以走进房子，房子很大，但不能推断出房子比人大。</p>
<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6523/6379" target="_blank" rel="noopener">Zhou et al. (2020)</a>and <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00331/96481" target="_blank" rel="noopener">Richardson and Sabharwal (2019)</a>还表明性能随着必要推理步骤的数量而下降。<a href="https://arxiv.org/abs/1911.03681" target="_blank" rel="noopener">(Poerner et al., 2019)</a>表明 BERT 的一些世界知识成功来自于学习刻板的联想。例如，一个听起来像是意大利名字的人被预测为意大利人，即使它是不正确的。</p>
<h3 id="一些局限性"><a href="#一些局限性" class="headerlink" title="一些局限性"></a>一些局限性</h3><p>第 3 节和第 4 节中的多项探索性研究报告称，BERT 拥有数量惊人的句法、语义和世界知识。然而，<a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Tenney et al. (2019a)</a>评论说，“我们的探测分类器没有观察到语言模式这一事实并不能保证它不存在，并且观察到一个模式并不能告诉我们它是如何使用的。”还有一个问题应该允许探测有多复杂<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">(Liu et al., 2019a)</a>。如果更复杂的探针可以恢复更多信息，那么我们在多大程度上仍然依赖原始模型？<br>此外，不同的探测方法可能会导致互补甚至相互矛盾的结论，这使得单一测试（如大多数研究）是不够的<a href="https://arxiv.org/pdf/1909.02597.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Warstadt et al., 2019)</a>。<br>给定的方法也可能有利于一个模型而不是另一个模型，例如，RoBERTa 用一种树提取方法跟踪 BERT，但用另一种方法领先<a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">(Htut et al., 2019)</a>。<br>语言形式的选择也很重要<a href="https://arxiv.org/pdf/2004.14999" target="_blank" rel="noopener">(Kuznetsov and Gurevych, 2020).</a></p>
<p>鉴于这一切，另一种选择是专注于识别 BERT 在推理时实际依赖的内容。目前在架构块级别都在追求这个方向。在模型权重编码的信息水平上。遗忘探测<a href="https://arxiv.org/abs/2006.00995" target="_blank" rel="noopener">(Elazar et al., 2020)</a> 旨在专门从模型中删除某些信息并查看它如何改变性能，例如发现语言建模确实依赖于词性信息。</p>
<p>另一个方向是信息论探索。<a href="https://arxiv.org/pdf/2004.03061" target="_blank" rel="noopener">Pimentel et al. (2020)</a> 将探测操作化为估计学习表示和给定语言属性之间的互信息，这强调了重点不应该放在表示中包含的信息量上，而应该放在从表示中提取它的难易程度上。<a href="https://arxiv.org/pdf/2003.12298" target="_blank" rel="noopener">Voita and Titov (2020)</a> 将从给定表示中提取信息所需的工作量量化为传达探测大小和完成任务所需的数据量所需的最小描述长度。</p>
<h2 id="局部化语言的知识"><a href="#局部化语言的知识" class="headerlink" title="局部化语言的知识"></a>局部化语言的知识</h2><h3 id="BERT-嵌入"><a href="#BERT-嵌入" class="headerlink" title="BERT 嵌入"></a>BERT 嵌入</h3><p>对于像BERT这样的模型，“嵌入”（embedding）代表了从模型输出的向量，无论是传统的词嵌入例如<a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" target="_blank" rel="noopener">word2vec</a>还是 BERT这样的嵌入方式都可以当作是对词互信息的极大化<a href="https://arxiv.org/pdf/1910.08350" target="_blank" rel="noopener">(Kong et al., 2019)</a>，但是后者的方式更偏向于对当前语境的理解——即每个词的表示向量取决于特定出现的上下文条件，也会包含一些关于上下文的信息<a href="https://aclanthology.org/2020.repl4nlp-1.15/" target="_blank" rel="noopener">(Miaschi and Dell’Orletta, 2020)</a>。</p>
<p>一些研究表明，蒸馏的上下文嵌入可以更好地编码词汇语义信息（比如说它们会在基于词汇的任务上有更好的表现，比如说词汇的相似度）。将上下文化表示提取为静态表示的方法包括跨多个上下文聚合信息（<a href="https://aclanthology.org/N19-1078/?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">Akbik et al., 2019;</a> <a href="https://aclanthology.org/2020.acl-main.431/" target="_blank" rel="noopener">Bommasani et al., 2020）</a>，编码几乎完全依赖给定单词含义的“语义漂白”句子（比如说，”This is &lt;&gt;”）<a href="https://arxiv.org/pdf/1903.10561" target="_blank" rel="noopener">(May et al., 2019)</a>,甚至使用上下文向量化的词嵌入去训练一个静态的词嵌入<a href="https://arxiv.org/pdf/1911.02929" target="_blank" rel="noopener">(Wang et al., 2020d).</a></p>
<p>当然，这其中是有一些可以探讨和提升的地方，比如说<a href="https://arxiv.org/pdf/1909.00512" target="_blank" rel="noopener">Ethayarajh (2019)</a>测量每一层中相同单词的嵌入有多相似，在 BERT 靠输出端的层会产生更多特定于上下文的表示。他们同时也发现BERT 嵌入在向量空间中占据了一个狭窄的锥体，并且这种效果从较早的层到后面的层增加。也就是说，如果嵌入是方向一致的（各向同性的），两个随机词的余弦相似度将比预期的要高得多。由于各向同性被证明有利于静态词嵌入<a href="https://arxiv.org/pdf/1702.01417" target="_blank" rel="noopener">(Mu and Viswanath, 2018)</a>，这可能是探索 BERT 的一个可能出更多成果的方向。</p>
<p>由于 BERT 嵌入是上下文化的，一个有趣的问题是它们在多大程度上捕捉到了多义词和同音词等现象。确实有证据表明 BERT 的上下文嵌入形成了与词义相对应的不同集群(<a href="https://arxiv.org/pdf/1909.10430" target="_blank" rel="noopener">Wiedemann et al., 2019;</a> <a href="https://arxiv.org/pdf/2003.02738" target="_blank" rel="noopener">Schmidt and Hofmann, 2020</a>)，使 BERT 在词义消歧任务中取得成功。然而，<a href="https://arxiv.org/pdf/1911.05758" target="_blank" rel="noopener">Mickus et al. (2019)</a>注意到，同一个词的表示取决于它出现的句子的位置，这可能是由于 NSP 目标函数所造成的（从语言学的角度来看，这是不可取的，并且可能是未来工作的有希望的途径。）。</p>
<p>上述讨论涉及词嵌入，但 BERT 通常用作句子或文本编码器。<br>生成用于分类的句子或文本表示的标准方法是使用 [CLS] 标记，但也正在讨论替代方案，包括连接标记表示<a href="https://link.springer.com/chapter/10.1007/978-981-15-6168-9_13" target="_blank" rel="noopener">（Tanaka et al., 2020）</a>，归一化平均值<a href="https://link.springer.com/chapter/10.1007/978-981-15-6168-9_13" target="_blank" rel="noopener">(Tanaka et al., 2020)</a>和层激活<br><a href="https://arxiv.org/pdf/1910.07973" target="_blank" rel="noopener">(Ma et al., 2019).</a> 参见 <a href="https://arxiv.org/pdf/2006.03866" target="_blank" rel="noopener">Toshniwal et al. (2020)</a>对跨任务和句子编码器的几种方法进行系统比较。</p>
<h3 id="关于自注意力头"><a href="#关于自注意力头" class="headerlink" title="关于自注意力头"></a>关于自注意力头</h3><p>一些研究提出了注意力头类型的分类。<a href="https://helda.helsinki.fi/bitstream/handle/10138/263704/W18_5431_1.pdf?sequence=1" target="_blank" rel="noopener">Raganato and Tiedemann (2018)</a> ，讨论关注词嵌入本身，上一个/下一个<br>token和句子结束位置的token。<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a><br>区分关注上一个/下一个token、[CLS]、[SEP]、标点符号和“广泛关注”序列。<br>如下图所示，<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">Kovaleva et al. (2019)</a>提出了注意力的5种机制。</p>
<p><img src="fig3.jpg" alt="avatar"></p>
<h4 id="具有语言功能的头部"><a href="#具有语言功能的头部" class="headerlink" title="具有语言功能的头部"></a>具有语言功能的头部</h4><p>上中显示的“异构”注意力模式可能在语言上是可解释的，并且许多研究都集中在识别自注意力头的功能上。特别是，一些BERT的头似乎专门应对一种特定类型的句法关系。<a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">Htut et al. (2019)</a> and <a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a> 研究称一些BERT的头相较于随机选择来收会将更多的注意力关注到一个特定的句法位置上。虽然他们在关于这方面的研究上采用了不同的数据集和方法，但是他们都发现有些头对于在obj角色里面的词的关注比位置基线要高。nsubj、advmod 和 amod 的证据因这两项研究而异（这块的obj,nsubj,advmod,amod是句法依存分析中对词汇的分类，有兴趣的同学可以去查阅相关资料）。在<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al. (2019b)</a>的研究结果也支持这样的结论。<a href="https://arxiv.org/pdf/1910.05276.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Hoover et al. (2019)</a>假设即使是像 dobj 这样的复杂依赖项也是由头组合而不是单个头编码的，但这项工作仅限于定性分析。<a href="https://aclanthology.org/2020.acl-main.429/?ref=https://githubhelp.com" target="_blank" rel="noopener">Zhao and Bethard (2020)</a><br>专门寻找头部编码否定范围。</p>
<p><a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>和<a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">Htut et al. (2019)</a>都认为没有一个头具有完整的句法树信息，符合部分句法知识的证据。然而，<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a> 认为识别一个可以直接用作分类器的 BERT 头，以与基于规则的系统一样执行共指解析，这本身似乎需要相当多的句法知识。</p>
<p><a href="https://arxiv.org/pdf/1906.01698.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Lin et al. (2019)</a>目前的证据表明注意力权重是主谓一致和反身照应的弱指标(reflexive anaphora)。BERT 的自注意力权重不是作为应该相关的标记之间的强指针，而是接近统一的注意力基线，但对与心理语言学数据一致的不同类型的干扰物存在一定的敏感性。这与 <a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf" target="_blank" rel="noopener">Ettinger (2019)</a> 的结论一致。</p>
<p>据我们所知，形态信息在 BERT 头中尚未解决，但<a href="https://arxiv.org/pdf/1909.00015" target="_blank" rel="noopener">Correia et al. (2019)</a>在一些基础的Transformer模型种使用稀疏注意力变体。一些注意力的头表现出将使用BPE分解的亚词融合的效果。对于语义关系，<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>称 self-attention head 编码核心帧-语义关系,(Cui et al., 2020)以及词典和常识关系。</p>
<p>自注意力作为一种可解释性机制的整体流行是由于这样一种想法：<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">(Clark et al., 2019)</a>注意力权重有一个明确的含义：在计算当前单词的下一个表示时，特定单词将被加权多少。(<a href="https://arxiv.org/pdf/1902.10186" target="_blank" rel="noopener">Jain and Wallace, 2019;</a> <a href="https://arxiv.org/pdf/1906.03731.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Serrano and Smith, 2019;</a> <a href="https://arxiv.org/pdf/1908.04626.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Wiegreffe and Pinter, 2019;</a> <a href="https://arxiv.org/pdf/1908.04211" target="_blank" rel="noopener">Brunner et al., 2020</a>)对这个问题也对这个问题进行了研究。在一个多层的模型中，往往注意力机制后面会进行非线性变化的激活。个别头捕获的特征并不能提供完整的语义理解。<br>很多文章也做了注意力可视化的工作，并且也有很多可视化的工具(<a href="https://arxiv.org/pdf/1904.02679.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Vig, 2019;</a> <a href="https://arxiv.org/pdf/1910.05276.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Hoover et al., 2019</a>)，可视化通常仅限于定性分析（通常带有精选示例）(<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00254/1923061/tacl_a_00254.pdf" target="_blank" rel="noopener">(Belinkov and Glass, 2019)</a>,并且不应被解释为确凿的证据。</p>
<h4 id="对特殊字段的注意力"><a href="#对特殊字段的注意力" class="headerlink" title="对特殊字段的注意力"></a>对特殊字段的注意力</h4><p><a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">Kovaleva et al. (2019)</a> 表明大多数自注意力头不直接编码任何非平凡的语言信息，至少是在GLUE上进行微调的任务(Wang et al., 2018）有这样的效果,因为只有不到 50% 的注意力头表现出“异构”模式。大部分头产生了垂直的特征（比如说更多注意到[CLS],[SEP],标点符号的token）。<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>得出了相同的结论。这种冗余可能与过度参数化问题有关。</p>
<p><a href="https://arxiv.org/pdf/2004.10102" target="_blank" rel="noopener">Kobayashi et al. (2020)</a> 的研究表明，注意加权输入向量的归一化可以更直观地解释自我注意，减少对特殊标记的注意。然而，即使注意力权重被规范化，大多数的头部仍然不是潜在可解释的<a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">(Prasanna et al., 2020)</a>。在许多注意力研究中，一种方法选择是关注词间注意力并简单地排除特殊标记，例如像. <a href="https://arxiv.org/pdf/1906.01698.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Lin et al. (2019)</a> and <a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">Htut et al. (2019)</a>的研究。然而，如果在推理时对特殊标记的关注确实很重要，那么纯粹从词间注意力模式得出结论似乎不是那么可信。</p>
<p>对于特殊的标记的理解往往并不是那么容易。比如对于[CLS]位的理解，通常是被认为对一句话的聚合理解(虽然所有的token多多少少都会有一些句子级别的语义信息)；在这种情况下，我们可能看不到例如词间注意力中的完整句法树，因为部分信息实际上包含在 [CLS] 中。</p>
<p><a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>尝试使用基本 BERT 对 Wikipedia 段落进行编码，以特别考虑对特殊标记的关注，并指出早期层的头部更多地关注 [CLS]，中间层关注 [SEP]，最后一层关注句点和逗号。他们假设它的功能可能是“无操作”之一，如果注意力头的模式不适用于当前情况，则忽略头部的信号。例如，[SEP] 从第 5 层开始受到越来越多的关注，但它对预测的重要性下降。然而，然而，对于具体的下游任务，在对 [SEP] 和 [CLS] 进行微调后后将会得到很多注意力k <a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>。有趣的是，BERT 也非常关注标点符号，<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>对此进行了研究，通过句号和逗号几乎与特殊标记一样频繁的事实来解释，因此模型可能会出于相同的原因学会依赖它们。</p>
<h3 id="BERT-层"><a href="#BERT-层" class="headerlink" title="BERT 层"></a>BERT 层</h3><p>对于BERT的第一个输入层将token，segment，位置编码的信息进行了融合。</p>
<p>理所当然地，较低层具有关于线性词序的最多信息。<a href="https://arxiv.org/pdf/1906.01698.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Lin et al. (2019)</a>报告了 BERT-base 中第 4 层周围线性词序的知识减少，并且伴随了一些分层句子结构知识的增加，由预测标记索引、主要助动词和句子主语的探测任务检测到。</p>
<p>在具有不同任务、数据集和方法的研究中，人们普遍认为句法信息在 BERT 的中间层中最为突出。<a href="https://aclanthology.org/N19-1419/" target="_blank" rel="noopener">Hewitt and Manning (2019)</a>从中间 BERT 层重建句法树深度最成功（base-BERT 为 6-9，BERT-large 为 14-19）。<a href="https://arxiv.org/pdf/1901.05287" target="_blank" rel="noopener">Goldberg (2019)</a>发现在第八第九层主谓一致性上表现效果最好，<a href="https://hal.inria.fr/hal-02131630/document" target="_blank" rel="noopener">Jawahar et al. (2019)</a>进行的句法探测任务上也大致在模型的中间层的效果最好。中间 BERT 层中句法信息的突出在 <a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">Liu et al. (2019a)</a>的研究中表明， Transformer 的中间层总体上表现最好，跨任务的迁移能力最强。如下图所示：</p>
<p><img src="fig4.jpg" alt="avatar"></p>
<center>BERT 层可迁移性（列对应于探测任务）</center>

<p>关于句法块则有一些相互矛盾的研究。<a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Tenney et al. (2019a)</a>总结到基本的句法信息会更早的出现在模型的浅层，而对于高层来说会更多捕捉到语义的特征。在典型的NLP pipline中如 词性标注，语义角色标签，依赖解析,验证了这一些结果。<a href="https://hal.inria.fr/hal-02131630/document" target="_blank" rel="noopener">Jawahar et al. (2019)</a>报告了模型低层对chunking任务更有用，而中间层对parsing任务更有用。但是与此同时，<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">Liu et al. (2019a)</a>的实验结果则表明，无论是词性标注还是chunking任务来说，都是中间层表现的最好。在Bert-base，Bert-large有着相同的结论。但是值得注意的是，他们用的探测任务套件不同。</p>
<p>对于BERT的最后一层来说，是最具任务针对性的层。比如在预训练阶段，这意味着它会更加针对MLM任务，并且在<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">(Liu et al., 2019a)</a>中解释了为啥中间层有更好的迁移性。在微调阶段，<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>解释了为什么最后一层参数的变化最为剧烈。并且<a href="https://arxiv.org/pdf/1908.05620.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Hao et al., 2019)</a>解释了为什么对低层的网络权重改编为原始的值不会对模型的性能发生剧烈的影响。</p>
<p><a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Tenney et al. (2019a)</a> 表明虽然句法信息出现在模型的早期并且可以局部化，但语义分布在整个模型中，这解释了为什么某些非平凡的示例在开始时得到错误解决但在后面的层中得到正确解决。<br>这是意料之中的：语义渗透到所有语言中，语言学家争论是否存在无意义的结构<a href="https://www.academia.edu/download/58135528/Sum_of_Cons_at_Work.pdf" target="_blank" rel="noopener">（Goldberg，2006，p.166-182）</a>。但这衍生出了一个问题，即在 BERT 中堆叠更多的 Transformer 层实际上在语义知识的传播方面实现了什么，以及这是否有益。Tenney et al 比较了 BERT-base 和 BERT-large，发现累积得分增益的总体模式是相同的，只是在更大的模型中更加分散。</p>
<p>Note that <a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Tenney et al. (2019a)</a>的实验主要关注在了句子级别的语义关系，<a href="https://arxiv.org/pdf/1906.08101" target="_blank" rel="noopener">Cui et al. (2020)</a>报告称对ConceptNet 语义关系的编码在早期层中是最差的，在向顶部转移的过程中增加。</p>
<p><a href="https://hal.inria.fr/hal-02131630/document" target="_blank" rel="noopener">Jawahar et al. (2019)</a>假设表面特征出现在低层，句法特征出现在中间层，语义特征出现在高层。但他们的结论令人惊讶，因为本研究中只有一个语义任务实际上在最后一层达到顶峰，另外三个在中间达到顶峰，然后在最后一层显著降低。</p>
<h2 id="BERT的训练"><a href="#BERT的训练" class="headerlink" title="BERT的训练"></a>BERT的训练</h2><p>在这部分章节将展开对于BERT优化模型训练和结构的内容。</p>
<h3 id="模型结构的选择"><a href="#模型结构的选择" class="headerlink" title="模型结构的选择"></a>模型结构的选择</h3><p><a href="https://arxiv.org/pdf/1912.07840" target="_blank" rel="noopener">Wang et al. (2019b)</a>对BERT模型结构的研究进行了系统的研究，他尝试了不同的层数、头部和模型参数，改变了一个选项并冻结了其他选项。他们总结认为模型的头的数量并没有模型层数的数量造成的影响大。这和<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al. (2019b)</a> and <a href="https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf" target="_blank" rel="noopener">Michel et al. (2019)</a> 的研究结果保持了一致，在<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">Liu et al. (2019a)</a>的研究中发现模型的中间层的可迁移性更强。较大的隐层表示大小始终更好，但增益因具体设置而异。</p>
<p>总的来说，对于注意力头的改变和层数的改变会表现出不一样的函数功能<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">(Liu et al., 2019a)</a>，对于初始层来说，表现出了任务不变性的效果<a href="https://arxiv.org/pdf/1908.05620.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Hao et al., 2019)</a>，并且输出的token表示也和初始的token embedding的表示最为相似<a href="https://arxiv.org/pdf/1908.04211" target="_blank" rel="noopener">(Brunner et al., 2020)</a>。如果是这种情况，更深的模型有更多的能力来编码非特定任务的信息。</p>
<p>另一方面，在 vanilla BERT中似乎许多的自注意力头似乎学习到了相同模式<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>。这就解释了为什么对它进行剪枝不会造成太多的影响。由此产生的问题是，我们可以通过有意识地鼓励多样化的自注意力机制模式走多远：从理论上讲，这意味着在相同数量的权重下增加模型中的信息量。<a href="https://arxiv.org/pdf/2002.10260.pdf" target="_blank" rel="noopener">Raganato et al. (2020)</a> 表示对于以 Transformer模型位backbone的机器翻译任务，我们可以简单地预设我们已经知道模型会学习的模式，而不是从头开始学习它们。</p>
<p>Vanilla BERT 在自注意力部分和前反馈网络层部分是对称和平衡的，但是也不是必须是这样。</p>
<h3 id="对于训练方法的改进"><a href="#对于训练方法的改进" class="headerlink" title="对于训练方法的改进"></a>对于训练方法的改进</h3><p><a href="https://arxiv.org/pdf/1907.11692.pdf%5C" target="_blank" rel="noopener">Liu et al. (2019b)</a> 证实了在训练过程中对于batchsize扩大的收益：对于8k大小的模型数量来说，无论是模型的困惑度还是在下游任务的表现上都有了明显的提升。并且他们推荐一些训练的参数。<a href="https://arxiv.org/pdf/1904.00962.pdf" target="_blank" rel="noopener">You et al. (2019)</a>) 报告称对于有32k大小的batchsize的模型训练，训练收敛的时间缩短并且在性能上没有损失。<a href="https://arxiv.org/abs/1911.03918" target="_blank" rel="noopener">Zhou et al. (2019)</a>观察到训练后的 [CLS] 标记的归一化稳定了训练并略微提高了文本分类任务的性能。</p>
<p><a href="http://proceedings.mlr.press/v97/gong19a/gong19a.pdf" target="_blank" rel="noopener">Gong et al. (2019)</a> 注意到，虽然自注意力机制的模式在低层和高层表现出相似的效果，模型的训练可以以递归的方式完成。其中首先训练较浅的版本，然后将训练的参数复制到更深层。这种“热启动”可以在不牺牲性能的情况下使训练速度提高 25%。</p>
<h3 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h3><p>原始的BERT是一个双向的Transformer并且通过NSP和MLM任务进行预训练。许多研究通过对预训练目标函数进行优化从而整体改进BERT的表达能力，可以分为以下几个点:</p>
<ul>
<li><p>如何去mask？ <a href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" target="_blank" rel="noopener">Raffel et al. (2019)</a> 系统地测试mask率和mask的跨度长度。<a href="(https://arxiv.org/pdf/1907.11692.pdf%5C">Liu et al. (2019b)</a>)提出了在一个训练epoch中的多种的mask方式。<a href="https://arxiv.org/pdf/1903.07785.pdf?fbclid=IwAR2eIBWLbo0EShXvIhkMtS9OCwAipX8xKMS3GibEfP5oDwzjRv8r5WdlMtc" target="_blank" rel="noopener">Baevski et al. (2019)</a> 选择去mask掉一个序列中的所有token而不是随机采样的方式。<a href="https://arxiv.org/pdf/1909.12744" target="_blank" rel="noopener">Clinchant et al. (2019)</a>通过将[unk]的token替换成MASK token的方式来帮助模型可以学习到更多的不在词表中单词的表示方式从而使得翻译任务更加受益。<a href="https://proceedings.neurips.cc/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf" target="_blank" rel="noopener">Song et al. (2020)</a>通过对masked和unmasked的token进行调节从而极大化信息的总量，使得模型模型可以看到有多少token被遗漏。</p>
</li>
<li><p>去对什么做mask? mask可以被作用于全词而不是word piece(<a href="https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ" target="_blank" rel="noopener">Devlin et al., 2019;</a> <a href="https://arxiv.org/pdf/1906.08101" target="_blank" rel="noopener">Cui et al., 2019</a>)。同样的，也可以mask一个具体跨距的文本信息<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00300/43539" target="_blank" rel="noopener">(Joshi et al., 2020)</a>。来预测到底其中有多少个词被mask掉<a href="https://arxiv.org/pdf/1910.13461" target="_blank" rel="noopener">(Lewis et al., 2019)</a>。mask掉词组信息和命名实体可以提高对结构化知识的表示能力<a href="https://arxiv.org/pdf/1904.09223.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Sun et al., 2019b)</a>。</p>
</li>
<li><p>在文本中的什么位置做mask?<a href="https://arxiv.org/pdf/1901.07291.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener"> Lample and Conneau (2019)</a> 使用任意的text stream 而不是句子对并且采用子样本频繁输出类似于<a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" target="_blank" rel="noopener">Mikolov et al. (2013)</a>中的方法。<a href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf" target="_blank" rel="noopener">Bao et al. (2020)</a>使用特殊的伪掩码标记将标准自动编码 MLM 与部分自回归 LM 目标相结合。</p>
</li>
<li><p>掩码的替代方式。<a href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" target="_blank" rel="noopener">Raffel et al. (2019)</a>实验了替换和丢弃spans的方式。<a href="https://arxiv.org/pdf/1910.13461" target="_blank" rel="noopener">Lewis et al. (2019)</a>探索删除、填充、句子排列和文档旋转等方式，<a href="https://ojs.aaai.org/index.php/AAAI/article/download/6428/6284" target="_blank" rel="noopener">Sun et al. (2019c)</a>预测一个token是否是大写并且它是否出现在文档中的其它位置。<a href="https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf" target="_blank" rel="noopener">Yang et al. (2019)</a> 训练输入序列中不同的词序排列，最大化原始词序的概率(参照n-gram 词序重建问题<a href="https://arxiv.org/pdf/1908.04577" target="_blank" rel="noopener">(Wang et al., 2019a)</a>)。<a href="https://arxiv.org/pdf/2003.10555.pdf%3C/p%3E" target="_blank" rel="noopener">Clark et al. (2020)</a>检测被生成网络替换的token而不是mask掉的token。</p>
</li>
<li><p>NSP任务的替代。移除 NSP任务并不会伤害到或者还有可能轻微提升模型的表达能力(<a href="https://arxiv.org/pdf/1907.11692.pdf%5C" target="_blank" rel="noopener">Liu et al., 2019b;</a><a href="(https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00300/43539">Joshi et al., 2020;</a>) <a href="https://arxiv.org/pdf/1909.12744" target="_blank" rel="noopener">Clinchant et al., 2019).</a> Wang et al. (2019a),and <a href="https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf" target="_blank" rel="noopener">Cheng et al. (2019)</a> 将NSP任务替换为预测上一句和下一句话。<a href="https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Lan et al. (2020a)</a>用来自正面例子的句子而不是来自不同文档的句子来替换负面的 NSP 例子。ERNIE 2.0包含了句子重新排序和句子距离预测任务。<a href="https://onikle.com/articles/299727" target="_blank" rel="noopener">Bai et al. (2020)</a>用段落、句子和标记索引嵌入的组合替换 NSP 和标记位置嵌入。<a href="https://arxiv.org/pdf/2004.03561" target="_blank" rel="noopener">Li and Choi (2020)</a>试验多方对话的话语顺序预测任务(并且采用的MLM任务不仅在utterancss的级别上并且还有整个对话上进行这样的操作)。</p>
</li>
<li><p>其它的任务。<a href="https://ojs.aaai.org/index.php/AAAI/article/download/6428/6284" target="_blank" rel="noopener">Sun et al. (2019c)</a>提出同时学习 7 个任务，包括话语关系分类和预测片段是否与 IR（infromation retrival）相关。<a href="https://arxiv.org/pdf/2002.08909.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Guu et al. (2020)</a> 在语言模型预训练中包含一个潜在知识检索器。<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089" target="_blank" rel="noopener">Wang et al. (2020c)</a>通过completion objective任务将MLM和知识进行结合。<a href="https://arxiv.org/pdf/1909.04120" target="_blank" rel="noopener">Glass et al. (2020)</a>将MLM任务替换为span预测任务（就像抽取式的question-answering）任务类似，模型的输出预期位提供的答案不是从它的权重中产生的，而是从一个存在问题正确答案的不同的段落汇总得出（相关的搜索引擎查询片段）。</p>
</li>
</ul>
<p>另外一个有提升空间的是预训练所得到的数据。很多研究探索增加训练数据集体量所带来的收益(<a href="https://arxiv.org/pdf/1907.11692.pdf%5C" target="_blank" rel="noopener">Liu et al., 2019b;</a> <a href="https://arxiv.org/pdf/1911.02116" target="_blank" rel="noopener">Conneau et al., 2019;</a> <a href="https://arxiv.org/pdf/1903.07785.pdf?fbclid=IwAR2eIBWLbo0EShXvIhkMtS9OCwAipX8xKMS3GibEfP5oDwzjRv8r5WdlMtc" target="_blank" rel="noopener">Baevski et al., 2019</a>),还有增加训练的轮数<a href="https://arxiv.org/pdf/1907.11692.pdf%5C" target="_blank" rel="noopener">(Liu et al., 2019b)</a>所带来的变化。数据也并不一定需要原始的文本内容，有很多研究整合了明确的语言信息。不论是在句法<a href="https://arxiv.org/pdf/1911.06156" target="_blank" rel="noopener">(Sundararaman et al., 2019)</a>还是在语义上<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6510/6366" target="_blank" rel="noopener">(Zhang et al., 2020)</a>。<a href="https://arxiv.org/pdf/1812.06705.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Wu et al. (2019b)</a> 和 <a href="https://arxiv.org/pdf/2003.02245" target="_blank" rel="noopener">Kumar et al. (2020)</a>增加了来自带标注的任务数据集的给定序列的标签。<a href="https://arxiv.org/pdf/1910.07181" target="_blank" rel="noopener">Schick and Schütze (2020)</a>的工作分别学习了稀有词的表示。</p>
<p>虽然BERT已经经常用于世界知识的源头上，当然也会存在一些提供明确结构化知识的工作。一种方法是通过实体增强的方式。比如说，<a href="https://arxiv.org/pdf/1909.04164" target="_blank" rel="noopener">Peters et al. (2019a);</a> <a href="https://arxiv.org/pdf/1905.07129.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Zhang et al. (2019)</a>在训练BERT的过程中引入了实体的嵌入信息，<a href="https://arxiv.org/abs/1911.03681" target="_blank" rel="noopener">Poerner et al. (2019)</a> 采用了实体向量作为BERT输入表示。上文中也提到过，<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089" target="_blank" rel="noopener">Wang et al. (2020c)</a>不仅通过实体嵌入来引入知识，并且通过额外的预训练目标————知识库补全预训练目标。<a href="(https://arxiv.org/pdf/1904.09223.pdf?ref=https://githubhelp.com">Sun et al. (2019b,</a>)<a href="https://ojs.aaai.org/index.php/AAAI/article/download/6428/6284" target="_blank" rel="noopener">  and  c)</a>调整标准 MLM 任务以MASK命名实体而不是随机词。<a href="https://arxiv.org/pdf/2005.08314.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Yin et al. (2020)</a> 同时在文本数据上和线性化表数据进行MLM任务。<a href="https://arxiv.org/pdf/1908.04577" target="_blank" rel="noopener">Wang et al. (2020a)</a>使用特定于任务的适配器增强 RoBERTa 的语言和事实知识。</p>
<p>预训练是训练 BERT 最昂贵的部分，所以知道它提供了多少好处可以说也是十分必要的了。<br>在某些任务上，随机初始化和微调的 BERT 比使用任务分类器和冻结权重的预训练 BERT 获得有竞争力或更高的结果<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>。普遍研究者的共识是预训练在大多数情况下确实有帮助，但程度及其确切贡献需要进一步调查。<a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">Prasanna et al. (2020)</a>发现大多数预训练BERT的权重在微调阶段是十分有用的，尽管有“更好”和“更差”的子网络。一种解释是，预训练的权重有助于微调的 BERT 找到泛化误差更小的更宽、更平坦的区域，这使得模型对过拟合更加鲁棒，如下图所示。</p>
<p><img src="fig5.jpg" alt="avatar"></p>
<center>与从头开始训练（左）相比，预训练的权重帮助 BERT 在 MRPC（右）的微调中找到更广泛的最优值</center>

<p>鉴于提议的修改数量众多且种类繁多，人们想知道它们各自的影响有多大。然而，随着总体上模型的朝着更大尺寸的方向上发展，因此想要做系统的消融实验将变得十分昂贵。大多数的新模型宣称在标准的benchmarks上有更好的表现效果，但是但收益通常是微不足道的，模型稳定性和显著性检验的估计非常罕见。</p>
<h3 id="BERT的微调"><a href="#BERT的微调" class="headerlink" title="BERT的微调"></a>BERT的微调</h3><p>预训练+微调的方式是BERT工作流中至关重要的。前者提供任务无关的知识，后者会教模型更多依赖于对下游任务有用的的知识。</p>
<p><a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">Kovaleva et al. (2019)</a>没有发现 BERT 在 GLUE 任务上微调的情况（表明对 Universal Dependencies 进行微调确实会产生语法上有意义的注意力模式，但没有定量评估。）：在微调阶段，在3 epochs时，模型的最后两层有更多的变化，但是这种变化更多是使得自注意力集中于[SEP]<br>而不是语言上的可解释模式。可以理解为什么微调会增加对 [CLS] 的关注，而不是 [SEP]。如果 <a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>  等人认为 [SEP] 作为“无操作”指标是正确的，那么微调基本上会告诉 BERT 忽略什么。</p>
<p>几项研究探讨了改进 BERT 微调的可能方向：</p>
<ul>
<li><p>把更多网络层考虑进去: 在深层和输出层中学习信息的互补表示<a href="https://arxiv.org/pdf/1911.01940" target="_blank" rel="noopener">(Yang and Zhao, 2019)</a>,使用所有层的加权组合而不是最后一层(<a href="https://arxiv.org/pdf/1910.03176" target="_blank" rel="noopener">Su and Cheng, 2019;</a> <a href="https://arxiv.org/pdf/1904.02099.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Kondratyuk and Straka, 2019</a>),和层dropout<a href="https://arxiv.org/pdf/1904.02099.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Kondratyuk and Straka, 2019)</a>。</p>
</li>
<li><p>两阶段微调的方式是一种中间监督训练的方式在预训练和微调阶段。(<a href="https://arxiv.org/pdf/1811.01088.pdf" target="_blank" rel="noopener">Phang et al., 2019;</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6282/6138" target="_blank" rel="noopener">Garg et al., 2020;</a> <a href="https://arxiv.org/pdf/1909.00931" target="_blank" rel="noopener">Arase and Tsujii, 2019;</a> <a href="https://arxiv.org/pdf/2005.00628" target="_blank" rel="noopener">Pruksachatkun et al., 2020;</a> <a href="https://arxiv.org/pdf/2008.06788" target="_blank" rel="noopener">Glavaš and Vuli´c, 2020)</a>.) <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00328/96489" target="_blank" rel="noopener">Ben-David et al. (2020)</a>提出了一种基于枢轴的 MLM 变体来微调 BERT 以进行域适应。</p>
</li>
<li><p>对抗性token扰动提高了模型的鲁棒性<a href="https://openreview.net/forum?id=Fk_47QnlVy" target="_blank" rel="noopener">(Zhu et al., 2019)</a>。</p>
</li>
<li><p>对抗正则化结合 Bregman Proximal Point Optimization 有助于缓解预训练的知识遗忘，从而防止 BERT 过度拟合下游任务<a href="https://arxiv.org/pdf/1911.03437" target="_blank" rel="noopener">(Jiang et al., 2019a)</a>。</p>
</li>
<li><p>Mixout regularization即使对于少量训练示例，也能提高 BERT 微调的稳定性<a href="https://arxiv.org/pdf/1909.11299" target="_blank" rel="noopener">(Lee et al., 2019)</a>。</p>
</li>
</ul>
<p>对于大型的模型来说，即使微调也是expensive的,但是 <a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf" target="_blank" rel="noopener">Houlsby et al. (2019)</a> 表明它可以通过适配器模块成功逼近。他们以一小部分计算成本在 26 个分类任务上实现了具有竞争力的性能。BERT 中的适配器也用于多任务学习<a href="http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf" target="_blank" rel="noopener">(Stickland and Murray, 2019)</a>和跨语言迁移<a href="https://arxiv.org/pdf/1910.11856" target="_blank" rel="noopener">(Artetxe et al., 2019)</a>。微调的替代方法是从冻结的表示中提取特征，但微调对 BERT 效果更好<a href="https://arxiv.org/pdf/1903.05987" target="_blank" rel="noopener">(Peters et al., 2019b)</a>。</p>
<p>当前 NLP 的一个重大方法学挑战是，新模型的报告性能改进很可能在环境因素引起的变化范围内<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00018/1567606/tacl_a_00018.pdf" target="_blank" rel="noopener">(Crane, 2018)</a>。BERT模型也不例外。<a href="https://arxiv.org/pdf/2002.06305.pdf%C3%A3%E2%82%AC%20https://www.aclweb.org/anthology/2020.trac-1.9%C3%A3%E2%82%AC%E2%80%9A" target="_blank" rel="noopener">Dodge et al. (2020)</a>报告由于权重初始化和训练数据顺序，BERT 在 GLUE 任务上微调的显着变化。他们还提出了尽早停止前景不佳的随机种子的训练的方法。</p>
<p>上述内容并没有论述了当前关于微调的全部内容，比如说Siamese 孪生结构，策略梯度训练，自动课程学习等。</p>
<h2 id="BERT模型的尺寸多大为好？"><a href="#BERT模型的尺寸多大为好？" class="headerlink" title="BERT模型的尺寸多大为好？"></a>BERT模型的尺寸多大为好？</h2><h3 id="过度参数化"><a href="#过度参数化" class="headerlink" title="过度参数化"></a>过度参数化</h3><p>以Transformer模型backbone保持数量级增长，1.1M大小的模型在Turing-NLG<a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" target="_blank" rel="noopener">(Microsoft, 2020)</a>时达到了170亿的参数量，与 GPT-3 的 1750亿<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">(Brown et al., 2020)</a> 相比都相形见绌，这个趋势引起了对自注意力计算复杂度的关切<a href="https://arxiv.org/pdf/1901.10430.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Wu et al., 2019a)</a>。环境问题的关切(<a href="https://arxiv.org/pdf/1906.02243.pdf%22%3EWachstum" target="_blank" rel="noopener">Strubell et al., 2019;</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3381831" target="_blank" rel="noopener">Schwartz et al., 2019</a>)，模型结构公平比较的问题<a href="https://arxiv.org/pdf/2001.00781" target="_blank" rel="noopener">(Aßenmacher and Heumann, 2020)</a>,可复现性等问题。</p>
<p>人类语言极其复杂，而且可能需要更多的参数来完全描述，但是当前的模型并没有很好地利用它们已经拥有的参数。<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al. (2019b)</a> 发现除了少数 Transformer 头外，其他所有的 Transformer 头都可以修剪，而不会显着降低性能。对于BERT来说，<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>观察到同一层中的大多数头都显示出相似的自注意力模式（可能与一层中所有自注意力头的输出都通过相同的 MLP 的事实有关），也就解释了为什么<a href="(https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf">Michel et al. (2019)</a>)可以对将多数层减少到一个头。</p>
<p>由于任务的特性，一些BERT的头，或者网络层不仅仅是多余的<a href="https://www.hindawi.com/journals/sp/2021/6641832/" target="_blank" rel="noopener">(Kao et al., 2020)</a>，也同时会对下游任务有损害，<a href="https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf" target="_blank" rel="noopener">(Michel et al., 2019)</a>报告了在机器翻译任务中停用某些头的积极影响。<a href="https://arxiv.org/pdf/1911.03898" target="_blank" rel="noopener">(Baan et al., 2019)</a>报告了在文本摘要任务中停用某些头的集体影响。<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">(Kovaleva et al., 2019)</a>报告了GLUE任务上的影响。此外，<a href="(https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com">Tenney et al. (2019a)</a>)检查他们的结构探测分类器的累积增益，观察到在 8 个探测任务中的 5 个中，某些层会导致分数下降（通常在最后一层）。<a href="https://arxiv.org/pdf/2002.08307" target="_blank" rel="noopener">Gordon et al. (2020)</a>发现可以剪枝 30-40% 的权重而不影响下游任务。</p>
<p>通常情况下，模型尺寸更大，表现效果越好(<a href="https://arxiv.org/pdf/1903.08855" target="_blank" rel="noopener">Liu et al., 2019a;</a> <a href="https://arxiv.org/pdf/2002.08910" target="_blank" rel="noopener">Roberts et al., 2020</a>)，但是在主谓一致探测的任务上，BERT-base竟然比BERT-large模型效果要好<a href="(https://arxiv.org/pdf/1906.01698.pdf?ref=https://githubhelp.com">(Lin et al., 2019)</a>)。考虑到语言的复杂性和大量的预训练数据，不清楚为什么 BERT 最终会出现冗余的头和层。<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">Clark et al. (2019)</a>认为一个可能的原因是使用注意力的dropout，这会导致一些注意权重在训练期间被归零。</p>
<h3 id="比较的方法"><a href="#比较的方法" class="headerlink" title="比较的方法"></a>比较的方法</h3><p>鉴于以上关于参数过度化的证据，对于可以将BERT高效的压缩而不至于使得accuracy降低并不是一件奇怪的事情。对于现实场景的应用将是一个非常好的用途。一些好的工作结果可以总结如下表所示:</p>
<p><img src="fig6.jpg" alt="avatar"><br>主要方法是知识蒸馏、量化和剪枝。</p>
<p><a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_BiHPF_Bilateral_High-Pass_WACV_2022_supplemental.zip" target="_blank" rel="noopener">(Hinton et al., 2014)</a> 对于知识蒸馏的框架具体是使用了一个尺寸较小的学生网络去模仿更大的教师网络的行为。对于BERT来说，这是通过损失函数实验实现的(<a href="https://arxiv.org/pdf/1910.01108.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Sanh et al., 2019b;</a> <a href="https://arxiv.org/pdf/1909.10351.pdf%E8%AE%BA%E6%96%87%E4%B8%AD" target="_blank" rel="noopener">Jiao et al., 2019</a>)。模仿教师网络各个部分的激活模式<a href="https://arxiv.org/pdf/1908.09355" target="_blank" rel="noopener">(Sun et al., 2019a)</a>，和在预训练结对对知识的迁移(<a href="https://arxiv.org/abs/1908.08962" target="_blank" rel="noopener">Turc et al., 2019;</a> <a href="https://arxiv.org/pdf/1909.10351.pdf%E8%AE%BA%E6%96%87%E4%B8%AD" target="_blank" rel="noopener">Jiao et al., 2019;</a> Sun et al., 2020)或者是在微调阶段的知识迁移<a href="https://arxiv.org/pdf/1909.10351.pdf%E8%AE%BA%E6%96%87%E4%B8%AD" target="_blank" rel="noopener">(Jiao et al., 2019)</a>，<a href="https://arxiv.org/pdf/1910.06360" target="_blank" rel="noopener">McCarley et al. (2020)</a>表明到目前为止，蒸馏对 GLUE 的效果比对阅读理解的效果更好，并且报告了结构化修剪和特定任务蒸馏相结合的 QA 任务的良好结果。</p>
<p>量化通过降低权重的精度来减少 BERT 的内存占用(<a href="https://ojs.aaai.org/index.php/AAAI/article/download/6409/6265" target="_blank" rel="noopener">Shen et al., 2019;</a> <a href="https://arxiv.org/pdf/1910.06188" target="_blank" rel="noopener">Zafrir et al., 2019</a>)。注意到这种策略通常需要兼容的硬件。</p>
<p>在上文中提到，可以禁用单个自注意力头和BERT层，而不会显着降低性能(<a href="(https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf">Michel et al., 2019;</a>)<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener"> Kovaleva et al., 2019;</a> Baan et al., 2019)。剪枝是一种利用这一事实的压缩技术，通常通过将大型模型的某些部分归零来减少计算量。在结构化修剪中，结构化的模块被丢弃，就如LayerDrop一样(Fan et al., 2019)。在非结构化中，无论它们的位置如何，整个模型中的权重都会被剪枝，就像幅度剪枝（magnitude pruning）(Chen et al., 2020)一样，或运动修建(movement pruning)(Sanh et al., 2020)。</p>
<p><a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">Prasanna et al. (2020)</a> 和 <a href="https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf" target="_blank" rel="noopener">Chen et al. (2020)</a> 探索了彩票假说（lottery ticket hypothesis）视角下的 BERT。专门研究预训练的 BERT 中的“获胜”子网络。他们独立发现确实存在这样的子网络，并且不同任务的子网络之间的可转移性各不相同。</p>
<p>如果训练 BERT 的最终目标是压缩，<a href="https://arxiv.org/pdf/2002.11794.pdf%3C/p%3E" target="_blank" rel="noopener">Li et al. (2020)</a>建议训练较大的模型并对其进行大量压缩，而不是对较小的模型进行轻微压缩。</p>
<p>其他技术包括将 BERT 的嵌入矩阵分解为更小的矩阵<a href="https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Lan et al., 2020a)</a>, 渐进式模块替换<a href="https://arxiv.org/pdf/2002.02925" target="_blank" rel="noopener">(Xu et al., 2020)</a>和中间编码器输出的动态消除<a href="https://openreview.net/forum?id=knPZR5brNG" target="_blank" rel="noopener">(Goyal et al., 2020)</a>。见 <a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00413/1964006/tacl_a_00413.pdf" target="_blank" rel="noopener">Ganesh et al. (2020)</a>更详细地讨论了压缩方法。</p>
<h3 id="剪枝和模型分析"><a href="#剪枝和模型分析" class="headerlink" title="剪枝和模型分析"></a>剪枝和模型分析</h3><p>关于剪枝作为一种模型分析技术的讨论刚刚开始。基本思想是先验压缩模型由对预测有用的元素组成；因此，通过找出他们在做什么，我们可能会发现整个网络在做什么。例如，BERT 的头部似乎编码了帧语义关系，但禁用它们可能不会损害下游任务的性能<a href="https://arxiv.org/pdf/1908.08593" target="_blank" rel="noopener">Kovaleva et al. (2019);</a>这表明这部分的知识并没有真正被用到。</p>
<p>对于基础的Transformer模型，<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al. (2019b)</a>识别 self-attention 头的功能，然后检查它们中的哪些在修剪后幸存下来，发现句法头和位置头是最后一个去做的事情。对于BERT来说，<a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">Prasanna et al. (2020)</a>朝相反的方向走：根据重要性分数进行修剪，并解释剩余的“好”子网络。特别是对于自注意力头，似乎并非只有可能编码非平凡语言模式的头才能在修剪后幸存下来。</p>
<p>这些研究中的模型和方法不同，所以证据是不确定的。特别指出的是，<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al. (2019b)</a>发现在剪枝之前大部分头都是句法信息的，<a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">Prasanna et al. (2020)</a> –大多数头部没有潜在的非平凡注意模式。</p>
<p>当前头部和层消融研究的一个重要限制(<a href="(https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf">Michel et al., 2019;</a>) Koval- eva et al., 2019)是他们固有地假设某些知识包含在头/层中。然而，有证据表明在整个网络中分布的表示更加分散，例如在困难的语义解析任务上的准确性逐渐提高<a href="https://arxiv.org/pdf/1905.05950.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Tenney et al., 2019a)</a>，或者没有“一般”执行解析的头部<a href="https://arxiv.org/pdf/1906.04341" target="_blank" rel="noopener">(Clark et al., 2019;</a> <a href="https://arxiv.org/pdf/1911.12246" target="_blank" rel="noopener">Htut et al., 2019</a>)。如果是这样的话，那么对于单个组件的消融会对整个网络的权重共享机制产生损害。</p>
<h2 id="一些比较重要的研究方向"><a href="#一些比较重要的研究方向" class="headerlink" title="一些比较重要的研究方向"></a>一些比较重要的研究方向</h2><p>BERTology 显然已经发展了很多的理论研究探索了，但是，总的来说我们还是关于BERT的工作方式有很多疑问。接下来将要列举一些未来可能会有潜力的发展方向。</p>
<p><strong>需要口头推理的基准。</strong>,虽然BERT在许多 NLP 基准测试上取得突破，越来越多的分析论文表明，它的语言能力并不像看起来那么令人印象深刻。特别是，它被证明依赖于自然语言推理中的浅层启发式(<a href="https://arxiv.org/pdf/1902.01007" target="_blank" rel="noopener">McCoy et al., 2019b;</a> <a href="https://arxiv.org/pdf/1905.07830" target="_blank" rel="noopener">Zellers et al., 2019;</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6311/6167" target="_blank" rel="noopener">Jin et al., 2020</a>)。阅读理解(<a href="https://arxiv.org/pdf/1910.12391" target="_blank" rel="noopener">Si et al., 2019a;</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6398/6254" target="_blank" rel="noopener">Rogers et al., 2020;</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278" target="_blank" rel="noopener">Sugawara et al., 2020;</a> <a href="https://arxiv.org/pdf/1910.12391" target="_blank" rel="noopener">Si et al., 2019b;</a> <a href="https://arxiv.org/pdf/1901.11373" target="_blank" rel="noopener">Yogatama et al., 2019</a>)，<br>论证推理理解<a href="https://arxiv.org/pdf/1907.07355.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Niven and Kao, 2019)</a>,和文本分类<a href="(https://ojs.aaai.org/index.php/AAAI/article/download/6311/6167">(Jin et al., 2020)</a>))。这种启发式甚至可以用于重建非公开可用的模型<a href="https://arxiv.org/pdf/1910.12366" target="_blank" rel="noopener">(Krishna et al., 2020)</a>。与任何优化方法一样，如果数据中存在捷径，我们没有理由指望 BERT 不学习它。但是，如果开发不如建模重视，则不太可能出现无法用浅层启发式解决的更难的数据集工作。</p>
<p><strong>全方位语言能力的基准。</strong> 虽然语言模型似乎获得了大量关于语言的知识，但我们目前还没有针对语言知识的不同方面进行全面的压力测试。朝这个方向迈出的一步是“检查表”行为测试<a href="https://arxiv.org/pdf/2005.04118.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">(Ribeiro et al., 2020)</a>，2020年ACL的最佳论文。理想情况下，此类测试不仅可以测量错误，还可以测量灵敏度<a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf" target="_blank" rel="noopener">(Ettinger, 2019)</a>。</p>
<p><strong>开发“教导”推理的方法。</strong>虽然大型预训练模型有很多知识，但如果需要在它们拥有的事实之上进行任何推理，它们通常会失败<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00342/96476" target="_blank" rel="noopener">(Talmor et al., 2019)</a>，例如，<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6397/6253" target="_blank" rel="noopener">Richardson et al. (2020)</a> 提出一种“教”BERT量化、条件、比较和布尔协调的方法。</p>
<p><strong>在推理的时候学习到发生了什么</strong> </p>
<p>大多数 BERT 分析论文侧重于模型的不同探测，目标是找出语言模型“知道”什么。然而，探测任务也有它本身的局限性。到目前为止，很少有论文专注于发现实际使用的知识。几个有希望的方向是“遗忘探测”<a href="https://arxiv.org/abs/2006.00995" target="_blank" rel="noopener">(Elazar et al., 2020)</a>，识别对给定任务的预测重要的特征<a href="https://arxiv.org/pdf/1910.06431" target="_blank" rel="noopener">(Arkhangelskaia and Dutta, 2019)</a>,并修剪模型以删除不重要的组件(<a href="https://arxiv.org/pdf/1905.09418.pdf?ref=https://githubhelp.com" target="_blank" rel="noopener">Voita et al., 2019b;</a> <a href="https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf" target="_blank" rel="noopener">Michel et al., 2019;</a> <a href="https://arxiv.org/pdf/2005.00561?fbclid=IwAR0tXWNR8Yn896ZTuDAxIVuDeK3YC1Cgnrj77kQgIvSaNPqCO9RtnhM31x0" target="_blank" rel="noopener">Prasanna et al., 2020</a>).</p>
<h2 id="all-in-all"><a href="#all-in-all" class="headerlink" title="all in all"></a>all in all</h2><p>在一年多一点的时间里，BERT 已经成为 NLP 实验中无处不在的基线，并激发了许多分析该模型并提出各种改进的研究。这篇论文的意义在于可以激发大家关注那些更需要解决的问题上。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>O(∩_∩)O哈哈~</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="bool_tbb 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="bool_tbb 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/BERT/" rel="tag"># BERT</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/" rel="next" title="bilstm+crf(batch)的实现">
                  <i class="fa fa-chevron-left"></i> bilstm+crf(batch)的实现
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2022/05/21/Parameter-Efficient-Transfer-Learning/" rel="prev" title="Parameter Efficient Transfer Learning">
                  Parameter Efficient Transfer Learning <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#序言"><span class="nav-number">1.</span> <span class="nav-text">序言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#动机"><span class="nav-number">2.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#主要内容"><span class="nav-number">3.</span> <span class="nav-text">主要内容</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT-模型概述"><span class="nav-number">3.1.</span> <span class="nav-text">BERT 模型概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT-到底学了个啥？"><span class="nav-number">3.2.</span> <span class="nav-text">BERT 到底学了个啥？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#句法知识"><span class="nav-number">3.2.1.</span> <span class="nav-text">句法知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语义知识"><span class="nav-number">3.2.2.</span> <span class="nav-text">语义知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#世界知识"><span class="nav-number">3.2.3.</span> <span class="nav-text">世界知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一些局限性"><span class="nav-number">3.2.4.</span> <span class="nav-text">一些局限性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#局部化语言的知识"><span class="nav-number">3.3.</span> <span class="nav-text">局部化语言的知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-嵌入"><span class="nav-number">3.3.1.</span> <span class="nav-text">BERT 嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关于自注意力头"><span class="nav-number">3.3.2.</span> <span class="nav-text">关于自注意力头</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#具有语言功能的头部"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">具有语言功能的头部</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对特殊字段的注意力"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">对特殊字段的注意力</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-层"><span class="nav-number">3.3.3.</span> <span class="nav-text">BERT 层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT的训练"><span class="nav-number">3.4.</span> <span class="nav-text">BERT的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型结构的选择"><span class="nav-number">3.4.1.</span> <span class="nav-text">模型结构的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对于训练方法的改进"><span class="nav-number">3.4.2.</span> <span class="nav-text">对于训练方法的改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预训练BERT"><span class="nav-number">3.4.3.</span> <span class="nav-text">预训练BERT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT的微调"><span class="nav-number">3.4.4.</span> <span class="nav-text">BERT的微调</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT模型的尺寸多大为好？"><span class="nav-number">3.5.</span> <span class="nav-text">BERT模型的尺寸多大为好？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#过度参数化"><span class="nav-number">3.5.1.</span> <span class="nav-text">过度参数化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#比较的方法"><span class="nav-number">3.5.2.</span> <span class="nav-text">比较的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#剪枝和模型分析"><span class="nav-number">3.5.3.</span> <span class="nav-text">剪枝和模型分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一些比较重要的研究方向"><span class="nav-number">3.6.</span> <span class="nav-text">一些比较重要的研究方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#all-in-all"><span class="nav-number">3.7.</span> <span class="nav-text">all in all</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bool_tbb"
      src="/images/tb3.jpeg">
  <p class="site-author-name" itemprop="name">bool_tbb</p>
  <div class="site-description" itemprop="description">关于一些在NLP领域的学习总结，或者随便写点啥。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/iiot-tbb" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;iiot-tbb" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/bool_tbb@sjtu.edu.cn" title="E-Mail → bool_tbb@sjtu.edu.cn"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/5313762851" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;5313762851" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/16817976/bool-tbb" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;16817976&#x2F;bool-tbb" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      推荐阅读
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://developer.apple.com/swift/" title="https:&#x2F;&#x2F;developer.apple.com&#x2F;swift&#x2F;" rel="noopener" target="_blank">Swift 4</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://developer.apple.com/documentation/objectivec" title="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;objectivec" rel="noopener" target="_blank">Objective -C</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

      
      <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
      <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
      <div class="widget-wrap">
          <h3 class="widget-title">Tag Cloud</h3>
          <div id="myCanvasContainer" class="widget tagcloud">
              <canvas width="250" height="250" id="resCanvas" style="width=100%">
                  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Autograd/" rel="tag">Autograd</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Languaage-Model/" rel="tag">Languaage Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/" rel="tag">Language Model</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PDF%E8%A7%A3%E6%9E%90/" rel="tag">PDF解析</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parameter-Efficient/" rel="tag">Parameter Efficient</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Syntax-Tree/" rel="tag">Syntax-Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crf/" rel="tag">crf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/myBorn/" rel="tag">myBorn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/" rel="tag">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E6%96%87/" rel="tag">人文</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%95%E5%BE%8B/" rel="tag">法律</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag">自然语言处理</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">1</span></li></ul>
              </canvas>
          </div>
      </div>
      
      
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bool_tbb</span>
</div>
  <div class="powered-by">由 <a href="https://github.com/iiot-tbb" class="theme-link" rel="noopener" target="_blank">Tbb</a> 强力驱动 v1.0.0
  </div>
  <span class="post-meta-divider">|</span>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共47.5k字</span>
</div>


<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("11/11/2019 12:49:25");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = " 运行了 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>














  

  

  

  
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
