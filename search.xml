<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Bertology:说一说那些不为人知的细节</title>
    <url>/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>自 Transformer 模型的文章《Attention is all you need》这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕获能力，基于Transformer系列的backbone预训练模型在下游任务微调成一种常用的方法。</p>
<p>虽然每年有大量的前沿研究基于Bert这样的模型，但是很多文章—-表面是对模型进行了创新获得了不错的效果，究其本质，无非是扩大了模型的参数量，提高训练数据的质量和数量。因此，取得了更为理想的效果。关于为什么模型的效果好，以及每一层的神经网络到底是取得了如何的效果则没有系统的阐释。今天读到了一篇《A Primer in BERTology: What We Know About How BERTWorks》发表于Transactions of the Association for Computational Linguistics。这篇文章对于bert模型的一些细节进行了系统的论述，很全面，因此，读了一下，把它分享给大家。</p>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>众所周知，BERT已经在nlp领域大杀四方，但是对于“why it works well”相对来说没有那么多的研究。如此一来，想要更进一步做出更加solid的研究，需要对该模型有更多的了解。 相比较一些经典的backbone，比如说卷积神经网络(CNN)，在感性上的动机则没有特别明显。我们希望对其中的一些结构进行分析，使得模型中具体所做的事情更容易被人们所认识，但是，对于模型过于大的尺寸所造成的预训练不好做（大部分研究人员没有超多的显卡，以及预训练所耗费的经费十分多）且想要做消融实验并不容易。因此对于近些年来对解释bert的文章做一个综述，方便大家对于该模型的认识。</p>
<a id="more"></a>
<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>这篇文章主要介绍了一些已经被人们所理解的模型其中的部分结构，并且也同样强调了一些仍旧需要探索的地方。</p>
<p>首先，将要介绍一些对模型“linguistic”上的探索。比如说：目前关于 BERT 学习的语言和世界知识类型的证据，以及这些知识可能存储在模型中的位置和方式。<br>之后将介绍模型的技术方面，并提供当前方案的概述，改进 BERT 的架构、预训练和微调。<br>最后，讨论了过度参数化的问题、压缩 BERT 的方法以及作为模型分析技术的修剪的新生领域。</p>
<h2 id="BERT-模型概述"><a href="#BERT-模型概述" class="headerlink" title="BERT 模型概述"></a>BERT 模型概述</h2><p>先简要说一下BERT模型的基本结构，BERT是由相同的Transformer的编码器堆叠而成，其中编码器中最重要的成分是“多头注意力机制”，对于每一个“头”来说，它由三个矩阵进行矩阵乘法完成对于注意力的计算，这三个矩阵分别称为，“key”矩阵，“query”矩阵，“value”矩阵，对于输入文本中的每一个词（这里说词是指一个文本的在输入端所能分解成的最小组成单元，对于英文来说可以是一个词组，单词，或者亚词，或者是字母）通过三个矩阵的线性变换求得每个词的表示，然后对于每一个头的输出进行组合，最后通过全连接层得到最终的文本表示，当然在输入端到多头注意力端有一个残差连接结构。并且最后的输出是经过归一化处理操作所得到(按照一定的均值和方差得到的分布)。</p>
<p>通常情况下，BERT的工作流程分为以下两个阶段：预训练阶段和微调阶段，在预训练阶段，运用MLM（Masked Language Model）任务和NSP(Next sentence prediction)任务,在微调阶段，会在BERT输出的基础上添加一层或多层全连接神经网络。</p>
<p>上文提到，对于模型的输入表示来说，可以分解成词组，单词，亚词，或者是字母，BERT模型使用了亚词的表示方式，通过wordpiece算法将一个文本中的每个英文单词切分成亚词的大小，通过这样的方法既可以保留更多的语意信息，相比较切分成单词来说，可以缩小此表的大小。同时，还需要增加另外两个表示层，分别是表示单词在一句话中位置关系的postion编码，另外一个段编码，表示的是一种句子级别的相对位置关系。这最终的输入表示为这三种表示方式相加。另外在一段文本开始时在word编码层加入一个特殊的字段[CLS]位。在两句话间隔处加入[SEP]字段。</p>
<p>谷歌和抱抱脸提供了多种版本的BERT源代码（看源码是搞清模型细节的最有效方式），其中包括了“base”和“large”版本的模型（区别在于模型的大小）。</p>
<h2 id="BERT-到底学了个啥？"><a href="#BERT-到底学了个啥？" class="headerlink" title="BERT 到底学了个啥？"></a>BERT 到底学了个啥？</h2><p>介绍完一些基础的概念后，下面要进入正题了，BERT中的网络到底都学了什么知识呢？</p>
<p>许多研究着眼于编码在BERT权重中的知识。流行的方法包括 MLM 的填充式探测、自我注意权重分析以及使用不同BERT表示作为输入的探测分类器。</p>
<h3 id="句法知识"><a href="#句法知识" class="headerlink" title="句法知识"></a>句法知识</h3><p>BERT的表示实际上是分层的而不是线性的(lin et al(2019))，举一个例子，除了词序信息以外，还有一些句法树结构的东西，<br>Ten- ney et al. (2019b) and Liu et al. (2019a)，BERT 嵌入对有关词性、句法块和角色的信息进行编码。 （Vilares et al., 2020; Kim et al., 2020; Rosa and Mareˇcek, 2019)这篇文章表明，有足够的句法知识似乎在词嵌入的部分被表示从而恢复句法树。尽管探测分类器无法恢复句法树中远距离父节点的标签。<br>(Liu et al., 2019a). Warstadt and Bowman (2020)表明在四分之三的探测任务中存在层次结构的证据。</p>
<p>至于句法是如何表示的，句法结构似乎没有直接编码在自注意力权重中。Htut et al. (2019) 表明即使有根的标注，也无法从 BERT 头中提取完整的解析树。Jawahar et al. (2019)直接从自注意力权重中提取的依赖树的简要说明，但不提供定量评估的方法。</p>
<p>然而，句法信息可以从BERT的token 表示中恢复。.Hewitt and Manning (2019) 的工作可以从bert的token嵌入中学习到一个转换矩阵，使得他们在PennTreebank data 上恢复句法依存关系(see also Manning et al., 2020)。Jawahar et al. (2019) experimented 使用张量积分解网络（Tensor Product Decomposition Networks(McCoy et al., 2019a)）对 [CLS] token进行转换实验，得出的结论是依赖树是 5 种分解方案中的最佳匹配（尽管报告的 MSE 差异非常小）。Miaschi and Dell’Orletta (2020)使用连接的标记表示作为输入执行一系列句法探测实验。</p>
<p>请注意，所有这些方法都在寻找标准语言结构的证据，并为探索添加一些额外的知识。Wu et al. (2020) 在 MLM 任务中，基于测量一个词对预测序列中另一个词的影响，提出了一种无参数方法。如图所示:</p>
<p><img src="fig1.jpg" alt="avtar"></p>
<center>句法知识的无参数探测：共享句法子树的词在MLM预测中相互影响较大(Wu et al., 2020)</center>

<p>他们得出的结论是，BERT“自然地”学习了一些句法信息，尽管它与语言注释资源不太相似。</p>
<p>MLM 的填充式探测表明，BERT 在执行完形填空任务时会考虑主谓一致(Gold- berg, 2019; van Schijndel et al., 2019),即使对于无意义的句子和在主语和动词之间有干扰从句的句子(Gold- berg, 2019). (Warstadt et al. (2019))对负极性项目 (NPI) 的研究表明，与范围违规相比，BERT 能够更好地检测 NPI 的存在（例如“ever”）和允许使用它们的单词（例如“whether”）。</p>
<p>上述关于句法知识的主张被 BERT 不“理解”否定并且对格式错误的输入不敏感的证据所掩盖。特别是，即使是打乱词序、截断句子、删除主语和宾语，它的预测也没有改变。后者似乎更有可能，因为 Glavaš 和 Vuli´c (2020) 报告说，带有监督解析的中间微调步骤对下游任务的性能没有太大影响。</p>
<h3 id="语义知识"><a href="#语义知识" class="headerlink" title="语义知识"></a>语义知识</h3><p>迄今为止，更多的研究致力于 BERT 的句法知识而非语义现象。但是，我们确实有来自 MLM 探索性研究的证据表明 BERT 对语义角色有一些了解(Ettinger, 2019).BERT 甚至对与正确角色语义相关的语义角色的错误填充物表现出一些偏好，而不是那些不相关的语义角色（例如，给厨师小费比给知更鸟小费更好，但比给服务员小费更糟糕）。</p>
<p>Tenney et al. (2019b) 表明，BERT 对有关实体类型、关系、语义角色和原型角色的信息进行编码，因为可以使用探测分类器检测到这些信息。<br>(Wallace et al., 2019b)表明BERT对于数字信息表示十分吃力。加法和数字解码任务表明，BERT 不能很好地表示浮点数，并且无法从训练数据中泛化。问题的一部分是 BERT 的 wordpiece 标记化，因为相似值的数量可以划分为截然不同的词块。</p>
<p>开箱即用的BERT模型对于命名实体的替换是十分敏感的，例如，在共同引用任务中替换名称会改变85%的预测(Balasubramanian et al., 2020)。 这表明该模型实际上并没有形成命名实体的一般概念，尽管它在NER探测任务中的F1分数很高。(Tenney et al., 2019a). Broscheit (2019) 发现在维基百科实体链接上微调 BERT “教”它额外的实体知识，这表明它在维基百科的预训练期间没有吸收所有相关的实体信息。</p>
<h3 id="世界知识"><a href="#世界知识" class="headerlink" title="世界知识"></a>世界知识</h3><p>BERT 中捕获的有关常识知识的大部分证据来自使用它来提取此类知识的从业者。一项对 BERT 的直接探索性研究报告称，BERT 在语用推理和基于角色的事件知识方面存在困难(Ettinger, 2019)。<br>BERT 还挣扎在对象的抽象属性以及可能被假设而不是提及的视觉和感知属性这些问题上(Da and Kasai, 2019)。</p>
<p>BERT 的 MLM 组件通过填空很容易适应知识归纳（例如“猫喜欢追逐 [<em>_</em>]”）。Petroni et al. (2019) 表明，对于某些关系类型，普通 BERT 与依赖知识库的方法相比具有竞争力。如下图所示：</p>
<p><img src="fig2.jpg" alt="avatar"></p>
<center>Bert的世界知识</center>

<p>Roberts et al. (2020)对使用 T5 模型的开放域 QA 显示乐相似的效果。<br>(Raffel et al., 2019). Davison et al. (2019)表明它可以更好地推广到看不见的数据。为了检索 BERT 的知识，我们需要好的模板句子，并且有一些有关于它们的自动提取和扩充的工作(Bouraoui et al., 2019; Jiang et al., 2019b)。</p>
<p>但是，BERT 无法根据其世界知识进行推理。Forbes et al. (2019)表明 BERT 可以“猜测”许多对象的可供性和属性，但不能推理属性和可供性之间的关系。例如，它“知道”人可以走进房子，房子很大，但不能推断出房子比人大。</p>
<p>Zhou et al. (2020) and Richardson and Sabharwal (2019)还表明性能随着必要推理步骤的数量而下降。(Poerner et al., 2019)表明 BERT 的一些世界知识成功来自于学习刻板的联想。例如，一个听起来像是意大利名字的人被预测为意大利人，即使它是不正确的。</p>
<h3 id="一些局限性"><a href="#一些局限性" class="headerlink" title="一些局限性"></a>一些局限性</h3><p>第 3 节和第 4 节中的多项探索性研究报告称，BERT 拥有数量惊人的句法、语义和世界知识。然而，Tenney et al. (2019a)评论说，“我们的探测分类器没有观察到语言模式这一事实并不能保证它不存在，并且观察到一个模式并不能告诉我们它是如何使用的。”还有一个问题应该允许探测有多复杂(Liu et al., 2019a)。如果更复杂的探针可以恢复更多信息，那么我们在多大程度上仍然依赖原始模型？<br>此外，不同的探测方法可能会导致互补甚至相互矛盾的结论，这使得单一测试（如大多数研究）是不够的(Warstadt et al., 2019)。<br>给定的方法也可能有利于一个模型而不是另一个模型，例如，RoBERTa 用一种树提取方法跟踪 BERT，但用另一种方法领先(Htut et al., 2019)。<br>语言形式的选择也很重要(Kuznetsov and Gurevych, 2020).</p>
<p>鉴于这一切，另一种选择是专注于识别 BERT 在推理时实际依赖的内容。目前在架构块级别都在追求这个方向。在模型权重编码的信息水平上。遗忘探测(Elazar et al., 2020) 旨在专门从模型中删除某些信息并查看它如何改变性能，例如发现语言建模确实依赖于词性信息。</p>
<p>另一个方向是信息论探索。Pimentel et al. (2020) 将探测操作化为估计学习表示和给定语言属性之间的互信息，这强调了重点不应该放在表示中包含的信息量上，而应该放在从表示中提取它的难易程度上。Voita and Titov (2020) 将从给定表示中提取信息所需的工作量量化为传达探测大小和完成任务所需的数据量所需的最小描述长度。</p>
<h2 id="局部化语言的知识"><a href="#局部化语言的知识" class="headerlink" title="局部化语言的知识"></a>局部化语言的知识</h2><h3 id="BERT-嵌入"><a href="#BERT-嵌入" class="headerlink" title="BERT 嵌入"></a>BERT 嵌入</h3><p>对于像BERT这样的模型，“嵌入”（embedding）代表了从模型输出的向量，无论是传统的词嵌入例如word2vec还是 BERT这样的嵌入方式都可以当作是对词互信息的极大化(Kong et al., 2019)，但是后者的方式更偏向于对当前语境的理解——即每个词的表示向量取决于特定出现的上下文条件，也会包含一些关于上下文的信息(Miaschi and Dell’Orletta, 2020)。</p>
<p>一些研究表明，蒸馏的上下文嵌入可以更好地编码词汇语义信息（比如说它们会在基于词汇的任务上有更好的表现，比如说词汇的相似度）。将上下文化表示提取为静态表示的方法包括跨多个上下文聚合信息（(Akbik et al., 2019; Bommasani et al., 2020），编码几乎完全依赖给定单词含义的“语义漂白”句子（比如说，”This is &lt;&gt;”）(May et al., 2019),甚至使用上下文向量化的词嵌入去训练一个静态的词嵌入(Wang et al., 2020d).</p>
<p>当然，这其中是有一些可以探讨和提升的地方，比如说Ethayarajh (2019)测量每一层中相同单词的嵌入有多相似，在 BERT 靠输出端的层会产生更多特定于上下文的表示。他们同时也发现BERT 嵌入在向量空间中占据了一个狭窄的锥体，并且这种效果从较早的层到后面的层增加。也就是说，如果嵌入是方向一致的（各向同性的），两个随机词的余弦相似度将比预期的要高得多。由于各向同性被证明有利于静态词嵌入(Mu and Viswanath, 2018)，这可能是探索 BERT 的一个可能出更多成果的方向。</p>
<p>由于 BERT 嵌入是上下文化的，一个有趣的问题是它们在多大程度上捕捉到了多义词和同音词等现象。确实有证据表明 BERT 的上下文嵌入形成了与词义相对应的不同集群(Wiedemann et al., 2019; Schmidt and Hofmann, 2020)，使 BERT 在词义消歧任务中取得成功。然而，Mickus et al. (2019)）注意到，同一个词的表示取决于它出现的句子的位置，这可能是由于 NSP 目标函数所造成的（从语言学的角度来看，这是不可取的，并且可能是未来工作的有希望的途径。）。</p>
<p>上述讨论涉及词嵌入，但 BERT 通常用作句子或文本编码器。<br>生成用于分类的句子或文本表示的标准方法是使用 [CLS] 标记，但也正在讨论替代方案，包括连接标记表示（Tanaka et al., 2020），归一化平均值（(Tanaka et al., 2020)）和层激活<br>(Ma et al., 2019). 参见 Toshniwal et al. (2020)对跨任务和句子编码器的几种方法进行系统比较。</p>
<h3 id="关于自注意力头"><a href="#关于自注意力头" class="headerlink" title="关于自注意力头"></a>关于自注意力头</h3><p>一些研究提出了注意力头类型的分类。Raganato and Tiedemann (2018) ，讨论关注词嵌入本身，上一个/下一个<br>token和句子结束位置的token。Clark et al. (2019)<br>区分关注上一个/下一个token、[CLS]、[SEP]、标点符号和“广泛关注”序列。<br>如下图所示，Kovaleva et al. (2019)提出了注意力的5种机制。</p>
<p><img src="fig3.jpg" alt="avatar"></p>
<h4 id="具有语言功能的头部"><a href="#具有语言功能的头部" class="headerlink" title="具有语言功能的头部"></a>具有语言功能的头部</h4><p>上中显示的“异构”注意力模式可能在语言上是可解释的，并且许多研究都集中在识别自注意力头的功能上。特别是，一些BERT的头似乎专门应对一种特定类型的句法关系。Htut et al. (2019) and Clark et al. (2019) 研究称一些BERT的头相较于随机选择来收会将更多的注意力关注到一个特定的句法位置上。虽然他们在关于这方面的研究上采用了不同的数据集和方法，但是他们都发现有些头对于在obj角色里面的词的关注比位置基线要高。nsubj、advmod 和 amod 的证据因这两项研究而异（这块的obj,nsubj,advmod,amod是句法依存分析中对词汇的分类，有兴趣的同学可以去查阅相关资料）。在Voita et al. (2019b)的研究结果也支持这样的结论。Hoover et al. (2019)假设即使是像 dobj 这样的复杂依赖项也是由头组合而不是单个头编码的，但这项工作仅限于定性分析。Zhao and Bethard (2020)<br>专门寻找头部编码否定范围。</p>
<p>Clark et al. (2019)和Htut et al. (2019)都认为没有一个头具有完整的句法树信息，符合部分句法知识的证据。然而，Clark et al. (2019) 认为识别一个可以直接用作分类器的 BERT 头，以与基于规则的系统一样执行共指解析，这本身似乎需要相当多的句法知识。</p>
<p>Lin et al. (2019)目前的证据表明注意力权重是主谓一致和反身照应的弱指标(reflexive anaphora)。BERT 的自注意力权重不是作为应该相关的标记之间的强指针，而是接近统一的注意力基线，但对与心理语言学数据一致的不同类型的干扰物存在一定的敏感性。这与 Ettinger (2019) 的结论一致。</p>
<p>据我们所知，形态信息在 BERT 头中尚未解决，但Correia et al. (2019)在一些基础的Transformer模型种使用稀疏注意力变体。一些注意力的头表现出将使用BPE分解的亚词融合的效果。对于语义关系，(Kovaleva et al., 2019)称 self-attention head 编码核心帧-语义关系,(Cui et al., 2020)以及词典和常识关系。</p>
<p>自注意力作为一种可解释性机制的整体流行是由于这样一种想法：(Clark et al., 2019)注意力权重有一个明确的含义：在计算当前单词的下一个表示时，特定单词将被加权多少。(Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020)对这个问题也对这个问题进行了研究。在一个多层的模型中，往往注意力机制后面会进行非线性变化的激活。个别头捕获的特征并不能提供完整的语义理解。<br>很多文章也做了注意力可视化的工作，并且也有很多可视化的工具(Vig, 2019; Hoover et al., 2019)，可视化通常仅限于定性分析（通常带有精选示例）((Belinkov and Glass, 2019),并且不应被解释为确凿的证据。</p>
<h4 id="对特殊字段的注意力"><a href="#对特殊字段的注意力" class="headerlink" title="对特殊字段的注意力"></a>对特殊字段的注意力</h4><p>Kovaleva et al. (2019) 表明大多数自注意力头不直接编码任何非平凡的语言信息，至少是在GLUE上进行微调的任务(Wang et al., 2018）有这样的效果,因为只有不到 50% 的注意力头表现出“异构”模式。大部分头产生了垂直的特征（比如说更多注意到[CLS],[SEP],标点符号的token）。Clark et al. (2019)得出了相同的结论。这种冗余可能与过度参数化问题有关。</p>
<p>Kobayashi et al. (2020) 的研究表明，注意加权输入向量的归一化可以更直观地解释自我注意，减少对特殊标记的注意。然而，即使注意力权重被规范化，大多数的头部仍然不是潜在可解释的(Prasanna et al., 2020)。在许多注意力研究中，一种方法选择是关注词间注意力并简单地排除特殊标记，例如像. Lin et al. (2019) and Htut et al. (2019)的研究。然而，如果在推理时对特殊标记的关注确实很重要，那么纯粹从词间注意力模式得出结论似乎不是那么可信。</p>
<p>对于特殊的标记的理解往往并不是那么容易。比如对于[CLS]位的理解，通常是被认为对一句话的聚合理解(虽然所有的token多多少少都会有一些句子级别的语义信息)；在这种情况下，我们可能看不到例如词间注意力中的完整句法树，因为部分信息实际上包含在 [CLS] 中。</p>
<p>Clark et al. (2019)尝试使用基本 BERT 对 Wikipedia 段落进行编码，以特别考虑对特殊标记的关注，并指出早期层的头部更多地关注 [CLS]，中间层关注 [SEP]，最后一层关注句点和逗号。他们假设它的功能可能是“无操作”之一，如果注意力头的模式不适用于当前情况，则忽略头部的信号。例如，[SEP] 从第 5 层开始受到越来越多的关注，但它对预测的重要性下降。然而，然而，对于具体的下游任务，在对 [SEP] 和 [CLS] 进行微调后后将会得到很多注意力k (Kovaleva et al., 2019)。有趣的是，BERT 也非常关注标点符号，Clark et al. (2019)对此进行了研究，通过句号和逗号几乎与特殊标记一样频繁的事实来解释，因此模型可能会出于相同的原因学会依赖它们。</p>
<h3 id="BERT-层"><a href="#BERT-层" class="headerlink" title="BERT 层"></a>BERT 层</h3><p>对于BERT的第一个输入层将token，segment，位置编码的信息进行了融合。</p>
<p>理所当然地，较低层具有关于线性词序的最多信息。Lin et al. (2019)报告了 BERT-base 中第 4 层周围线性词序的知识减少，并且伴随了一些分层句子结构知识的增加，由预测标记索引、主要助动词和句子主语的探测任务检测到。</p>
<p>在具有不同任务、数据集和方法的研究中，人们普遍认为句法信息在 BERT 的中间层中最为突出。Hewitt and Manning (2019)从中间 BERT 层重建句法树深度最成功（base-BERT 为 6-9，BERT-large 为 14-19）。Goldberg (2019)发现在第八第九层主谓一致性上表现效果最好，Jawahar et al. (2019)进行的句法探测任务上也大致在模型的中间层的效果最好。中间 BERT 层中句法信息的突出在 Liu et al. (2019a)的研究中表明， Transformer 的中间层总体上表现最好，跨任务的迁移能力最强。如下图所示：</p>
<p><img src="fig4.jpg" alt="avatar"></p>
<center>BERT 层可迁移性（列对应于探测任务）</center>

<p>关于句法块则有一些相互矛盾的研究。Tenney et al. (2019a)总结到基本的句法信息会更早的出现在模型的浅层，而对于高层来说会更多捕捉到语义的特征。在典型的NLP pipline中如 词性标注，语义角色标签，依赖解析,验证了这一些结果。Jawahar et al. (2019)报告了模型低层对chunking任务更有用，而中间层对parsing任务更有用。但是与此同时，Liu et al. (2019a)的实验结果则表明，无论是词性标注还是chunking任务来说，都是中间层表现的最好。在Bert-base，Bert-large有着相同的结论。但是值得注意的是，他们用的探测任务套件不同。</p>
<p>对于BERT的最后一层来说，是最具任务针对性的层。比如在预训练阶段，这意味着它会更加针对MLM任务，并且在(Liu et al., 2019a)中解释了为啥中间层有更好的迁移性。在微调阶段，(Kovaleva et al., 2019)解释了为什么最后一层参数的变化最为剧烈。并且(Hao et al., 2019)解释了为什么对低层的网络权重改编为原始的值不会对模型的性能发生剧烈的影响。</p>
<p>Tenney et al. (2019a) 表明虽然句法信息出现在模型的早期并且可以局部化，但语义分布在整个模型中，这解释了为什么某些非平凡的示例在开始时得到错误解决但在后面的层中得到正确解决。<br>这是意料之中的：语义渗透到所有语言中，语言学家争论是否存在无意义的结构（Goldberg，2006，p.166-182）。但这衍生出了一个问题，即在 BERT 中堆叠更多的 Transformer 层实际上在语义知识的传播方面实现了什么，以及这是否有益。Tenney et al 比较了 BERT-base 和 BERT-large，发现累积得分增益的总体模式是相同的，只是在更大的模型中更加分散。</p>
<p>Note that Tenney et al. (2019a)的实验主要关注在了句子级别的语义关系，Cui et al. (2020)报告称对ConceptNet 语义关系的编码在早期层中是最差的，在向顶部转移的过程中增加。</p>
<p>Jawahar et al. (2019)假设表面特征出现在低层，句法特征出现在中间层，语义特征出现在高层。但他们的结论令人惊讶，因为本研究中只有一个语义任务实际上在最后一层达到顶峰，另外三个在中间达到顶峰，然后在最后一层显著降低。</p>
<h2 id="BERT的训练"><a href="#BERT的训练" class="headerlink" title="BERT的训练"></a>BERT的训练</h2><p>在这部分章节将展开对于BERT优化模型训练和结构的内容。</p>
<h3 id="模型结构的选择"><a href="#模型结构的选择" class="headerlink" title="模型结构的选择"></a>模型结构的选择</h3><p>Wang et al. (2019b)对BERT模型结构的研究进行了系统的研究，他尝试了不同的层数、头部和模型参数，改变了一个选项并冻结了其他选项。他们总结认为模型的头的数量并没有模型层数的数量造成的影响大。这和Voita et al. (2019b) and Michel et al. (2019) 的研究结果保持了一致，在Liu et al. (2019a)的研究中发现模型的中间层的可迁移性更强。较大的隐层表示大小始终更好，但增益因具体设置而异。</p>
<p>总的来说，对于注意力头的改变和层数的改变会表现出不一样的函数功能(Liu et al., 2019a)，对于初始层来说，表现出了任务不变性的效果(Hao et al., 2019)，并且输出的token表示也和初始的token embedding的表示最为相似(Brun- ner et al., 2020)。如果是这种情况，更深的模型有更多的能力来编码非特定任务的信息。</p>
<p>另一方面，在 vanilla BERT中似乎许多的自注意力头似乎学习到了相同模式(Kovaleva et al., 2019)。这就解释了为什么对它进行剪枝不会造成太多的影响。由此产生的问题是，我们可以通过有意识地鼓励多样化的自注意力机制模式走多远：从理论上讲，这意味着在相同数量的权重下增加模型中的信息量。Raganato et al. (2020) 表示对于以 Transformer模型位backbone的机器翻译任务，我们可以简单地预设我们已经知道模型会学习的模式，而不是从头开始学习它们。</p>
<p>Vanilla BERT 在自注意力部分和前反馈网络层部分是对称和平衡的，但是也不是必须是这样。</p>
<h3 id="对于训练方法的改进"><a href="#对于训练方法的改进" class="headerlink" title="对于训练方法的改进"></a>对于训练方法的改进</h3><p>Liu et al. (2019b) 证实了在训练过程中对于batchsize扩大的收益：对于8k大小的模型数量来说，无论是模型的困惑度还是在下游任务的表现上都有了明显的提升。并且他们推荐一些训练的参数。You et al. (2019) 报告称对于有32k大小的batchsize的模型训练，训练收敛的时间缩短并且在性能上没有损失。Zhou et al. (2019)观察到训练后的 [CLS] 标记的归一化稳定了训练并略微提高了文本分类任务的性能。</p>
<p>Gong et al. (2019) 注意到，虽然自注意力机制的模式在低层和高层表现出相似的效果，模型的训练可以以递归的方式完成。其中首先训练较浅的版本，然后将训练的参数复制到更深层。这种“热启动”可以在不牺牲性能的情况下使训练速度提高 25%。</p>
<h3 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h3><p>原始的BERT是一个双向的Transformer并且通过NSP和MLM任务进行预训练。许多研究通过对预训练目标函数进行优化从而整体改进BERT的表达能力，可以分为以下几个点:</p>
<ul>
<li><p>如何去mask？ Raffel et al. (2019) 系统地试mask率和mask的跨度长度。Liu et al. (2019b)提出了在一个训练epoch中的多种的mask方式。Baevski et al. (2019) 选择去mask掉一个序列中的所有token而不是随机采样的方式。Clinchant et al. (2019)通过将[unk]的token替换成MASK token的方式来帮助模型可以学习到更多的不在词表中单词的表示方式从而使得翻译任务更加受益。Song et al. (2020)通过对masked和unmasked的token进行调节从而极大化信息的总量，使得模型模型可以看到有多少token被遗漏。</p>
</li>
<li><p>去对什么做mask? mask可以被作用于全词而不是word piece(Devlin et al., 2019; Cui et al., 2019)。同样的，也可以mask一个具体跨距的文本信息(Joshi et al., 2020)。来预测到底其中有多少个词被mask掉(Lewis et al., 2019)。mask掉词组信息和命名实体可以提高对结构化知识的表示能力(Sun et al., 2019b)。</p>
</li>
<li><p>在文本中的什么位置做mask? Lample and Conneau (2019) 使用任意的text stream 而不是句子对并且采用子样本频繁输出类似于Mikolov et al. (2013)中的方法。Bao et al. (2020)使用特殊的伪掩码标记将标准自动编码 MLM 与部分自回归 LM 目标相结合。</p>
</li>
<li><p>掩码的替代方式。Raffel et al. (2019)实验了替换和丢弃spans的方式。Lewis et al. (2019)探索删除、填充、句子排列和文档旋转等方式，Sun et al. (2019c)预测一个token是否是大写并且它是否出现在文档中的其它位置。Yang et al. (2019) 训练输入序列中不同的词序排列，最大化原始词序的概率(参照n-gram 词序重建问题)。Clark et al. (2020)检测被生成网络替换的token而不是mask掉的token。</p>
</li>
<li><p>NSP任务的替代。移除 NSP任务并不会伤害到或者还有可能轻微提升模型的表达能力(Liu et al., 2019b; Joshi et al., 2020; Clinchant et al., 2019). Wang et al. (2019a),Wang et al. (2019a) and Cheng et al. (2019) 将NSP任务替换为预测上一句和下一句话。Lan et al. (2020a)用来自正面例子的句子而不是来自不同文档的句子来替换负面的 NSP 例子。ERNIE 2.0包含了句子重新排序和句子距离预测任务。Bai et al. (2020)用段落、句子和标记索引嵌入的组合替换 NSP 和标记位置嵌入。Li and Choi (2020)试验多方对话的话语顺序预测任务(并且采用的MLM任务不仅在utterancss的级别上并且还有整个对话上进行这样的操作)。</p>
</li>
<li><p>其它的任务。Sun et al. (2019c)提出同时学习 7 个任务，包括话语关系分类和预测片段是否与 IR（infromation retrival）相关。Guu et al. (2020) 在语言模型预训练中包含一个潜在知识检索器。Wang et al. (2020c)通过completion objective任务将MLM和知识进行结合。Glass et al. (2020)将MLM任务替换为span预测任务（就像抽取式的question-answering）任务类似，模型的输出预期位提供的答案不是从它的权重中产生的，而是从一个存在问题正确答案的不同的段落汇总得出（相关的搜索引擎查询片段）。</p>
</li>
</ul>
<p>另外一个有提升空间的是预训练所得到的数据。很多研究探索增加训练数据集体量所带来的收益(Liu et al., 2019b; Conneau et al., 2019; Baevski et al., 2019,还有增加训练的轮数(Liu et al., 2019b)所带来的变化。数据也并不一定需要原始的文本内容，有很多研究整合了明确的语言信息。不论是在句法(Sundararaman et al., 2019)还是在语义上(Zhang et al., 2020)。Wu et al. (2019b) 和 Kumar et al. (2020)增加了来自带标注的任务数据集的给定序列的标签。Schick and Schütze (2020)的工作分别学习了稀有词的表示。</p>
<p>虽然BERT已经经常用于世界知识的源头上，当然也会存在一些提供明确结构化知识的工作。一种方法是通过实体增强的方式。比如说，Peters et al. (2019a); Zhang et al. (2019)在训练BERT的过程中引入了实体的嵌入信息，Poerner et al. (2019) adapt采用了实体向量作为BERT输入表示。上文中也提到过，Wang et al. (2020c)不仅通过实体嵌入来引入知识，并且通过额外的预训练目标————知识库补全预训练目标。Sun et al. (2019b,c)调整标准 MLM 任务以MASK命名实体而不是随机词。Yin et al. (2020) 同时在文本数据上和线性化表数据进行MLM任务。Wang et al. (2020a)使用特定于任务的适配器增强 RoBERTa 的语言和事实知识。</p>
<p>预训练是训练 BERT 最昂贵的部分，所以知道它提供了多少好处可以说也是十分必要的了。<br>在某些任务上，随机初始化和微调的 BERT 比使用任务分类器和冻结权重的预训练 BERT 获得有竞争力或更高的结果(Kovaleva et al., 2019)。普遍研究者的共识是预训练在大多数情况下确实有帮助，但程度及其确切贡献需要进一步调查。Prasanna et al. (2020)发现大多数预训练BERT的权重在微调阶段是十分有用的，尽管有“更好”和“更差”的子网络。一种解释是，预训练的权重有助于微调的 BERT 找到泛化误差更小的更宽、更平坦的区域，这使得模型对过拟合更加鲁棒，如下图所示。</p>
<p><img src="fig5.jpg" alt="avatar"></p>
<center>与从头开始训练（左）相比，预训练的权重帮助 BERT 在 MRPC（右）的微调中找到更广泛的最优值</center>

<p>鉴于提议的修改数量众多且种类繁多，人们想知道它们各自的影响有多大。然而，随着总体上模型的朝着更大尺寸的方向上发展，因此想要做系统的消融实验将变得十分昂贵。大多数的新模型宣称在标准的benchmars上有更好的表现效果，但是但收益通常是微不足道的，模型稳定性和显著性检验的估计非常罕见。</p>
<h3 id="BERT的微调"><a href="#BERT的微调" class="headerlink" title="BERT的微调"></a>BERT的微调</h3><p>预训练+微调的方式是BERT工作流中至关重要的。前者提供任务无关的知识，后者会教模型更多依赖于对下游任务有用的的知识。</p>
<p>Kovaleva et al. (2019)没有发现 BERT 在 GLUE 任务上微调的情况（表明对 Universal Dependencies 进行微调确实会产生语法上有意义的注意力模式，但没有定量评估。）：在微调阶段，在3 epochs时，模型的最后两层有更多的变化，但是这种变化更多是使得自注意力集中于[SEP]<br>而不是语言上的可解释模式。可以理解为什么微调会增加对 [CLS] 的关注，而不是 [SEP]。如果 Clark et al. (2019)  等人认为 [SEP] 作为“无操作”指标是正确的，那么微调基本上会告诉 BERT 忽略什么。</p>
<p>几项研究探讨了改进 BERT 微调的可能方向：</p>
<ul>
<li><p>把更多网络层考虑进去: 在深层和输出层中学习信息的互补表示(Yang and Zhao, 2019),使用所有层的加权组合而不是最后一层(Su and Cheng, 2019; Kondratyuk and Straka, 2019),和层dropout((Kondratyuk and Straka, 2019)。</p>
</li>
<li><p>两阶段微调的方式是一种中间监督训练的方式在预训练和微调阶段。(Phang et al., 2019; Garg et al., 2020; Arase and Tsujii, 2019; Pruksachatkun et al., 2020; Glavaš and Vuli´c, 2020). Ben-David et al. (2020)提出了一种基于枢轴的 MLM 变体来微调 BERT 以进行域适应。</p>
</li>
<li><p>对抗性token扰动提高了模型的鲁棒性(Zhu et al., 2019)。</p>
</li>
<li><p>对抗正则化结合 Bregman Proximal Point Optimization 有助于缓解预训练的知识遗忘，从而防止 BERT 过度拟合下游任务(Jiang et al., 2019a)。</p>
</li>
<li><p>Mixout regularization即使对于少量训练示例，也能提高 BERT 微调的稳定性(Lee et al., 2019)。</p>
</li>
</ul>
<p>对于大型的模型来说，即使微调也是expensive的,但是t Houlsby et al. (2019) 表明它可以通过适配器模块成功逼近。他们以一小部分计算成本在 26 个分类任务上实现了具有竞争力的性能。BERT 中的适配器也用于多任务学习(Stickland and Murray, 2019)和跨语言迁移(Artetxe et al., 2019)。微调的替代方法是从冻结的表示中提取特征，但微调对 BERT 效果更好(Peters et al., 2019b)。</p>
<p>当前 NLP 的一个重大方法学挑战是，新模型的报告性能改进很可能在环境因素引起的变化范围内(Crane, 2018)。BERT模型也不例外。Dodge et al. (2020)报告由于权重初始化和训练数据顺序，BERT 在 GLUE 任务上微调的显着变化。他们还提出了尽早停止前景不佳的随机种子的训练的方法。</p>
<p>上述内容并没有论述了当前关于微调的全部内容，比如说Siamese 孪生结构，策略梯度训练，自动课程学习等。</p>
<h2 id="BERT模型的尺寸多大为好？"><a href="#BERT模型的尺寸多大为好？" class="headerlink" title="BERT模型的尺寸多大为好？"></a>BERT模型的尺寸多大为好？</h2><h3 id="过度参数化"><a href="#过度参数化" class="headerlink" title="过度参数化"></a>过度参数化</h3><p>以Transformer模型backbone保持数量级增长，1.1M大小的模型在Turing-NLG(Microsoft, 2020)时达到了170亿的参数量，与 GPT-3 的 1750亿(Brown et al., 2020) 相比都相形见绌，这个趋势引起了对自注意力计算复杂度的关切(Wu et al., 2019a)。环境问题的关切(Strubell et al., 2019; Schwartz et al., 2019)，模型结构公平比较的问题(Aßenmacher and Heumann, 2020),可复现性等问题。</p>
<p>人类语言极其复杂，而且可能需要更多的参数来完全描述，但是当前的模型并没有很好地利用它们已经拥有的参数。Voita et al. (2019b) 发现除了少数 Transformer 头外，其他所有的 Transformer 头都可以修剪，而不会显着降低性能。对于BERT来说，Clark et al. (2019)观察到同一层中的大多数头都显示出相似的自注意力模式（可能与一层中所有自注意力头的输出都通过相同的 MLP 的事实有关），也就解释了为什么Michel et al. (2019)可以对将多数层减少到一个头。</p>
<p>由于任务的特性，一些BERT的头，或者网络层不仅仅是多余的(Kao et al., 2020)，也同时会对下游任务有损害，(Michel et al., 2019)报告了在机器翻译任务中停用某些头的积极影响。(Baan et al., 2019)报告了在文本摘要任务中停用某些头的集体影响。(Kovaleva et al., 2019)报告了GLUE任务上的影响。此外，Ten-ney et al. (2019a)检查他们的结构探测分类器的累积增益，观察到在 8 个探测任务中的 5 个中，某些层会导致分数下降（通常在最后一层）。Gordon et al. (2020)发现可以剪枝 30-40% 的权重而不影响下游任务。</p>
<p>通常情况下，模型尺寸更大，表现效果越好(Liu et al., 2019a; Roberts et al., 2020)，但是在主谓一致探测的任务上，BERT-base竟然比BERT-large模型效果要好(Lin et al., 2019)。考虑到语言的复杂性和大量的预训练数据，不清楚为什么 BERT 最终会出现冗余的头和层。Clark et al. (2019)认为一个可能的原因是使用注意力的dropout，这会导致一些注意权重在训练期间被归零。</p>
<h3 id="比较的方法"><a href="#比较的方法" class="headerlink" title="比较的方法"></a>比较的方法</h3><p>鉴于以上关于参数过度化的证据，对于可以将BERT高效的压缩而不至于使得accuracy降低并不是一件奇怪的事情。对于现实场景的应用将是一个非常好的用途。一些好的工作结果可以总结如下表所示:</p>
<p><img src="fig6.jpg" alt="avatar"><br>主要方法是知识蒸馏、量化和剪枝。</p>
<p>(Hinton et al., 2014) 对于知识蒸馏的框架具体是使用了一个尺寸较小的学生网络去模仿更大的教师网络的行为。对于BERT来说，对于BERT来说，这是通过损失函数实验实现的(Sanh et al., 2019b; Jiao et al., 2019)。模仿教师网络各个部分的激活模式(Sun et al., 2019a)，和在预训练结对对知识的迁移(Turc et al., 2019; Jiao et al., 2019; Sun et al., 2020)或者是在微调阶段的知识迁移(Jiao et al., 2019)，McCarley et al. (2020)表明到目前为止，蒸馏对 GLUE 的效果比对阅读理解的效果更好，并且报告了结构化修剪和特定任务蒸馏相结合的 QA 任务的良好结果。</p>
<p>量化通过降低权重的精度来减少 BERT 的内存占用(Shen et al., 2019; Zafrir et al., 2019)。注意到这种策略通常需要兼容的硬件。</p>
<p>在上文中提到，可以禁用单个自注意力头和BERT层，而不会显着降低性能(Michel et al., 2019; Kovaleva et al., 2019; Baan et al., 2019)。剪枝是一种利用这一事实的压缩技术，通常通过将大型模型的某些部分归零来减少计算量。在结构化修剪中，结构化的模块被丢弃，就如LayerDrop一样(Fan et al., 2019)。在非结构化中，无论它们的位置如何，整个模型中的权重都会被剪枝，就像幅度剪枝（magnitude pruning）(Chen et al., 2020)一样，或运动修建(movement pruning)(Sanh et al., 2020)。</p>
<p>Prasanna et al. (2020) 和 Chen et al. (2020) 探索了彩票假说（lottery ticket hypothesis）视角下的 BERT。专门研究预训练的 BERT 中的“获胜”子网络。他们独立发现确实存在这样的子网络，并且不同任务的子网络之间的可转移性各不相同。</p>
<p>如果训练 BERT 的最终目标是压缩，Li et al. (2020)建议训练较大的模型并对其进行大量压缩，而不是对较小的模型进行轻微压缩。</p>
<p>其他技术包括将 BERT 的嵌入矩阵分解为更小的矩阵(Lan et al., 2020a), 渐进式模块替换(Xu et al., 2020)和中间编码器输出的动态消除(Goyal et al., 2020)。见 Ganesh et al. (2020)更详细地讨论了压缩方法。</p>
<h3 id="剪枝和模型分析"><a href="#剪枝和模型分析" class="headerlink" title="剪枝和模型分析"></a>剪枝和模型分析</h3><p>关于剪枝作为一种模型分析技术的讨论刚刚开始。基本思想是先验压缩模型由对预测有用的元素组成；因此，通过找出他们在做什么，我们可能会发现整个网络在做什么。例如，BERT 的头部似乎编码了帧语义关系，但禁用它们可能不会损害下游任务的性能Kovaleva et al. (2019);这表明这部分的知识并没有真正被用到。</p>
<p>对于基础的Transformer模型，Voita et al. (2019b)识别 self-attention 头的功能，然后检查它们中的哪些在修剪后幸存下来，发现句法头和位置头是最后一个去做的事情。对于BERT来说，Prasanna et al. (2020)朝相反的方向走：根据重要性分数进行修剪，并解释剩余的“好”子网络。特别是对于自注意力头，似乎并非只有可能编码非平凡语言模式的头才能在修剪后幸存下来。</p>
<p>这些研究中的模型和方法不同，所以证据是不确定的。特别指出的是，Voita et al. (2019b)发现在剪枝之前大部分头都是句法信息的，Prasanna et al. (2020) –大多数头部没有潜在的非平凡注意模式。</p>
<p>当前头部和层消融研究的一个重要限制(Michel et al., 2019; Koval- eva et al., 2019)是他们固有地假设某些知识包含在头/层中。然而，有证据表明在整个网络中分布的表示更加分散，例如在困难的语义解析任务上的准确性逐渐提高(Tenney et al., 2019a)，或者没有“一般”执行解析的头部(Clark et al., 2019; Htut et al., 2019)。如果是这样的话，那么对于单个组件的消融会对整个网络的权重共享机制产生损害。</p>
<h2 id="一些比较重要的研究方向"><a href="#一些比较重要的研究方向" class="headerlink" title="一些比较重要的研究方向"></a>一些比较重要的研究方向</h2><p>BERTology 显然已经发展了很多的理论研究探索了，但是，总的来说我们还是关于BERT的工作方式有很多疑问。接下来将要列举一些未来可能会有潜力的发展方向。</p>
<p><strong>需要口头推理的基准。</strong>,虽然BERT在许多 NLP 基准测试上取得突破，越来越多的分析论文表明，它的语言能力并不像看起来那么令人印象深刻。特别是，它被证明依赖于自然语言推理中的浅层启发式(McCoy et al., 2019b; Zellers et al., 2019; Jin et al., 2020)。阅读理解(Si et al., 2019a; Rogers et al., 2020; Sugawara et al., 2020; Si et al., 2019b; Yogatama et al., 2019)，<br>论证推理理解(Niven and Kao, 2019),和文本分类(Jin et al., 2020)。这种启发式甚至可以用于重建非公开可用的模型(Krishna et al., 2020)。与任何优化方法一样，如果数据中存在捷径，我们没有理由指望 BERT 不学习它。但是，如果开发不如建模重视，则不太可能出现无法用浅层启发式解决的更难的数据集工作。</p>
<p><strong>全方位语言能力的基准。</strong> 虽然语言模型似乎获得了大量关于语言的知识，但我们目前还没有针对语言知识的不同方面进行全面的压力测试。朝这个方向迈出的一步是“检查表”行为测试(Ribeiro et al., 2020)，2020年ACL的最佳论文。理想情况下，此类测试不仅可以测量错误，还可以测量灵敏度(Ettinger, 2019)。</p>
<p><strong>开发“教导”推理的方法。</strong>虽然大型预训练模型有很多知识，但如果需要在它们拥有的事实之上进行任何推理，它们通常会失败(Talmor et al., 2019)，例如，Richardson et al. (2020) 提出一种“教”BERT量化、条件、比较和布尔协调的方法。</p>
<p><strong>在推理的时候学习到发生了什么</strong> </p>
<p>大多数 BERT 分析论文侧重于模型的不同探测，目标是找出语言模型“知道”什么。然而，探测任务也有它本身的局限性。到目前为止，很少有论文专注于发现实际使用的知识。几个有希望的方向是“遗忘探测”(Elazar et al., 2020)，识别对给定任务的预测重要的特征(Arkhangelskaia and Dutta, 2019),并修剪模型以删除不重要的组件(Voita et al., 2019b; Michel et al., 2019; Prasanna et al., 2020).</p>
<h2 id="all-in-all"><a href="#all-in-all" class="headerlink" title="all in all"></a>all in all</h2><p>在一年多一点的时间里，BERT 已经成为 NLP 实验中无处不在的基线，并激发了许多分析该模型并提出各种改进的研究。这篇论文的意义在于可以激发大家关注那些更需要解决的问题上。</p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>bilstm+crf(batch)的实现</title>
    <url>/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="batch版本的条件随机场"><a href="#batch版本的条件随机场" class="headerlink" title="batch版本的条件随机场"></a>batch版本的条件随机场</h2><p>在上一篇文章中我们按照了 pytorch的官方教程复现了 简单版本的 条件随机场。基本了解了其中的代码实现。但是我们可以看到上一篇中的 crf需要反复的条件循环，同时也没有支持批处理的操作，如果实际应用的话，速度应该会慢很多，因此，在这里，我们实现了Batch版本的条件随机场。to be honest, batch版本相比较 傻瓜版本的实现有一丢丢的复杂，尤其是需要考虑大量的矩阵并行的操作，十分伤脑筋。还好，我参考了<a href="https://github.com/jtlin-sync/batch_bilstm_crf" target="_blank" rel="noopener">batch lstm+crf</a>代码并且认真的剖析其中的细节后，基本弄清了其中的实现细节。如果你对 这部分代码感兴趣，可以参考我下面的代码跑起来学一学。在这部分代码中，一些核心地方给出了注释，但是可能仍然不够清晰，希望你可以自己画画图搞清楚这些细节。<br><a id="more"></a></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> rnn_utils</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> xarray <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, tagset, embedding_dim, hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers, bidirectional, dropout, pretrained=None)</span>:</span></span><br><span class="line">        super(BiLSTM, self).__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.tagset_size = len(tagset)</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="keyword">if</span> pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.word_embeds = nn.Embedding.from_pretrained(pretrained)</span><br><span class="line">        self.lstm = nn.LSTM(</span><br><span class="line">            input_size=embedding_dim,</span><br><span class="line">            hidden_size=hidden_dim // <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> hidden_dim,</span><br><span class="line">            num_layers=num_layers,</span><br><span class="line">            dropout=dropout,</span><br><span class="line">            bidirectional=bidirectional,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)</span><br><span class="line">        self.hidden = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        init_hidden_dim = self.hidden_dim // <span class="number">2</span> <span class="keyword">if</span> self.bidirectional <span class="keyword">else</span> self.hidden_dim</span><br><span class="line">        init_first_dim = self.num_layers * <span class="number">2</span> <span class="keyword">if</span> self.bidirectional <span class="keyword">else</span> self.num_layers</span><br><span class="line">        self.hidden = (</span><br><span class="line">            torch.randn(init_first_dim, batch_size, init_hidden_dim).to(device),</span><br><span class="line">            torch.randn(init_first_dim, batch_size, init_hidden_dim).to(device)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">repackage_hidden</span><span class="params">(self, hidden)</span>:</span></span><br><span class="line">        <span class="string">"""Wraps hidden states in new Tensors, to detach them from their history."""</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(hidden, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> hidden.detach_()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tuple(self.repackage_hidden(h) <span class="keyword">for</span> h <span class="keyword">in</span> hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_input, batch_input_lens, batch_mask)</span>:</span></span><br><span class="line">        batch_size, padding_length = batch_input.size()</span><br><span class="line">        batch_input = self.word_embeds(batch_input)  <span class="comment"># size: #batch * padding_length * embedding_dim</span></span><br><span class="line">        batch_input = rnn_utils.pack_padded_sequence(</span><br><span class="line">            batch_input, batch_input_lens, batch_first=<span class="literal">True</span>)</span><br><span class="line">        batch_output, self.hidden = self.lstm(batch_input, self.hidden)</span><br><span class="line">        self.repackage_hidden(self.hidden)</span><br><span class="line">        batch_output, _ = rnn_utils.pad_packed_sequence(batch_output, batch_first=<span class="literal">True</span>)</span><br><span class="line">       </span><br><span class="line">        batch_output = batch_output.contiguous().view(batch_size * padding_length, <span class="number">-1</span>)</span><br><span class="line">       </span><br><span class="line">        batch_output = batch_output[batch_mask, ...]</span><br><span class="line">        </span><br><span class="line">        out = self.hidden2tag(batch_output)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span><span class="params">(self, batch_input, batch_input_lens, batch_mask, batch_target)</span>:</span></span><br><span class="line">        loss = nn.CrossEntropyLoss(reduction=<span class="string">'mean'</span>)</span><br><span class="line">        feats = self(batch_input, batch_input_lens, batch_mask)</span><br><span class="line">        batch_target = torch.cat(batch_target, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> loss(feats, batch_target)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, batch_input, batch_input_lens, batch_mask)</span>:</span></span><br><span class="line">        feats = self(batch_input, batch_input_lens, batch_mask)</span><br><span class="line">        val, pred = torch.max(feats, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 下面让我们看一下关于 batch 版本的 CRF</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tagset, start_tag, end_tag, device)</span>:</span></span><br><span class="line">        super(CRF, self).__init__()</span><br><span class="line">        self.tagset_size = len(tagset)</span><br><span class="line">        self.START_TAG_IDX = tagset.index(start_tag)</span><br><span class="line">        self.END_TAG_IDX = tagset.index(end_tag)</span><br><span class="line">        self.START_TAG_TENSOR = torch.LongTensor([self.START_TAG_IDX], device=device)</span><br><span class="line">        self.END_TAG_TENSOR = torch.LongTensor([self.END_TAG_IDX], device=device)</span><br><span class="line">        <span class="comment"># trans: (tagset_size, tagset_size) trans (i, j) means state_i -&gt; state_j</span></span><br><span class="line">        self.trans = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size, self.tagset_size)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># self.trans.data[...] = 1</span></span><br><span class="line">        self.trans.data[:, self.START_TAG_IDX] = <span class="number">-10000</span></span><br><span class="line">        self.trans.data[self.END_TAG_IDX, :] = <span class="number">-10000</span></span><br><span class="line">        self.device = device</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始状态</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_alpha</span><span class="params">(self, batch_size, tagset_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.full((batch_size, tagset_size, <span class="number">1</span>), <span class="number">-10000</span>, dtype=torch.float, device=self.device)</span><br><span class="line">    <span class="comment"># 做维特比解码的图，初始为word_size * tag_szie。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_path</span><span class="params">(self, size_shape)</span>:</span></span><br><span class="line">        <span class="comment"># Initialization Path - LongTensor + Device + Full_value=0</span></span><br><span class="line">        <span class="keyword">return</span> torch.full(size_shape, <span class="number">0</span>, dtype=torch.long, device=self.device)</span><br><span class="line">    <span class="comment"># 重新打包，将相同位置的词汇作为一个组。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_iter_legal_batch</span><span class="params">(self, batch_input_lens, reverse=False)</span>:</span></span><br><span class="line">        index = torch.arange(<span class="number">0</span>, batch_input_lens.sum(), dtype=torch.long) <span class="comment"># 初始了一个tensor[0,length_sum)</span></span><br><span class="line">        </span><br><span class="line">        packed_index = rnn_utils.pack_sequence(</span><br><span class="line">            torch.split(index, batch_input_lens.tolist())                 <span class="comment"># split 应该是要把它找到对应的索引</span></span><br><span class="line">        )                                                                 <span class="comment"># 再 把 它打包，具体来说是让列元素对其</span></span><br><span class="line">        <span class="comment">#print("pack_index",packed_index)</span></span><br><span class="line">        batch_iter = torch.split(packed_index.data, packed_index.batch_sizes.tolist()) <span class="comment"># 重新组batch,按照每个text文本词的对应位置</span></span><br><span class="line">        batch_iter = reversed(batch_iter) <span class="keyword">if</span> reverse <span class="keyword">else</span> batch_iter</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> batch_iter:</span><br><span class="line">            <span class="keyword">yield</span> idx, idx.size()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score_z</span><span class="params">(self, feats, batch_input_lens)</span>:</span></span><br><span class="line">        <span class="comment"># 模拟packed pad过程</span></span><br><span class="line">        tagset_size = feats.shape[<span class="number">1</span>]</span><br><span class="line">        batch_size = len(batch_input_lens)</span><br><span class="line">        alpha = self.init_alpha(batch_size, tagset_size) <span class="comment"># batch_size,tag_size,1</span></span><br><span class="line">        alpha[:, self.START_TAG_IDX, :] = <span class="number">0</span>  <span class="comment"># Initialization</span></span><br><span class="line">        <span class="keyword">for</span> legal_idx, legal_batch_size <span class="keyword">in</span> self._iter_legal_batch(batch_input_lens):</span><br><span class="line">            <span class="comment">#print(feats.shape) #batch_size,tag_size</span></span><br><span class="line">            feat = feats[legal_idx, ].view(legal_batch_size, <span class="number">1</span>, tagset_size)  <span class="comment"># </span></span><br><span class="line">            <span class="comment"># #batch * 1 * |tag| + #batch * |tag| * 1 + |tag| * |tag| = #batch * |tag| * |tag| 广播机制</span></span><br><span class="line">            legal_batch_score = feat + alpha[:legal_batch_size, ] + self.trans <span class="comment"># 每一列是一个被转移状态的i-j的分布</span></span><br><span class="line">            alpha_new = torch.logsumexp(legal_batch_score, <span class="number">1</span>).unsqueeze(<span class="number">2</span>) <span class="comment"># batch_size,tag_size,1</span></span><br><span class="line">            alpha[:legal_batch_size, ] = alpha_new</span><br><span class="line">        alpha = alpha + self.trans[:, self.END_TAG_IDX].unsqueeze(<span class="number">1</span>)</span><br><span class="line">        score = torch.logsumexp(alpha, <span class="number">1</span>).sum()</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score_sentence</span><span class="params">(self, feats, batch_target)</span>:</span></span><br><span class="line">        <span class="comment"># CRF Batched Sentence Score</span></span><br><span class="line">        <span class="comment"># feats: (#batch_state(#words), tagset_size)</span></span><br><span class="line">        <span class="comment"># batch_target: list&lt;torch.LongTensor&gt; At least One LongTensor</span></span><br><span class="line">        <span class="comment"># Warning: words order =  batch_target order</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_add_start_tag</span><span class="params">(target)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> torch.cat([self.START_TAG_TENSOR, target])</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_add_end_tag</span><span class="params">(target)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> torch.cat([target, self.END_TAG_TENSOR])</span><br><span class="line"></span><br><span class="line">        from_state = [_add_start_tag(target) <span class="keyword">for</span> target <span class="keyword">in</span> batch_target]</span><br><span class="line">        to_state = [_add_end_tag(target) <span class="keyword">for</span> target <span class="keyword">in</span> batch_target]</span><br><span class="line">      </span><br><span class="line">        from_state = torch.cat(from_state)  <span class="comment">#拼接成一维tensor</span></span><br><span class="line">        to_state = torch.cat(to_state)      <span class="comment">#同理拼接成一维tensor</span></span><br><span class="line">        trans_score = self.trans[from_state, to_state] <span class="comment"># 转移概率得分</span></span><br><span class="line">      </span><br><span class="line">        gather_target = torch.cat(batch_target).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        emit_score = torch.gather(feats, <span class="number">1</span>, gather_target)  <span class="comment"># 得到对应每一个标签位置的观测概率得分</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trans_score.sum() + emit_score.sum()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, feats, batch_input_lens)</span>:</span></span><br><span class="line">        word_size, tagset_size = feats.shape</span><br><span class="line">        </span><br><span class="line">        batch_size = len(batch_input_lens)</span><br><span class="line">        viterbi_path = self.init_path(feats.shape)  <span class="comment"># use feats.shape to init path.shape</span></span><br><span class="line">        alpha = self.init_alpha(batch_size, tagset_size) <span class="comment"># batch_size, tag_size ,1</span></span><br><span class="line">        alpha[:, self.START_TAG_IDX, :] = <span class="number">0</span>  <span class="comment"># Initialization</span></span><br><span class="line">        <span class="keyword">for</span> legal_idx, legal_batch_size <span class="keyword">in</span> self._iter_legal_batch(batch_input_lens):</span><br><span class="line">            feat = feats[legal_idx, :].view(legal_batch_size, <span class="number">1</span>, tagset_size)</span><br><span class="line">            legal_batch_score = feat + alpha[:legal_batch_size, ] + self.trans <span class="comment"># batch_size, tag_size,tag_size</span></span><br><span class="line">            alpha_new, best_tag = torch.max(legal_batch_score, <span class="number">1</span>)</span><br><span class="line">          </span><br><span class="line">           </span><br><span class="line">            alpha[:legal_batch_size, ] = alpha_new.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            viterbi_path[legal_idx, ] = best_tag</span><br><span class="line">        alpha = alpha + self.trans[:, self.END_TAG_IDX].unsqueeze(<span class="number">1</span>)</span><br><span class="line">        path_score, best_tag = torch.max(alpha, <span class="number">1</span>) <span class="comment"># batch_size,1</span></span><br><span class="line">     </span><br><span class="line">        path_score = path_score.squeeze()  <span class="comment"># path_score=#batch</span></span><br><span class="line"></span><br><span class="line">        best_paths = self.init_path((word_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> legal_idx, legal_batch_size <span class="keyword">in</span> self._iter_legal_batch(batch_input_lens, reverse=<span class="literal">True</span>):</span><br><span class="line">            best_paths[legal_idx, ] = best_tag[:legal_batch_size, ]  <span class="comment"># </span></span><br><span class="line">            backword_path = viterbi_path[legal_idx, ]  <span class="comment"># legal_size * |Tag|</span></span><br><span class="line">           </span><br><span class="line">            this_tag = best_tag[:legal_batch_size, ]  <span class="comment"># |legal_batch_size| * 1</span></span><br><span class="line">         </span><br><span class="line">            backword_tag = torch.gather(backword_path, <span class="number">1</span>, this_tag) <span class="comment"># backward_path size:legal_size * |Tag| </span></span><br><span class="line">            best_tag[:legal_batch_size, ] = backword_tag</span><br><span class="line">            <span class="comment"># never computing &lt;START&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># best_paths = #words</span></span><br><span class="line">        <span class="keyword">return</span> path_score.view(<span class="number">-1</span>), best_paths.view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, tagset, embedding_dim, hidden_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers, bidirectional, dropout, start_tag, end_tag, device, pretrained=None)</span>:</span></span><br><span class="line">        super(BiLSTM_CRF, self).__init__()</span><br><span class="line">        self.bilstm = BiLSTM(vocab_size, tagset, embedding_dim, hidden_dim,</span><br><span class="line">                             num_layers, bidirectional, dropout, pretrained)</span><br><span class="line">        self.CRF = CRF(tagset, start_tag, end_tag, device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        self.bilstm.hidden = self.bilstm.init_hidden(batch_size, device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_input, batch_input_lens, batch_mask)</span>:</span></span><br><span class="line">        feats = self.bilstm(batch_input, batch_input_lens, batch_mask)</span><br><span class="line">        score, path = self.CRF.viterbi(feats, batch_input_lens)</span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span><span class="params">(self, batch_input, batch_input_lens, batch_mask, batch_target)</span>:</span></span><br><span class="line">        feats = self.bilstm(batch_input, batch_input_lens, batch_mask)</span><br><span class="line">        gold_score = self.CRF.score_sentence(feats, batch_target)</span><br><span class="line">        forward_score = self.CRF.score_z(feats, batch_input_lens)</span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, batch_input, batch_input_lens, batch_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self(batch_input, batch_input_lens, batch_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mydataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataList)</span>:</span></span><br><span class="line">        self.datalst = dataList</span><br><span class="line">        self.word_to_ix = &#123;&#125;</span><br><span class="line">        self.tag_to_ix = &#123;<span class="string">"PAD"</span>:<span class="number">0</span>,<span class="string">"B"</span>:<span class="number">1</span>,<span class="string">"I"</span>:<span class="number">2</span>,<span class="string">"O"</span>:<span class="number">3</span>,START_TAG:<span class="number">4</span>,STOP_TAG:<span class="number">5</span>&#125;</span><br><span class="line">        self.data = []</span><br><span class="line">        self.label = []</span><br><span class="line">        self.input_lens = []</span><br><span class="line">        self.mask = []</span><br><span class="line">        <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> self.datalst:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word_to_ix:</span><br><span class="line">                    self.word_to_ix[word] = len(self.word_to_ix)+<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> sentence,tags <span class="keyword">in</span> self.datalst:</span><br><span class="line"></span><br><span class="line">            self.label.append([self.tag_to_ix[ids] <span class="keyword">for</span> ids <span class="keyword">in</span> tags])</span><br><span class="line">            self.data.append([self.word_to_ix[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence])</span><br><span class="line">            lens = len(sentence)</span><br><span class="line">            self.mask.append([<span class="literal">True</span>]*lens)  </span><br><span class="line">            self.input_lens.append(lens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.datalst)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index: int)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">"label"</span>:self.label[index],<span class="string">"data"</span>:self.data[index],<span class="string">"len"</span>:self.input_lens[index],<span class="string">'mask'</span>:self.mask[index]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(all_example)</span>:</span></span><br><span class="line"></span><br><span class="line">        data =   rnn_utils.pad_sequence(batch_first = <span class="literal">True</span>,padding_value = <span class="number">0</span>,sequences=[torch.tensor(dic[<span class="string">"data"</span>],dtype=torch.long) <span class="keyword">for</span> dic <span class="keyword">in</span> all_example]) </span><br><span class="line">        label = [torch.tensor(dic[<span class="string">"label"</span>]) <span class="keyword">for</span> dic <span class="keyword">in</span> all_example]</span><br><span class="line">        lens = torch.tensor([dic[<span class="string">"len"</span>] <span class="keyword">for</span> dic <span class="keyword">in</span> all_example],dtype=torch.long)</span><br><span class="line">        mask = rnn_utils.pad_sequence(batch_first = <span class="literal">True</span>,padding_value = <span class="literal">False</span>,sequences=[torch.tensor(dic[<span class="string">"data"</span>],dtype=torch.bool) <span class="keyword">for</span> dic <span class="keyword">in</span> all_example]).reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data,lens,mask,label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">START_TAG = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">STOP_TAG = <span class="string">"&lt;STOP"</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">training_data = [(</span><br><span class="line">    <span class="string">"the wall street journal reported today that apple corporation made money"</span>.split(),</span><br><span class="line">    <span class="string">"B I I I O O O B I O O"</span>.split()</span><br><span class="line">), (</span><br><span class="line">    <span class="string">"georgia tech is a university in georgia"</span>.split(),</span><br><span class="line">    <span class="string">"B I O O O O B"</span>.split()</span><br><span class="line">)]</span><br><span class="line">dataset = mydataset(training_data)</span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">2</span>,collate_fn=mydataset.collate_fn)</span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">"B"</span>:<span class="number">0</span>,<span class="string">"I"</span>:<span class="number">1</span>,<span class="string">"O"</span>:<span class="number">2</span>,START_TAG:<span class="number">3</span>,STOP_TAG:<span class="number">4</span>&#125;</span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">device = <span class="string">'cpu'</span></span><br><span class="line">model = BiLSTM_CRF(vocab_size=len(dataset.word_to_ix)+<span class="number">1</span>,tagset=[<span class="string">"PAD"</span>,<span class="string">"B"</span>,<span class="string">"I"</span>,<span class="string">"O"</span>,START_TAG,STOP_TAG],embedding_dim=<span class="number">4</span>,hidden_dim=<span class="number">4</span>,num_layers=<span class="number">1</span>,bidirectional=<span class="literal">True</span>,dropout=<span class="number">0.01</span>,start_tag=START_TAG,end_tag=STOP_TAG,device=<span class="string">'cpu'</span>)</span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr = <span class="number">0.1</span>, weight_decay= <span class="number">1e-4</span>)</span><br><span class="line">model.init_hidden(batch_size, device)</span><br><span class="line"><span class="keyword">for</span> times <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_info <span class="keyword">in</span> dataloader:</span><br><span class="line">        batch_input, batch_input_lens, batch_mask, batch_target = batch_info</span><br><span class="line">        loss_train = model.neg_log_likelihood(batch_input, batch_input_lens, batch_mask, batch_target)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss_train.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        print(loss_train.item())</span><br><span class="line">model.init_hidden(batch_size, device)</span><br><span class="line"><span class="keyword">for</span> batch_info <span class="keyword">in</span> dataloader:</span><br><span class="line">    batch_input, batch_input_lens, batch_mask, batch_target = batch_info</span><br><span class="line">    batch_pred = model.predict(batch_input, batch_input_lens, batch_mask)</span><br><span class="line">    print(batch_target)</span><br><span class="line">    print(batch_pred)</span><br><span class="line">    <span class="comment">#loss_test = loss_fn(batch_input, batch_input_lens, batch_mask, batch_target)</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>pytorch</tag>
        <tag>crf</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch版本的bilstm+crf的实现</title>
    <url>/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="CRF-代码实现"><a href="#CRF-代码实现" class="headerlink" title="CRF 代码实现"></a>CRF 代码实现</h2><p>最近没啥事情，想把CRF相关的内容再捋一捋，因此研究了Pytoch的CRF实现，代码如下展示，之后将会研究如何做batch版本的CRF。</p>
<p>关于CRF的实现，表面上和HMM模型基本一致，从我的角度来看，因为其观测矩阵的实现由模型给出，即 $P(Y|X)$，因此其展示的是判别模型。所以才认为其是CRF模型，<br>如果变成隐马尔可夫模型，需要建模$P(Y,X)$, 十分有趣，不同观测矩阵的给出方式决定了模型的类型。</p>
<a id="more"></a>
<p>关于代码实现部分，其关键点在于 前向概率的计算和解码的维特比算法部分。关于下文中借助了 Bi-LSTM作为CRF模型的观测矩阵的来源必不可少，这部分模型也可以由其它模型替换，比如 Transformer类型的模型。</p>
<p>对于该部分代码的讲解，我在知乎上看到一个比较好的回答，如果大家对这部分代码不是很熟悉，建议先去《统计学习方法》这本书上看一下具体的理论介绍，然后可以参考<a href="https://zhuanlan.zhihu.com/p/140479197" target="_blank" rel="noopener">PyTorch Bi-LSTM+CRF NER标注代码精读</a>这篇文章，通过理论和代码实现上的学习，我相信大家可以对该部分内容有一个较好的理解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">START_TAG = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">STOP_TAG = <span class="string">"&lt;STOP&gt;"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(vec)</span>:</span></span><br><span class="line">    _ , idx = torch.max(vec,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx.item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span><span class="params">(seq,to_ix)</span>:</span></span><br><span class="line">    idxs = [ to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq ]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs,dtype= torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减去最大值，防止在exp的时候溢出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(vec)</span>:</span></span><br><span class="line">    max_score = vec[<span class="number">0</span>,argmax(vec)]</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>,<span class="number">-1</span>).expand(<span class="number">1</span>,vec.size()[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,vocab_size,tag_to_idx,embedding_dim,hidden_dim)</span>:</span></span><br><span class="line">        super(BiLSTM_CRF,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding_dim = embedding_dim </span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.tag_to_ix = tag_to_idx</span><br><span class="line">        self.tagset_size = len(tag_to_idx)</span><br><span class="line"></span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim//<span class="number">2</span>,num_layers=<span class="number">1</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#隐藏层与tag层的映射维度变换</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim,self.tagset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#状态转移矩阵 元素i,j的值代表从j--&gt;i 的状态转移概率</span></span><br><span class="line">        self.trasitions = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size,self.tagset_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态转移加入条件，任何状态无法转移到起始状态</span></span><br><span class="line">        <span class="comment"># 且 任何状态无法从停止状态转移而来</span></span><br><span class="line">        self.trasitions.data[tag_to_idx[START_TAG],:] = <span class="number">-10000</span></span><br><span class="line">        self.trasitions.data[:,tag_to_idx[STOP_TAG]] = <span class="number">-10000</span></span><br><span class="line"></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对lstm网络层初始的ceil,hidden状态进行预设置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (torch.randn(<span class="number">2</span>,<span class="number">1</span>,self.hidden_dim //<span class="number">2</span>),</span><br><span class="line">                torch.randn(<span class="number">2</span>,<span class="number">1</span>,self.hidden_dim//<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span><span class="params">(self,feats)</span>:</span></span><br><span class="line"></span><br><span class="line">        init_alphas = torch.full((<span class="number">1</span>,self.tagset_size),<span class="number">-10000.</span>)</span><br><span class="line"></span><br><span class="line">        init_alphas[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历句子中的每一个词</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line"></span><br><span class="line">            alphas_t = [] <span class="comment"># 获取当前词对应的tag概率分布</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size): <span class="comment">#遍历所有的tag</span></span><br><span class="line">                <span class="comment"># 广播发射矩阵(观测矩阵)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 遍历得到当前的标签</span></span><br><span class="line"></span><br><span class="line">                emit_score = feat[next_tag].view(<span class="number">1</span>,<span class="number">-1</span>).expand(<span class="number">1</span>,self.tagset_size)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 转移概率的值为 第 i 个 条目从 i 过渡到 next_tag 的分数</span></span><br><span class="line">                <span class="comment"># </span></span><br><span class="line">                   </span><br><span class="line">                trans_score = self.trasitions[next_tag].view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">                next_tag_var = forward_var + trans_score + emit_score</span><br><span class="line"></span><br><span class="line">                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line">            </span><br><span class="line">            forward_var = torch.cat(alphas_t).view(<span class="number">1</span>,<span class="number">-1</span>) <span class="comment"># 将上一阶段的tag概率分布作为下一个阶段的初始状态概率分布</span></span><br><span class="line"></span><br><span class="line">        terminal_var = forward_var + self.trasitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line"></span><br><span class="line">        alpha = log_sum_exp(terminal_var)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lstm_features</span><span class="params">(self,sentence)</span>:</span></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">        embeds = self.word_embeds(sentence).view(len(sentence),<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        lstm_out, self.hidden = self.lstm(embeds, self.hidden)</span><br><span class="line"></span><br><span class="line">        lstm_out = lstm_out.view(len(sentence),self.hidden_dim)</span><br><span class="line"></span><br><span class="line">        lstm_feats = self.hidden2tag(lstm_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> lstm_feats</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span><span class="params">(self,feats,tags)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算  output feature 和tags之间的误差</span></span><br><span class="line"></span><br><span class="line">        score = torch.zeros(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]],dtype=torch.long),tags])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># score 的计算方式为 概率转移矩阵的对应tag之间的转移概率加上，观测矩阵对应位置x-y的概率值</span></span><br><span class="line">        <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(feats):</span><br><span class="line">            score = score + \</span><br><span class="line">                self.trasitions[tags[i+<span class="number">1</span>],tags[i]] +feat[tags[i+<span class="number">1</span>]]</span><br><span class="line">        score = score + self.trasitions[self.tag_to_ix[STOP_TAG],tags[<span class="number">-1</span>]]</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self,feats)</span>:</span></span><br><span class="line">        backpointers = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始 状态 变量</span></span><br><span class="line">        init_vvars = torch.full((<span class="number">1</span>,self.tagset_size), <span class="number">-10000.</span>)</span><br><span class="line">        init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        forward_var = init_vvars</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span>  feats:</span><br><span class="line">            bptrs_t = [] <span class="comment"># 保存当前节点的上一时间的节点转移概率最大值的节点</span></span><br><span class="line"></span><br><span class="line">            viterbivars_t = [] <span class="comment"># 保存当前的时间节点的概率分布</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size):</span><br><span class="line">                <span class="comment"># next_tag_var 保存了 当前时刻维特比变量转移到</span></span><br><span class="line">                <span class="comment"># 下一个时间节点tag的概率分布</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 这里没有考虑到观测概率矩阵的概率原因在于 最大值不依赖于观测矩阵</span></span><br><span class="line">                next_tag_var = forward_var + self.trasitions[next_tag]</span><br><span class="line"></span><br><span class="line">                best_tag_id = argmax(next_tag_var)</span><br><span class="line">                bptrs_t.append(best_tag_id)</span><br><span class="line">                viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            forward_var = (torch.cat(viterbivars_t) + feat ).view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">            backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转移至 STOP_TAG</span></span><br><span class="line">        ternminal_var = forward_var + self.trasitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        best_tag_id = argmax(ternminal_var)</span><br><span class="line">        path_score = ternminal_var[<span class="number">0</span>][best_tag_id]</span><br><span class="line"></span><br><span class="line">        best_path = [best_tag_id]</span><br><span class="line">        <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> reversed(backpointers):</span><br><span class="line">            best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">            best_path.append(best_tag_id)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将 START tag 从得到的状态序列中移除</span></span><br><span class="line">        start= best_path.pop()</span><br><span class="line">        <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG] <span class="comment">#检查正确性</span></span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> path_score,best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_liklihood</span><span class="params">(self,sentence,tags)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得观测概率矩阵</span></span><br><span class="line">        feats =  self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        forward_score = self._forward_alg(feats)</span><br><span class="line"></span><br><span class="line">        gold_score = self._score_sentence(feats,tags)</span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,sentence)</span>:</span></span><br><span class="line">        lstm_feats = self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        score,tag_seq = self._viterbi_decode(lstm_feats)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score ,tag_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">training_data = [(</span><br><span class="line">    <span class="string">"the wall street journal reported today that apple corporation made money"</span>.split(),</span><br><span class="line">    <span class="string">"B I I I O O O B I O O"</span>.split()</span><br><span class="line">), (</span><br><span class="line">    <span class="string">"georgia tech is a university in georgia"</span>.split(),</span><br><span class="line">    <span class="string">"B I O O O O B"</span>.split()</span><br><span class="line">)]</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">"B"</span>:<span class="number">0</span>,<span class="string">"I"</span>:<span class="number">1</span>,<span class="string">"O"</span>:<span class="number">2</span>,START_TAG:<span class="number">3</span>,STOP_TAG:<span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">model = BiLSTM_CRF(len(word_to_ix),tag_to_ix,EMBEDDING_DIM,HIDDEN_DIM)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr = <span class="number">0.01</span>, weight_decay= <span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>],word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]],dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> sentence,tags <span class="keyword">in</span> training_data:</span><br><span class="line"></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        sentence = prepare_sequence(sentence,word_to_ix)</span><br><span class="line"></span><br><span class="line">        targets = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> tags], dtype = torch.long)</span><br><span class="line"></span><br><span class="line">        loss = model.neg_log_liklihood(sentence,targets)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step() </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>],word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]],dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>pytorch</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>kmeans numpy实现</title>
    <url>/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="k-means算法"><a href="#k-means算法" class="headerlink" title="k-means算法"></a>k-means算法</h2><p>k-means作为基本的无监督机器学习算法，在一些面试场景下经常会被拉起来让做手动实现，算法本身其实不难，但在面试场景下复现的准确且简洁则十分重要，因此，本篇博文实现了k-means基本算法，希望大家都可以动手自己实现一遍以保证在需要手写的时候能够快速完成。关于k-means算法的基本原理，我相信大家都应该十分的清楚，因此在这个不多展开介绍，不懂的同学请自行百度或者Google。</p>
<a id="more"></a>
<p>具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#距离计算以欧式距离为例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eulDistance</span><span class="params">(vector1,vevtor2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.power(vector1-vevtor2,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机初始化中心点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(dataset,k)</span>:</span></span><br><span class="line">    numSamples, dim = dataset.shape</span><br><span class="line">    centerid = np.zeros((k,dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = int(random.uniform(<span class="number">0</span>,numSamples))</span><br><span class="line">        centerid[i,:] =dataset[index,:]</span><br><span class="line">    <span class="keyword">return</span> centerid</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(dataset,k)</span>:</span></span><br><span class="line">    numsamples = dataset.shape[<span class="number">0</span>]</span><br><span class="line">   </span><br><span class="line">    cluster = np.mat(np.zeros((numsamples,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    cluseter_flag = <span class="literal">True</span> <span class="comment">#判断是否簇发生了改变</span></span><br><span class="line"></span><br><span class="line">    centroids = initCentroids(dataset,k)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> cluseter_flag:</span><br><span class="line">        cluseter_flag =<span class="literal">False</span></span><br><span class="line">        <span class="comment">#更新每一个样本所属的簇</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numsamples):</span><br><span class="line">            minDis = float(<span class="string">'inf'</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> cls <span class="keyword">in</span> range(k):</span><br><span class="line">                dis_tmp = eulDistance(dataset[i,:],centroids[cls,:])</span><br><span class="line">                <span class="keyword">if</span> dis_tmp &lt; minDis:</span><br><span class="line">                    minDis  = dis_tmp</span><br><span class="line">                    minIndex = cls</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cluster[i,<span class="number">0</span>]!=minIndex:</span><br><span class="line">                cluseter_flag =<span class="literal">True</span></span><br><span class="line">                cluster[i] = minIndex,minDis**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新簇的中心位置</span></span><br><span class="line">        <span class="keyword">for</span> cls <span class="keyword">in</span> range(k):</span><br><span class="line"></span><br><span class="line">            pointers = dataset[np.nonzero(cluster[:,<span class="number">0</span>].A == cls)[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">            centroids[cls,:] = np.mean(pointers,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids, cluster <span class="comment">#返回簇中心点和对应每一个样本点的簇分配情况</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotCluster</span><span class="params">(dataSet,k,centroids,cluster)</span>:</span></span><br><span class="line">    numsamples ,dim = dataSet.shape</span><br><span class="line"></span><br><span class="line">    color = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numsamples):</span><br><span class="line">        indx = int(cluster[i,<span class="number">0</span>])</span><br><span class="line">        plt.plot(dataSet[i,<span class="number">0</span>],dataSet[i,<span class="number">1</span>],color[indx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        plt.plot(centroids[i,<span class="number">0</span>],centroids[i,<span class="number">1</span>],color[i],markersize =<span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">    plt.savefig(<span class="string">"./output.png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    dataset = []</span><br><span class="line">    f = open(<span class="string">'./ceshi.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        dataset.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line">    dataset = np.mat(dataset)                </span><br><span class="line">    k = <span class="number">4</span></span><br><span class="line">    cenid,cluster = kmeans(dataset,k)</span><br><span class="line"></span><br><span class="line">    plotCluster(dataset,k,cenid,cluster)</span><br></pre></td></tr></table></figure>
<p>输出如下图所示<br><img src="output.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Autograd 机制介绍</title>
    <url>/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="什么是-Autograd"><a href="#什么是-Autograd" class="headerlink" title="什么是 Autograd?"></a>什么是 Autograd?</h2><p>Autograd是反向自动微分系统。从概念上讲，autograd 记录一个图结构，记录执行操作时创建数据的所有操作，一个有向无环图，其叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，可以使用链式法则自动计算梯度。</p>
<a id="more"></a>
<p>更为具体的来说，Autograd方法是构造一个图结构的函数传播形式。它在计算网络的前向传播过程中构造了一个梯度的函数计算图。当完成前向过程后，进行反向传播时，计算每一个节点的梯度。在每一轮次都是即时构建这个图结构，并不用提前对图结构进行编码。按照python的函数流程，所构即所得。</p>
<h2 id="保存Tensor张量"><a href="#保存Tensor张量" class="headerlink" title="保存Tensor张量"></a>保存Tensor张量</h2><p>一些操作需要在前向传递期间保存中间结果，以便执行反向传递。例如，函数 $x\mapsto x^{2}$ 保存输入 $x$ 以计算梯度。</p>
<p>定义自定义 <code>Python</code> 函数时，您可以使用 <code>save_for_backward()</code> 在前向传递期间保存张量，并在后向传递期间使用 <code>saved_tensors</code> 检索它们。</p>
<p>对于<code>PyTorch</code>定义的操作（例如 <code>torch.pow()</code>），张量会根据需要自动保存。您可以通过查找以前缀 <code>_saved</code> 开头的属性来探索某个<code>grad_fn</code> 保存了哪些张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>)</span><br><span class="line">print(x.equal(y.grad_fn._saved_self))  <span class="comment"># True</span></span><br><span class="line">print(x <span class="keyword">is</span> y.grad_fn._saved_self)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>在前面的代码中，<code>y.grad_fn._saved_self</code> 指的是与 <code>x</code> 相同的 Tensor 对象。但情况可能并非总是如此。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.exp()</span><br><span class="line">print(y.equal(y.grad_fn._saved_result))  <span class="comment"># True</span></span><br><span class="line">print(y <span class="keyword">is</span> y.grad_fn._saved_result)  <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>在底层，为了防止引用循环，PyTorch 在保存时打包了张量，并将其解包到不同的张量中以供读取。在这里，您通过访问 y.grad_fn._saved_result 获得的张量是与 y 不同的张量对象（但它们仍然共享相同的存储）。一个张量是否会被打包到不同的张量对象中，取决于它是否是它自己的 grad_fn 的输出，这是一个可能会发生变化的实现细节，因此在操作时不应该依赖它。</p>
<h2 id="局部禁用梯度计算"><a href="#局部禁用梯度计算" class="headerlink" title="局部禁用梯度计算"></a>局部禁用梯度计算</h2><p>Python 有几种机制可以局部的禁用梯度计算：</p>
<p>要禁用整个代码块的梯度，有上下文管理器，如 no-grad 模式和 inference 模式。为了从梯度计算中更细粒度地排除子图，可以设置张量的 <code>requires_grad</code> 字段。<br>下面，除了讨论上述机制之外，我们还描述了 evaluate 模式<code>nn.Module.eval()</code>，这种方法实际上并不用于禁用梯度计算，但由于其名称，经常与这三种方法混淆。</p>
<h3 id="设置-required-grad"><a href="#设置-required-grad" class="headerlink" title="设置 required_grad"></a>设置 <code>required_grad</code></h3><p><code>requires_grad</code> 是一个标志，除非包含在 <code>nn.Parameter</code> 中，否则默认为 false，它允许从梯度计算中细粒度地排除子图。它在向前和向后传递中都生效：</p>
<p>在前向传递期间，只有至少一个输入张量需要 grad 时，才会将操作记录在后向图中。在后向传递 (.backward()) 期间，只有 requires_grad=True 的叶张量才会将梯度累积到它们的 <code>.grad</code> 字段中。</p>
<p>重要的是要注意，即使每个张量都有这个标志，设置它只对叶张量有意义（没有 grad_fn 的张量，例如，nn.Module 的参数）。非叶张量（具有 grad_fn 的张量）是具有与之关联的后向图的张量。因此，需要它们的梯度作为中间结果来计算需要 grad 的叶张量的梯度。从这个定义中，很明显所有非叶张量都会自动具有<code>require_grad=True</code>。</p>
<p>设置<code>require_grad</code>字段是有效的控制在你所用的模型中哪一部分的梯度需要被计算。比如说，当你使用预训练模型Bert类进行微调时，可以冻结这部分参数不跟随训练。当你设置<code>require_grad=False</code>时，你的这部分参数将不会在前向传播的过程中不会被保存记录，他们不会在后向传递中更新他们的 <code>.grad</code>字段，因为它们一开始就不会成为后向图的一部分。因为这是一种常见的模式，所以也可以使用 <code>nn.Module.requires_grad_()</code> 在模块级别设置<code>requires_grad</code>。当应用于模块时， .<code>requires_grad_()</code> 会影响模块的所有参数（默认情况下 <code>requires_grad=True</code> ）。</p>
<h3 id="梯度模式"><a href="#梯度模式" class="headerlink" title="梯度模式"></a>梯度模式</h3><p>除了设置 <code>requires_grad</code>之外，还可以从 Python 启用三种可能的模式，这些模式可以影响 PyTorch 中 autograd 计算的处理的方式：默认模式（grad 模式）、no-grad 模式和inference模式，所有这些都可以通过context manager和decorator。</p>
<h4 id="默认模式"><a href="#默认模式" class="headerlink" title="默认模式"></a>默认模式</h4><p>“默认模式”实际上是我们在没有启用其他模式（如 no-grad 和 inference 模式）时隐含的模式。与“no-grad mode”相比，默认模式有时也称为“grad mode”。<br>关于默认模式，最重要的一点是它是 <code>requires_grad</code> 生效的唯一模式。在其他两种模式下，<code>requires_grad</code> 总是被覆盖为 False。</p>
<h4 id="no-grad模式"><a href="#no-grad模式" class="headerlink" title="no-grad模式"></a>no-grad模式</h4><p>在no-grad模式下，所有的计算都表现为所有的输入都不需要有梯度，即使有 <code>require_grad=True</code> 的输入，no-grad 模式下的计算也永远不会记录在后向图中。<br>当您需要执行不应该由 autograd 记录的操作时启用 no-grad 模式，但您以后仍希望在 grad 模式下使用这些计算的输出。这个context-manager可以方便地禁用代码块或函数的梯度，而无需临时将张量设置为 requires_grad=False。</p>
<p>例如，在编写优化器时，no-grad 模式可能很有用：在执行训练更新时，您希望就地更新参数，而 autograd 不会记录更新。您还打算在下一次正向传递中使用更新的参数以 grad 模式进行计算。</p>
<p><code>torch.nn.init</code> 中的实现在初始化参数时也依赖于 no-grad 模式，以避免在就地更新初始化参数时进行 autograd 跟踪。</p>
<h4 id="Inference-模式"><a href="#Inference-模式" class="headerlink" title="Inference 模式"></a>Inference 模式</h4><p>推理模式是无梯度模式的极端版本​​。就像在 no-grad 模式下一样，推理模式下的计算不会记录在后向图中，但启用推理模式将允许 PyTorch 进一步加速您的模型。这种更好的运行时有一个缺点：在推理模式下创建的张量将无法用于退出推理模式后由 autograd 记录的计算。</p>
<p>当您执行不需要在后向图中记录的计算时启用推理模式，并且您不打算在稍后将由 autograd 记录的任何计算中使用在推理模式中创建的张量。</p>
<p>建议您在代码中不需要自动梯度跟踪的部分（例如，数据处理和模型评估）尝试推理模式。如果它为您的用例开箱即用，那将是免费的性能胜利。如果您在启用推理模式后遇到错误，请检查您是否没有在退出推理模式后由 autograd 记录的计算中使用在推理模式下创建的张量。如果在您的情况下无法避免此类使用，您可以随时切换回 no-grad 模式。</p>
<h3 id="评估-Evaluation-模式-nn-Module-eval"><a href="#评估-Evaluation-模式-nn-Module-eval" class="headerlink" title="评估(Evaluation)模式 (nn.Module.eval())"></a>评估(Evaluation)模式 (<code>nn.Module.eval()</code>)</h3><p>评估模式实际上并不是一种在本地禁用梯度计算的机制。在这里为什么要介绍它呢，因为它有时会被混淆为这样的机制。</p>
<p>在功能上，module.eval()（或等效的 module.train()）与 no-grad 模式和推理模式完全正交(即不存在相关性)。 model.eval() 如何影响您的模型完全取决于模型中使用的特定模块以及它们是否定义任何训练模式特定行为。</p>
<p>如果您的模型依赖于诸如 torch.nn.Dropout 和 torch.nn.BatchNorm2d 之类的模块，这些模块可能会因训练模式而有所不同，则您应当区分调用 model.eval() 和 model.train()。<br>例如，为了避免更新验证数据的 BatchNorm 运行统计信息。</p>
<p>建议您在训练时始终使用 model.train()，在评估模型（验证/测试）时始终使用 model.eval()，即使您不确定模型是否具有特定于训练模式的行为，因为您使用的某个模块可能会更新为在训练和评估模式下表现不同。</p>
<hr>
<h3 id="使用-autograd-进行原地操作"><a href="#使用-autograd-进行原地操作" class="headerlink" title="使用 autograd 进行原地操作"></a>使用 autograd 进行原地操作</h3><p>对于使用in-place的操作运算，pytorch这边不是非常建议使用，Autograd本身的缓冲区的释放和重用的效率是非常高的，极少情况下进行原地操作会显著降低内存的使用，</p>
<p>限制原地操作有两个主要原因：</p>
<ol>
<li>原地操作可能会覆盖计算梯度所需要的值。</li>
<li>原地操作需要重写计算图的操作实现，out-of-place的方法只需要分配新的内存给创造的对象，并且保存了对旧图的引用。在原地操作时，需要将所有输入的创建者更改为表示此操作的函数。这可能很棘手，特别是如果有许多张量引用相同的存储。比如，通过索引或转置创建。如果修改的输入的存储被任何其他张量引用，则in-place函数实际上会引发错误。</li>
</ol>
<h3 id="pytorch如何保证原地操作的正确性？"><a href="#pytorch如何保证原地操作的正确性？" class="headerlink" title="pytorch如何保证原地操作的正确性？"></a>pytorch如何保证原地操作的正确性？</h3><p> 每个张量都有一个版本计数器，每次它在任何操作中被标记为脏时都会递增。当一个函数保存任何用于后向计算的张量时，它们包含的张量的版本计数器也会被保存。当你访问<code>self.saved_tensor</code>时它就会被检查，如果它大于保存的值，则会引发错误。这可以确保如果您使用原地函数并且没有看到任何错误，则可以确保计算出的梯度是正确的。</p>
<h3 id="多线程-Autograd"><a href="#多线程-Autograd" class="headerlink" title="多线程 Autograd"></a>多线程 Autograd</h3><p>autograd 引擎负责运行计算反向传递所需的所有反向操作。下面将介绍在多线程环境中充分利用它的所有细节。<br>（仅与 PyTorch 1.6+ 相关，因为以前版本中的行为不同）</p>
<p>用户可以使用多线程代码（例如 Hogwild 训练）训练他们的模型，并且不会阻塞并发的反向计算，示例代码可以是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Define a train function to be used in different threads</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">()</span>:</span></span><br><span class="line">    x = torch.ones(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    y = (x + <span class="number">3</span>) * (x + <span class="number">4</span>) * <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    y.sum().backward()</span><br><span class="line">    <span class="comment"># potential optimizer update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># User write their own threading code to drive the train_fn</span></span><br><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    p = threading.Thread(target=train_fn, args=())</span><br><span class="line">    p.start()</span><br><span class="line">    threads.append(p)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> threads:</span><br><span class="line">    p.join()</span><br></pre></td></tr></table></figure>
<p>一些需要被了解的特性：</p>
<h3 id="CPU-上的并发"><a href="#CPU-上的并发" class="headerlink" title="CPU 上的并发"></a>CPU 上的并发</h3><hr>
<p>当您在 CPU 上的多个线程中通过 python 或 C++ API 运行<code>backward()</code> 或 grad() 时，将会看到额外的并发性，而不是在执行期间以特定顺序序列化所有向后调用（PyTorch 1.6 之前的行为）。</p>
<h4 id="Non-determinism"><a href="#Non-determinism" class="headerlink" title="Non-determinism"></a>Non-determinism</h4><p>如果您同时在多个线程上调用<code>backward()</code>，但使用共享输入（即Hogwild CPU 训练）。由于参数在线程之间自动共享，梯度累积可能在线程之间的反向调用中变得不确定，因为两个反向调用可能会访问并尝试累积相同的 <code>.grad</code>属性。这在技术上是不安全的，它可能会导致赛车状态，结果可能无法使用。</p>
<p>但是，如果您使用多线程方法来驱动整个训练过程但使用共享参数，那么这是预期的模式，使用多线程的用户应该牢记线程模型并且应该预期会发生这种情况。用户可以使用功能 API <code>torch.autograd.grad()</code> 来计算梯度，而不是使用 <code>backward()</code> 来避免不确定性。</p>
<h4 id="Graph-retaining"><a href="#Graph-retaining" class="headerlink" title="Graph retaining"></a>Graph retaining</h4><p>如果autograd图结构在多线程中被共享，例如图的第一部分运行在单线程上，图的第二部分采用多线程进行优化，在这种情况下，图结构的第一部分被优化，在这种情况下，不同的线程在同一个图上执行 <code>grad()</code>或 <code>backward()</code>可能会在一个线程的运行中破坏图，并且在这种情况下另一个线程将崩溃。 Autograd 将向用户发出错误，类似于在没有 <code>retain_graph=True</code> 的情况下两次调用 <code>backward()</code>，并让用户知道他们应该使用 <code>retain_graph=True</code>。</p>
<h4 id="在-Autograd节点上的线程安全。"><a href="#在-Autograd节点上的线程安全。" class="headerlink" title="在 Autograd节点上的线程安全。"></a>在 Autograd节点上的线程安全。</h4><p>由于 Autograd 允许调用者线程驱动其向后执行以实现潜在的并行性，因此我们必须确保 CPU 上的线程安全，并行反向传播共享 GraphTask 的部分/全部。</p>
<p>由于 GIL，自定义 Python <code>autograd.function</code> 是自动线程安全的。对于内置 C++ Autograd 节点（例如 AccumulateGrad、CopySlices）和自定义 autograd::Function，Autograd 引擎使用线程互斥锁来保护可能具有状态写入/读取的 autograd 节点上的线程安全。</p>
<h4 id="C-hooks上没有线程安全"><a href="#C-hooks上没有线程安全" class="headerlink" title="C++ hooks上没有线程安全"></a>C++ hooks上没有线程安全</h4><p>Autograd 依赖于用户编写线程安全的 C++ hooks。如果您希望hooks在多线程环境中正确应用，则需要编写适当的线程锁定代码以确保hooks是线程安全的。</p>
<hr>
<h2 id="对于复数的Autograd"><a href="#对于复数的Autograd" class="headerlink" title="对于复数的Autograd"></a>对于复数的Autograd</h2><p>简要概括:</p>
<ol>
<li>当你对具有复数域或者共域的函数$f(z)$进行微分时，梯度是在假设函数是更大的实值损失函数$g(input)=L$的一部分的情况下计算的，<br>$\frac{\partial L}{\partial z^{<em>}}$($z^{</em>}$代表 $z$ 的共轭)。其负值正是梯度下降算法中使用的最陡下降方向。因此说有的优化器对于复数参数都是开箱即用的。</li>
<li>此约定与 TensorFlow 的复杂微分约定相匹配，但与 JAX(计算$\frac{\partial L}{\partial z}$) 不同。</li>
<li>如果您有一个内部使用复数操作的实对实函数，则此处的约定无关紧要：您将始终获得与仅使用实数操作实现相同的结果。</li>
</ol>
]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>Autograd</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer语言模型训练</title>
    <url>/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code> module.<br>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><br>The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as<br><code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code>)<br>to draw global dependencies between input and output. The <code>nn.Transformer</code> module is now highly modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>in this tutorial) can be easily adapted/composed.</p>
<a id="more"></a>
<p><img src="transformer_architecture.jpg" alt="avatar"></p>
<h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sz=<span class="number">10</span></span><br><span class="line">mask=(torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">mask</span><br><span class="line">mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre><p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p>
<script type="math/tex; mode=display">\begin{aligned}\begin{bmatrix}\text{A} & \text{B}& \text{C} & \ldots & \text{X} & \text{Y} & \text{Z}\end{bmatrix}\Rightarrow\begin{bmatrix}\begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F} \end{bmatrix} &\begin{bmatrix}\text{G}\\\text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &\begin{bmatrix}\text{M} \\\text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &\begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}\end{bmatrix}\end{aligned}</script><p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure>
<pre><code>/home/bool_tbb/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.
  warnings.warn(&#39;{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;.format(self.__class__.__name__), UserWarning)
/home/bool_tbb/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.
  warnings.warn(&#39;Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;, UserWarning)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = TEXT.numericalize([train_txt.examples[<span class="number">0</span>].text])</span><br></pre></td></tr></table></figure>
<p>Functions to generate input and target sequence</p>
<p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>we’d get the following two Variables for <code>i</code> = 0:</p>
<p><img src="transformer_input_target.png" alt="avatar"></p>
<p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_batch(train_data,<span class="number">0</span>)[<span class="number">0</span>][:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[    3,    25,  1849,   570,     7,     5,     5,  9258,     4,    56,
             0,     7,     6,  6634,     4,  6603,     6,     5,    65,    30],
        [   12,    66,    13,  4889,   458,     8,  1045,    21, 19094,    34,
           147,     4,     0,    10,  2280,  2294,    58,    35,  2438,  4064],
        [ 3852, 13667,  2962,    68,     6, 28374,    39,   417,     0,  2034,
            29,    88, 27804,   350,     7,    17,  4811,   902,    33,    20],
        [ 3872,     5,     9,     4,   155,     8,  1669,    32,  2634,   257,
             4,     5,     5,    11,  4568,  8205,    78,  5258,  7723, 12009],
        [  884,    91,   963,   294,     4,   548,    29,   279,    37,     4,
           391,    31,     4,  2614,   948, 13583,   405,   545,    15,    16],
        [   12,    25,     5,     5,  1688,     0,    39,    59,  8785,     0,
             6,    13,  3026,    43,    11,     6,     0,   349,  3134,  4538],
        [    3,     6,    82,  1780,    21,     6,  2158,     4,     8,     8,
            27,  1485,     0,   194,    96,   195,  3545,   101,  1150,  3486],
        [    3,    25,    13,   885,     4,  6360,    15,   670,     0,    13,
            26,    17,     5,   417,   894,    10,     5,     5,  2998,    27],
        [20003,   190,    33,  1516,  1085,    34,   680,  3597,  2475,   664,
            47,    11,   127,    63,     6,    46, 24995,    72, 10190,    26],
        [   86,  9076, 10540,     6,     9,    74,   198,     7,     6,    17,
          3134,  5312,     4,     4,     3, 25509,     5,  2034,     5,    86]])
</code></pre><h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = nn.DataParallel(model)</span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code>is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code><br>function to scale all the gradient together to prevent exploding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Loop over epochs. Save the model if the validation loss is the best<br>we’ve seen so far. Adjust the learning rate after each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">100</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<pre><code>| epoch   1 |   200/ 2981 batches | lr 4.07 | ms/batch  9.71 | loss  5.39 | ppl   218.21
| epoch   1 |   400/ 2981 batches | lr 4.07 | ms/batch  9.50 | loss  5.39 | ppl   220.00
| epoch   1 |   600/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.20 | ppl   181.36
| epoch   1 |   800/ 2981 batches | lr 4.07 | ms/batch  9.60 | loss  5.26 | ppl   193.18
| epoch   1 |  1000/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.23 | ppl   186.05
| epoch   1 |  1200/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.26 | ppl   192.45
| epoch   1 |  1400/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.29 | ppl   197.86
| epoch   1 |  1600/ 2981 batches | lr 4.07 | ms/batch  9.60 | loss  5.33 | ppl   206.42
| epoch   1 |  1800/ 2981 batches | lr 4.07 | ms/batch  9.59 | loss  5.27 | ppl   193.88
| epoch   1 |  2000/ 2981 batches | lr 4.07 | ms/batch  9.70 | loss  5.30 | ppl   200.64
| epoch   1 |  2200/ 2981 batches | lr 4.07 | ms/batch  9.64 | loss  5.17 | ppl   176.64
| epoch   1 |  2400/ 2981 batches | lr 4.07 | ms/batch  9.62 | loss  5.26 | ppl   192.57
| epoch   1 |  2600/ 2981 batches | lr 4.07 | ms/batch  9.63 | loss  5.28 | ppl   195.69
| epoch   1 |  2800/ 2981 batches | lr 4.07 | ms/batch  9.64 | loss  5.21 | ppl   182.35
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 30.36s | valid loss  5.55 | valid ppl   256.32
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 3.87 | ms/batch  9.73 | loss  5.26 | ppl   192.78
| epoch   2 |   400/ 2981 batches | lr 3.87 | ms/batch  9.65 | loss  5.27 | ppl   194.78
| epoch   2 |   600/ 2981 batches | lr 3.87 | ms/batch  9.68 | loss  5.08 | ppl   160.59
| epoch   2 |   800/ 2981 batches | lr 3.87 | ms/batch  9.67 | loss  5.14 | ppl   171.46
| epoch   2 |  1000/ 2981 batches | lr 3.87 | ms/batch  9.68 | loss  5.10 | ppl   164.78
| epoch   2 |  1200/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.14 | ppl   171.25
| epoch   2 |  1400/ 2981 batches | lr 3.87 | ms/batch  9.71 | loss  5.18 | ppl   177.40
| epoch   2 |  1600/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.23 | ppl   186.59
| epoch   2 |  1800/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.16 | ppl   173.63
| epoch   2 |  2000/ 2981 batches | lr 3.87 | ms/batch  9.61 | loss  5.19 | ppl   179.61
| epoch   2 |  2200/ 2981 batches | lr 3.87 | ms/batch  9.61 | loss  5.06 | ppl   158.22
| epoch   2 |  2400/ 2981 batches | lr 3.87 | ms/batch  9.66 | loss  5.14 | ppl   170.97
| epoch   2 |  2600/ 2981 batches | lr 3.87 | ms/batch  9.63 | loss  5.16 | ppl   173.44
| epoch   2 |  2800/ 2981 batches | lr 3.87 | ms/batch  9.62 | loss  5.10 | ppl   163.57
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 30.54s | valid loss  5.44 | valid ppl   231.52
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 3.68 | ms/batch  9.74 | loss  5.15 | ppl   172.66
| epoch   3 |   400/ 2981 batches | lr 3.68 | ms/batch  9.81 | loss  5.16 | ppl   174.57
| epoch   3 |   600/ 2981 batches | lr 3.68 | ms/batch  9.76 | loss  4.98 | ppl   145.22
| epoch   3 |   800/ 2981 batches | lr 3.68 | ms/batch  9.69 | loss  5.04 | ppl   154.24
| epoch   3 |  1000/ 2981 batches | lr 3.68 | ms/batch  9.92 | loss  5.02 | ppl   150.68
| epoch   3 |  1200/ 2981 batches | lr 3.68 | ms/batch  9.75 | loss  5.05 | ppl   156.65
| epoch   3 |  1400/ 2981 batches | lr 3.68 | ms/batch  9.81 | loss  5.08 | ppl   161.32
| epoch   3 |  1600/ 2981 batches | lr 3.68 | ms/batch  9.87 | loss  5.13 | ppl   168.46
| epoch   3 |  1800/ 2981 batches | lr 3.68 | ms/batch  9.73 | loss  5.06 | ppl   158.11
| epoch   3 |  2000/ 2981 batches | lr 3.68 | ms/batch  9.78 | loss  5.09 | ppl   162.57
| epoch   3 |  2200/ 2981 batches | lr 3.68 | ms/batch  9.80 | loss  4.97 | ppl   143.40
| epoch   3 |  2400/ 2981 batches | lr 3.68 | ms/batch  9.84 | loss  5.05 | ppl   156.10
| epoch   3 |  2600/ 2981 batches | lr 3.68 | ms/batch  9.78 | loss  5.07 | ppl   158.92
| epoch   3 |  2800/ 2981 batches | lr 3.68 | ms/batch  9.80 | loss  5.01 | ppl   149.25
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 30.91s | valid loss  5.46 | valid ppl   234.33
-----------------------------------------------------------------------------------------
</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>
<pre><code>=========================================================================================
| End of training | test loss  5.48 | test ppl   238.72
=========================================================================================
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'./transformer_net.pth'</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_dict=model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(model_dict)</span><br></pre></td></tr></table></figure>
<pre><code>torch.nn.modules.module._IncompatibleKeys
</code></pre><p> <a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model" target="_blank" rel="noopener">REFERENCE:SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT</a>. </p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>pytorch</tag>
        <tag>Deep Learning</tag>
        <tag>Transformer</tag>
        <tag>Languaage Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/07/29/Transformer/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h4 id="注意力计算公式"><a href="#注意力计算公式" class="headerlink" title="注意力计算公式"></a>注意力计算公式</h4><script type="math/tex; mode=display">A = Softmax(Q*K^T/\sqrt{d})*V</script><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[-1.6685, -1.7979,  0.0747]],

         [[ 1.1604,  1.1415,  0.4631]],

         [[ 1.6218, -1.3112, -0.6065]]],


        [[[ 0.2836, -0.8159, -0.4028]],

         [[-0.0721, -0.3244,  0.2214]],

         [[-0.9558,  0.5414, -0.4869]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 0.5522, -3.4192,  0.7006]],

         [[-1.5651, -1.0705,  1.7866]],

         [[-2.1893, -0.2521,  0.2480]]],


        [[[ 0.4621,  1.0492,  0.5085]],

         [[-0.3847, -1.9930,  1.6604]],

         [[-1.0364, -0.3537,  1.5496]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.matmul(x,y.transpose(<span class="number">-1</span>,<span class="number">-2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 5.2783]],

         [[-2.2107]],

         [[-3.3705]]],


        [[[-0.9298]],

         [[ 1.0419]],

         [[ 0.0446]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3, 3, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=nn.Softmax()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop=nn.Dropout(<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop(torch.randn(<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 0.0000, -1.8719,  0.5233, -0.0000],
        [-0.0000,  0.6212,  0.2304, -0.1491],
        [-1.5584, -2.3030,  0.0000, -0.8582]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s(torch.FloatTensor([<span class="number">1</span>,-np.inf,<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-33-50788e72e9da&gt;:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  s(torch.FloatTensor([1,-np.inf,3]))





tensor([0.1192, 0.0000, 0.8808])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#计算注意力</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention_dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(ScaledDotProductAttention,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(attention_dropout)</span><br><span class="line">        self.softmax = nn.Softmax(dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,q,k,v,scale=None,attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        attention = torch.matmul(q,k.transpose(<span class="number">-2</span>,<span class="number">-1</span>)) <span class="comment"># 计算 Q*K^T ,交换最后两个维度的数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line">            attention = attention * scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mask attention. The attentions between the masked words and</span></span><br><span class="line">        <span class="comment"># other words are set to negative infinity</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention = attention.masked_fill_(attn_mask,-np.inf) </span><br><span class="line">        <span class="comment"># 这里掩码会把 Q*K^T里需要被掩盖的部分换成-inf 这样在softmax里该数值就变为零</span></span><br><span class="line">        <span class="comment"># 在Encoder里 需要掩盖住填充的0  在Decoder里除了掩盖住填充的0外 还要掩盖住后面的词</span></span><br><span class="line"></span><br><span class="line">        attention = self.softmax(attention)</span><br><span class="line">        attention = self.dropout(attention)</span><br><span class="line">        context = torch.matmul(attention,v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure>
<h4 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.Linear(<span class="number">10</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Linear(in_features=10, out_features=10, bias=True)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[ 0.4640,  0.5466, -0.6880,  0.1568],
         [ 0.8788,  0.9843, -0.4244, -1.5735],
         [ 0.1039,  1.2114,  0.7816, -0.8735]],

        [[ 1.1619, -2.5654,  0.5679, -1.1354],
         [-0.9004,  0.5074,  1.4977, -0.5807],
         [-1.3787,  0.7510, -1.1061,  1.2569]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.unsqueeze(<span class="number">2</span>).repeat(<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>).size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3, 10, 4])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># compute multi heads attention</span></span><br><span class="line">    <span class="comment"># 多头注意力的本质是由多个Wq,Wk,Wv计算出多组 Q,K,V从而得到多个向量 </span></span><br><span class="line">    <span class="comment"># 这里实现的方式是 由一个大的Wq,Wk,Wv 计算出一组大的Q,K,V 再把这个Q,K,V分成若干个</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_modl=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(MultiHeadAttention,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.dim_per_head = d_modl // num_heads <span class="comment">#计算每个头的维度</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.linear_k = nn.Linear(d_modl, d_modl)</span><br><span class="line">        self.linear_v = nn.Linear(d_modl, d_modl)</span><br><span class="line">        self.linear_q = nn.Linear(d_modl, d_modl)</span><br><span class="line"></span><br><span class="line">        self.dot_product_attention = ScaledDotProductAttention(dropout)</span><br><span class="line">        self.linear_final = nn.Linear(d_modl,d_modl)</span><br><span class="line">        self.norm = nn.LayerNorm(d_modl)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, keys, values, queries, attn_mask=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        residual = queries</span><br><span class="line">        batch_size = keys.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#generate keys,values and queries from inputs</span></span><br><span class="line">        keys = self.linear_k(keys) <span class="comment"># 计算Wk * E(输入词向量) = K</span></span><br><span class="line">        values = self.linear_v(values) <span class="comment"># Wv * E  = V</span></span><br><span class="line">        queries = self.linear_q(queries) <span class="comment">#Wq *E =Q</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#以下做的就是将Q,K,V分别拆分成num_head个 q,k,v</span></span><br><span class="line">        keys = keys.view(batch_size , <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>) </span><br><span class="line">        values = values.view(batch_size, <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        queries = queries.view(batch_size, <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>,self.num_heads,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        scale = (keys.size(<span class="number">-1</span>)) ** <span class="number">-0.5</span></span><br><span class="line">        <span class="comment">#计算注意力</span></span><br><span class="line">        context = self.dot_product_attention(queries,keys,values,scale,attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将多个头的输出向量拼接合并</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>,<span class="number">2</span>).contiguous() \</span><br><span class="line">                  .view(batch_size,<span class="number">-1</span>,self.num_heads * self.dim_per_head)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization and residual network</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(residual+self.linear_final(context)) <span class="comment"># linear 将拼接够的多头 进行信息融合和映射回d维度</span></span><br></pre></td></tr></table></figure>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(\frac{pos}{1000^{2i/d_{model}}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)} = cos(\frac{pos}{1000^{2i/d_{model}}})</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">0</span>,<span class="number">10</span>).unsqueeze(<span class="number">1</span>).size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">div_term = torch.exp(torch.arange(<span class="number">0.</span>,<span class="number">512</span>,<span class="number">2</span>)*-(math.log(<span class="number">10000.0</span>)/<span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">div_term.size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([256])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe = torch.randn(<span class="number">20</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 9.0318e-01,  0.0000e+00,  1.2352e-01,  0.0000e+00, -3.0140e-01,
          0.0000e+00, -3.6465e-01,  0.0000e+00, -5.0365e-01,  0.0000e+00],
        [-4.5975e-01,  0.0000e+00,  8.5064e-01,  0.0000e+00, -2.6547e+00,
          0.0000e+00,  7.4937e-01,  0.0000e+00, -4.1507e-01,  0.0000e+00],
        [-1.4702e+00,  0.0000e+00,  4.7715e-01,  0.0000e+00,  8.0542e-01,
          0.0000e+00, -4.0687e-01,  0.0000e+00, -7.3654e-01,  0.0000e+00],
        [ 1.2496e+00,  0.0000e+00,  1.0493e+00,  0.0000e+00,  1.4115e+00,
          0.0000e+00, -4.0402e-01,  0.0000e+00,  1.9959e-01,  0.0000e+00],
        [ 4.1005e-01,  0.0000e+00, -1.3749e+00,  0.0000e+00, -9.4356e-02,
          0.0000e+00, -2.5279e-01,  0.0000e+00,  1.3641e+00,  0.0000e+00],
        [ 3.0355e-01,  0.0000e+00, -7.0061e-01,  0.0000e+00, -6.3308e-01,
          0.0000e+00,  7.0820e-02,  0.0000e+00, -6.3141e-02,  0.0000e+00],
        [-1.7276e+00,  0.0000e+00,  7.1022e-01,  0.0000e+00, -3.7692e-01,
          0.0000e+00,  5.7131e-01,  0.0000e+00, -1.0790e+00,  0.0000e+00],
        [-1.9643e+00,  0.0000e+00, -8.7474e-01,  0.0000e+00, -1.2753e+00,
          0.0000e+00,  2.8921e-01,  0.0000e+00, -1.4253e+00,  0.0000e+00],
        [ 8.4792e-01,  0.0000e+00,  2.9655e-02,  0.0000e+00, -9.0477e-02,
          0.0000e+00,  3.1047e-01,  0.0000e+00,  1.8603e+00,  0.0000e+00],
        [-5.7733e-01,  0.0000e+00, -2.1318e-01,  0.0000e+00, -2.9424e-01,
          0.0000e+00,  5.5969e-01,  0.0000e+00,  5.9077e-01,  0.0000e+00],
        [-9.6322e-01,  0.0000e+00,  8.8474e-01,  0.0000e+00,  2.2378e-01,
          0.0000e+00, -6.0010e-01,  0.0000e+00, -3.6576e-01,  0.0000e+00],
        [ 8.8694e-01,  0.0000e+00,  2.8291e-02,  0.0000e+00, -6.5218e-01,
          0.0000e+00, -3.9719e-01,  0.0000e+00, -8.0203e-01,  0.0000e+00],
        [ 4.1978e-01,  0.0000e+00, -2.4290e-01,  0.0000e+00,  7.7798e-02,
          0.0000e+00, -9.2004e-01,  0.0000e+00,  5.3866e-01,  0.0000e+00],
        [-1.0515e+00,  0.0000e+00, -1.0967e+00,  0.0000e+00, -1.0951e+00,
          0.0000e+00,  2.9280e-01,  0.0000e+00, -9.3913e-01,  0.0000e+00],
        [ 8.6279e-01,  0.0000e+00,  4.4137e-01,  0.0000e+00,  2.5958e-01,
          0.0000e+00,  7.3830e-01,  0.0000e+00,  7.2514e-01,  0.0000e+00],
        [ 1.5696e+00,  0.0000e+00, -6.6977e-01,  0.0000e+00, -1.4154e+00,
          0.0000e+00,  1.1696e+00,  0.0000e+00,  2.2280e-01,  0.0000e+00],
        [-1.2376e+00,  0.0000e+00, -1.3173e-01,  0.0000e+00,  1.9464e-01,
          0.0000e+00,  2.0106e-01,  0.0000e+00, -1.9465e-01,  0.0000e+00],
        [-8.8660e-01,  0.0000e+00, -1.7934e-01,  0.0000e+00,  1.1574e+00,
          0.0000e+00,  4.0144e-01,  0.0000e+00, -1.7495e-03,  0.0000e+00],
        [ 6.2252e-01,  0.0000e+00, -3.1496e-01,  0.0000e+00,  6.6546e-01,
          0.0000e+00, -1.8034e-01,  0.0000e+00, -7.8079e-01,  0.0000e+00],
        [ 7.3892e-01,  0.0000e+00,  1.0642e+00,  0.0000e+00, -1.8440e-01,
          0.0000e+00, -1.8549e+00,  0.0000e+00, -1.6177e+00,  0.0000e+00]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe[:,<span class="number">1</span>::<span class="number">2</span>]=<span class="number">0</span><span class="comment">#从第二个维度的第一个数据开始，为2表示每两个取其中第一个，简单说就是隔一行取一个</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">2</span>]=[<span class="number">1</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>[1, 0, 3]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe[:,<span class="number">0</span>::<span class="number">8</span>].size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([20, 2])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#compute position encoding</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(PositionalEncoding,self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_seq_len,d_model) <span class="comment">#初始化位置向量</span></span><br><span class="line">        position = torch.arange(<span class="number">0.</span>,max_seq_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>,d_model,<span class="number">2</span>)*-(math.log(<span class="number">10000.0</span>)/d_model)) <span class="comment">#计算分母</span></span><br><span class="line"></span><br><span class="line">        pe[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment">#计算位置编码向量里偶数位子的数值</span></span><br><span class="line">        pe[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment">#计算位置编码里奇数位置的数值</span></span><br><span class="line"></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">"pe"</span>,pe)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        x = x + Variable(self.pe[:,:x.size(<span class="number">1</span>)],requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h4 id="前向-层归一"><a href="#前向-层归一" class="headerlink" title="前向+层归一"></a>前向+层归一</h4><script type="math/tex; mode=display">Out = Layernorm(x + W_2*ReLu(W_1+bias)+bias)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalWiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="comment">#前向传播+residual connection</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(PositionalWiseFeedForward,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.w1 = nn.Linear(d_model,ffn_dim)</span><br><span class="line">        self.w2 = nn.Linear(ffn_dim,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        output = self.w2(F.relu(self.w1(x)))</span><br><span class="line">        <span class="comment"># layer normalization and residual network</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x+self.dropout(output))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2018</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(EncoderLayer,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class="line">        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        context = self.attention(x,x,x,attn_mask)</span><br><span class="line">        output = self.feed_forward(context)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Encoder,self).__init__()</span><br><span class="line">        <span class="comment">#以下代码是建立num_layer层 </span></span><br><span class="line">        self.encoder_layers = nn.ModuleList(</span><br><span class="line">                            [EncoderLayer(d_model,num_heads,ffn_dim,dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = PositionalEncoding(d_model, max_seq_len,dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, seq_embedding)</span>:</span></span><br><span class="line"></span><br><span class="line">        embedding = seq_embedding(x)</span><br><span class="line">        output = self.pos_embedding(embedding)</span><br><span class="line">        self_attention_mask = padding_mask(x,x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> encoder <span class="keyword">in</span> self.encoder_layers:</span><br><span class="line">            output = encoder(output,self_attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.norm(output)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(DecoderLayer,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class="line">        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_inputs, enc_outputs, self_attn_mask = None,context_attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        dec_ouput  = self.attention(dec_inputs, dec_inputs, dec_inputs ,self_attn_mask)</span><br><span class="line"></span><br><span class="line">        dec_ouput = self.attention(enc_outputs, enc_outputs,dec_ouput, context_attn_mask)</span><br><span class="line"></span><br><span class="line">        dec_ouput = self.feed_forward(dec_ouput)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_ouput</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model  = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 )</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Decoder,self).__init__()</span><br><span class="line">        self.device = device</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        self.decoder_layers = nn.ModuleList(</span><br><span class="line">            [DecoderLayer(d_model,num_heads,ffn_dim,dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line"></span><br><span class="line">        self.seq_embedding = nn.Embedding(vocab_size, d_model, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.pos_embedding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line">        self.linear = nn.Linear(d_model, vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, enc_output, seq_embedding, context_attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        embedding = seq_embedding(inputs)</span><br><span class="line">        output =  embedding + self.pos_embedding(embedding)</span><br><span class="line"></span><br><span class="line">        self_attention_padding_mask = padding_mask(inputs, inputs)</span><br><span class="line">        seq_mask = sequence_mask(inputs).to(self.device)</span><br><span class="line">        self_attn_mask = torch.gt((self_attention_padding_mask+seq_mask), <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> decoder <span class="keyword">in</span> self.decoder_layers:</span><br><span class="line">            output = decoder(output, enc_output,self_attn_mask,context_attn_mask)</span><br><span class="line"></span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">nn.Embedding??</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#Build transformer model</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 stack_layers= <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.2</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(vocab_size, max_len,num_layers,d_model,num_heads,ffn_dim,dropout)</span><br><span class="line">        self.decoder = Decoder(vocab_size, max_len,device, num_layers,d_model,num_heads, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size,d_model)</span><br><span class="line">        self.linear = nn.Linear(d_model, vocab_size, bias = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#self.softmax = nn.Softmax(dim = 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src_seq, dec_tgt,dec_in)</span>:</span>                           <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        context_attn_mask_dec = padding_mask(dec_tgt, src_seq)</span><br><span class="line"></span><br><span class="line">        en_output = self.encoder(src_seq,self.embedding)</span><br><span class="line"></span><br><span class="line">        dec_output = self.decoder(dec_tgt, en_output,self.embedding,context_attn_mask_dec)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span><span class="params">(seq_k, seq_q)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pad sentence</span></span><br><span class="line">    len_q = seq_q.size(<span class="number">1</span>)</span><br><span class="line">    pad_mask = seq_k.eq(<span class="number">0</span>)</span><br><span class="line">    pad_mask = pad_mask.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>,len_q,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pad_mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">padding_mask(inputs,inputs)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True]],

        [[False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True]],

        [[False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span><span class="params">(seq)</span>:</span></span><br><span class="line"></span><br><span class="line">    batch_size , seq_len = seq.size()</span><br><span class="line">    mask = torch.triu(torch.ones((seq_len, seq_len),dtype = torch.uint8),<span class="comment">#上三角矩阵，加上diagnoal</span></span><br><span class="line">                      diagonal = <span class="number">1</span>)</span><br><span class="line">    mask = mask.unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>,<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sequence_mask(inputs)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Language Model</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP学习笔记</title>
    <url>/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="NLP中如何发掘模型的可解释性"><a href="#NLP中如何发掘模型的可解释性" class="headerlink" title="NLP中如何发掘模型的可解释性"></a>NLP中如何发掘模型的可解释性</h2><p>可解释性在AI的模型设计中十分重要。需要防止模型存在偏见和缺陷带来的伦理问题，并且帮助决策者理解如何正确地使用我们的模型。越是严苛的场景，越需要模型提供证明它们是如何运作且避免错误的证据。如实时性较强的无人驾驶领域，黑盒模型无法让人们信服其工作的安全性。</p>
<p>通常深度学习模型就像一个黑匣子，它能预测出很好的结果，但是你并不知道它为什么会预测出这样的结果。想知道它是如何工作的，那么得尝试打开这个黑匣子，解释模型的意义十分必要。</p>
<a id="more"></a>
<h3 id="现有方法："><a href="#现有方法：" class="headerlink" title="现有方法："></a>现有方法：</h3><h4 id="通用性思路："><a href="#通用性思路：" class="headerlink" title="通用性思路："></a>通用性思路：</h4><p>建模前：选用可解释性模型，如：决策树模型、线性回归、逻辑回归、广义线性回归、广义加性模型、贝叶斯实例模型等<br>建模后：使用可解释性方法，主要是针对具有黑箱性质的深度学习模型而言的，主要分为以下几类的工作：隐层分析方法、 模拟/代理模型、敏感性分析方法</p>
<p><a href="https://arxiv.org/abs/1602.04938" target="_blank" rel="noopener">LIME：通过局部线性逼近解释模型预测的方法</a></p>
<p><a href="https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf" target="_blank" rel="noopener">激活最大化：了解哪种输入模式产生最大的模型响应</a></p>
<p><a href="https://distill.pub/2017/feature-visualization/" target="_blank" rel="noopener">特征可视化：</a></p>
<p><a href="http://www.interpretable-ml.org/nips2017workshop/papers/04.pdf" target="_blank" rel="noopener">将DNN层嵌入到低维可解释的空间中:</a></p>
<p><a href="https://deepmind.com/blog/cognitive-psychology/" target="_blank" rel="noopener">采用认知心理学的方法：</a></p>
<p>不确定性估计方法</p>
<p>对于NLP，BERT模型的可视化</p>
<p><a href="https://www.jianshu.com/p/df7906a2a28e" target="_blank" rel="noopener">这里是一篇详细介绍BERT可视化的文章</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/78745325" target="_blank" rel="noopener">面向可解释的NLP：北大、哈工大等提出文本分类的生成性解释框架</a></p>
<h3 id="在Seq2Seq和注意力机制中如何可视化模型细节"><a href="#在Seq2Seq和注意力机制中如何可视化模型细节" class="headerlink" title="在Seq2Seq和注意力机制中如何可视化模型细节"></a>在Seq2Seq和注意力机制中如何可视化模型细节</h3><p><a href="https://zhuanlan.zhihu.com/p/51428657" target="_blank" rel="noopener">大致参考</a></p>
<h3 id="对抗样本能否运用到自然语言处理模型中"><a href="#对抗样本能否运用到自然语言处理模型中" class="headerlink" title="对抗样本能否运用到自然语言处理模型中"></a>对抗样本能否运用到自然语言处理模型中</h3><p><a href="https://www.secrss.com/articles/11247" target="_blank" rel="noopener">AI最前线：清华大学开源对抗样本必读论文列表</a></p>
<p>自然语言处理方面的研究在近几年取得了惊人的进步，深度神经网络模型已经取代了许多传统的方法。但是，当前提出的许多自然语言处理模型并不能够反映文本的多样特征。因此，许多研究者认为应该开辟新的研究方法，特别是利用近几年较为流行的对抗样本生成和防御的相关研究方法。</p>
<p>使用对抗样本生成和防御的自然语言处理研究可以基本概括为以下三种：1. 用未察觉的扰动迷惑模型，并评价模型在这种情况下的表现；2. 有意的改变深度神经网络的输出；3. 检测深度神经网络是否过于敏感或过于稳定，并寻找防御攻击的方法。</p>
<p>Jia 和 Liang 首先考虑在深度神经网络中采用对抗样本生成（或者「对抗攻击」，两者皆可）方法完成文本处理相关任务。他们的研究在自然语言处理社区很快获得了研究方面的关注。</p>
<p>然而，由于图片和文本数据内在的不同，用于图像的对抗攻击方法无法直接应用与文本数据上。首先，图像数据（例如像素值）是连续的，但文本数据是离散的。其次，仅仅对像素值进行微小的改变就可以造成图像数据的扰动，而且这种扰动是很难被人眼差距的。但是对于文本的对抗攻击中，小的扰动很容易被察觉，但人类同样能「猜出」本来表达的意义。因此 NLP 模型需要对可辨识的特征鲁棒，而不像视觉只需要对「不太重要」的特征鲁棒。</p>
<p><img src="image1.png" alt="image.png"></p>
<p>DeepWordBug 的深度网络攻击示例。选自 arXiv：1902.07285</p>
<p>与图像领域一样，有进攻就会有防御，目前也有很多研究尝试构建更鲁棒的自然语言处理模型。例如在 CMU 的一篇对抗性拼写错误论文（arXiv：1905.11268）中，研究者通过移除、添加或调序单词内部的字符，以构建更稳健的文本分类模型。这些增减或调序都是一种扰动，就像人类也很可能出现这些笔误一样。通过这些扰动，模型能学会如何处理错别字，从而不至于对分类结果产生影响</p>
<p><img src="image2.png" alt="image.png"></p>
<p>对抗性拼写错误导致的情感误分类，与通过字识别防御手段获得的更稳健模型。选自 arXiv：1905.11268</p>
<h3 id="复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧"><a href="#复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧" class="headerlink" title="复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧"></a>复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧</h3><p><a href="https://www.kaggle.com/scratchpad/kernel4b1df8c794/edit" target="_blank" rel="noopener">kernel</a></p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>PdfMiner文档解析</title>
    <url>/2020/02/27/PdfMiner%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>PDFMiner是一个可以从PDF文档中提取信息的工具。与其他PDF相关的工具不同，它注重的完全是获取和分析文本数据。PDFMiner允许你获取某一页中文本的准确位置和一些诸如字体、行数的信息。它包括一个PDF转换器，可以把PDF文件转换成HTML等格式。它还有一个扩展的PDF解析器，可以用于除文本分析以外的其他用途。<a href="https://euske.github.io/pdfminer/" target="_blank" rel="noopener">官方主页</a></p>
<p>其特征有：1、完全使用python编写。（适用于2.4或更新版本）2、解析，分析，并转换成PDF文档。3、PDF-1.7规范的支持。（几乎）4、中日韩语言和垂直书写脚本支持。5、各种字体类型（Type1、TrueType、Type3，和CID）的支持。6、基本加密（RC4）的支持。7、PDF与HTML转换。8、纲要（TOC）的提取。9、标签内容提取。10、通过分组文本块重建原始的布局。<br>如果你的Python有安装pip模块，就可以通过命令“python pip install pdfminer”，自动安装pdfminer。<br><a id="more"></a><br>解析pdf文件用到的类：</p>
<ul>
<li>PDFParser：从一个文件中获取数据</li>
<li>PDFDocument：保存获取的数据，和PDFParser是相互关联的</li>
<li>PDFPageInterpreter处理页面内容</li>
<li>PDFDevice将其翻译成你需要的格式</li>
<li>PDFResourceManager用于存储共享资源，如字体或图像。</li>
</ul>
<p><strong>pdfminer之间的关系图：</strong></p>
<hr>
<p><img src="pdfminer.png" alt="avatar"></p>
<p>Layout布局分析返回的PDF文档中的每个页面LTPage对象。这个对象和页内包含的子对象，形成一个树结构。如图所示<br><img src="pdfminer2.png" alt="avatar"></p>
<p><em>pdf转换成text的核心代码</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFResourceManager,PDFPageInterpreter</span><br><span class="line"><span class="keyword">from</span> pdfminer.converter <span class="keyword">import</span> TextConverter</span><br><span class="line"><span class="keyword">from</span> pdfminer.layout <span class="keyword">import</span> LAParams</span><br><span class="line"><span class="keyword">from</span> pdfminer.pdfpage <span class="keyword">import</span> PDFPage</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_pdf_to_txt</span><span class="params">(path)</span>:</span></span><br><span class="line">    rsrcmgr =PDFResourceManager()</span><br><span class="line">    retstr = StringIO()</span><br><span class="line">    codec =<span class="string">'utf-8'</span></span><br><span class="line">    laparams =LAParams()</span><br><span class="line">    device = TextConverter(rsrcmgr, retstr,laparams = laparams)</span><br><span class="line">    fp =open(path,<span class="string">'rb'</span>)</span><br><span class="line">    interpreter =PDFPageInterpreter(rsrcmgr, device)</span><br><span class="line">    password = <span class="string">""</span></span><br><span class="line">    maxpages = <span class="number">0</span></span><br><span class="line">    caching =<span class="literal">True</span></span><br><span class="line">    pagenos= set()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> PDFPage.get_pages(fp,pagenos,maxpages=maxpages,password=password,caching=caching,check_extractable=<span class="literal">True</span>):</span><br><span class="line">        interpreter.process_page(page)</span><br><span class="line"></span><br><span class="line">    text = retstr.getvalue()</span><br><span class="line"></span><br><span class="line">    fp.close()</span><br><span class="line">    device.close()</span><br><span class="line">    retstr.close()</span><br><span class="line">    <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>
<p><em>保存成文本文件</em><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveTxt</span><span class="params">(txt, filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename[:<span class="number">-3</span>]+<span class="string">'txt'</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(<span class="string">'openTxt:'</span> + filename[:<span class="number">-3</span>]+<span class="string">'txt'</span>)</span><br><span class="line">        f.write(txt)</span><br></pre></td></tr></table></figure><br><em>转换一个文件夹中的所有pdf文件</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traversal</span><span class="params">(rootdir)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> parent, dirnames, filenames <span class="keyword">in</span> os.walk(rootdir):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            filenameFull = os.path.join(parent, filename)</span><br><span class="line">            <span class="keyword">if</span> (filenameFull.endswith(<span class="string">'pdf'</span>) <span class="keyword">or</span> filenameFull.endswith(<span class="string">'PDF'</span>)):</span><br><span class="line">                txt = readPDF(filenameFull)</span><br><span class="line">                saveTxt(txt.replace(<span class="string">u'\xa9'</span>, <span class="string">u''</span>).replace(<span class="string">u'\xa0'</span>,<span class="string">u''</span>).replace(<span class="string">u'\xad'</span>,<span class="string">u''</span>).replace(<span class="string">u'\u037e'</span>,<span class="string">u''</span>), filenameFull)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    rootdir = <span class="string">'./'</span></span><br><span class="line">    traversal(rootdir)</span><br></pre></td></tr></table></figure>
<p>我们拿其中的一篇pdf文件做解析[PDF] WHO R&amp;D Blueprint novel Coronavirus prospects for evaluating cross-reactivity of nCoV with SARS-CoV January 24, 2020, Geneva, Switzerland.pdf</p>
<p>解析部分页面如下:<br><img src="pdf.png" alt="avatar"></p>
<p>我们通过解析：可以将此pdf转换为纯文本形式：<br><img src="text.png" alt="avatar"></p>
<p>通过对比发现，可以将PDF文件合理的解析成纯文本文件。</p>
]]></content>
      <tags>
        <tag>PDF解析</tag>
      </tags>
  </entry>
  <entry>
    <title>全连接神经网络-FNN</title>
    <url>/2020/02/27/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="comment">#该类为所有其他图节点类的父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,inputs=[])</span>:</span></span><br><span class="line">        <span class="comment">#定义每个节点的输入和输出</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.outputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每个节点都是其输入节点的输出节点</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs:</span><br><span class="line">            n.outputs.append(self)</span><br><span class="line">            <span class="comment"># set 'self' node as inbound_nodes's outbound_nodes</span></span><br><span class="line">            </span><br><span class="line">        self.value = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        self.gradients = &#123;&#125;</span><br><span class="line">        <span class="comment"># keys are the inputs to this node,and</span></span><br><span class="line">        <span class="comment">#their values are the partials of this node with</span></span><br><span class="line">        <span class="comment"># respect to that input.</span></span><br><span class="line">        <span class="comment"># \partial&#123;node&#125;&#123;input_i&#125;</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播函数，继承该类的其他类会覆写该函数</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string">        Compute the output value based on 'inbound_nodes' and store the</span></span><br><span class="line"><span class="string">        result in self.value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">raise</span> <span class="built_in">NotImplemented</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播函数，继承该类的其他类会覆写该函数</span></span><br><span class="line">        <span class="keyword">raise</span> <span class="built_in">NotImplemented</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Input</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#输入节点，包括神经网络输入节点，权重节点，和偏差节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        An Input node has no inbound nodes,</span></span><br><span class="line"><span class="string">        So no need to pass anythinto the Node instantiator.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Node.__init__(self)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, value=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Only input node is the node where the value may be passed</span></span><br><span class="line"><span class="string">        as anargument to forward().</span></span><br><span class="line"><span class="string">        All other node implementations should get the value of the </span></span><br><span class="line"><span class="string">        previous node from self.inbound_nodes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string">        val0:self.inbound_nodes[0].value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#定义节点数值</span></span><br><span class="line">        <span class="keyword">if</span> value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.value =value</span><br><span class="line">            <span class="comment">#It's is input node,when need to forward,this node initiate</span></span><br><span class="line">            <span class="comment">#self's value.</span></span><br><span class="line">        <span class="comment"># Input subclass just holds a value,such as a data feature or</span></span><br><span class="line">        <span class="comment"># model parameter(weight/bias)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#计算节点梯度</span></span><br><span class="line">        self.gradients = &#123;self:<span class="number">0</span>&#125;<span class="comment"># initialization</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            <span class="comment">#以下计算该节点的输出节点对该节点的梯度</span></span><br><span class="line">            grad_cost = n.gradients[self]</span><br><span class="line">            self.gradients[self] =grad_cost*<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># input --&gt; N1,N2</span></span><br><span class="line">            <span class="comment">#\partial L / \partial N</span></span><br><span class="line">            <span class="comment"># ==&gt; \partial L / partial N1 * \ partial N1 / \partial N</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Add</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,*nodes)</span>:</span></span><br><span class="line">        Node.__init__(self,nodes)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.value = sum(map(<span class="keyword">lambda</span> n:n.value,self.inputs))</span><br><span class="line">        <span class="comment"># when execute forward, this node cacultae value as defined</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#全连接网络层的计算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,nodes,weights,bias)</span>:</span></span><br><span class="line">        Node.__init__(self,[nodes,weights,bias])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播的计算 y=w*x + b</span></span><br><span class="line">        inputs = self.inputs[<span class="number">0</span>].value</span><br><span class="line">        weights = self.inputs[<span class="number">1</span>].value</span><br><span class="line">        bias = self.inputs[<span class="number">2</span>].value</span><br><span class="line">        </span><br><span class="line">        self.value =np.dot(inputs,weights) + bias</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播计算</span></span><br><span class="line">        <span class="comment"># initial a partial for each of the inbound_nodes.</span></span><br><span class="line">        self.gradients = &#123;n:np.zeros_like(n.value) <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            <span class="comment"># Get the partial of the cost w.r.t this node.</span></span><br><span class="line">            grad_cost = n.gradients[self]</span><br><span class="line">            </span><br><span class="line">            self.gradients[self.inputs[<span class="number">0</span>]] = np.dot(grad_cost,self.inputs[<span class="number">1</span>].value.T)</span><br><span class="line">            self.gradients[self.inputs[<span class="number">1</span>]] = np.dot(self.inputs[<span class="number">0</span>].value.T,grad_cost)</span><br><span class="line">            self.gradients[self.inputs[<span class="number">2</span>]] = np.sum(grad_cost,axis=<span class="number">0</span>,keepdims=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># WX + B / W ==&gt; X</span></span><br><span class="line">        <span class="comment"># WX + B / X ==&gt; W</span></span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#定义sigmod函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,node)</span>:</span></span><br><span class="line">        Node.__init__(self,[node])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1</span>+np.exp(<span class="number">-1</span>*x))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向 即sigmoid函数计算</span></span><br><span class="line">        self.x = self.inputs[<span class="number">0</span>].value <span class="comment"># [0] input is a list</span></span><br><span class="line">        self.value = self._sigmoid(self.x)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播计算梯度</span></span><br><span class="line">        self.partial = self._sigmoid(self.x) * (<span class="number">1</span> -self._sigmoid(self.x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># y = 1/(1+ e^-x)</span></span><br><span class="line">        <span class="comment"># y'= 1/(1 + e^-x) (1 - 1/(1 + e^-x))</span></span><br><span class="line">        </span><br><span class="line">        self.gradients = &#123;n:np.zeros_like(n.value) <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            grad_cost = n.gradients[self] <span class="comment"># Get the partial of the cost with respect to this node</span></span><br><span class="line">            </span><br><span class="line">            self.gradients[self.inputs[<span class="number">0</span>]] = grad_cost * self.partial</span><br><span class="line">            <span class="comment"># use * to keep all the dimension same!.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSE</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment"># 定义平均平方误差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,y,a)</span>:</span></span><br><span class="line">        Node.__init__(self,[y,a])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播计算</span></span><br><span class="line">        y = self.inputs[<span class="number">0</span>].value.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        a = self.inputs[<span class="number">1</span>].value.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span>(y.shape == a.shape)</span><br><span class="line">        </span><br><span class="line">        self.m = self.inputs[<span class="number">0</span>].value.shape[<span class="number">0</span>]</span><br><span class="line">        self.diff = y -a</span><br><span class="line">        </span><br><span class="line">        self.value = np.mean(self.diff**<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向计算相应的梯度</span></span><br><span class="line">        self.gradients[self.inputs[<span class="number">0</span>]] = ( <span class="number">2</span> / self.m) * self.diff</span><br><span class="line">        self.gradients[self.inputs[<span class="number">1</span>]] = ( <span class="number">-2</span> /self.m) * self.diff</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_and_backward</span><span class="params">(outputnode,graph)</span>:</span></span><br><span class="line">    <span class="comment"># execute all the forward method of sorted_nodes.</span></span><br><span class="line">    <span class="comment">## In practice,it's common to feed in mutiple data example in each forward pass rather than just 1. Because the example can be</span></span><br><span class="line">    <span class="comment">## processed in parallel.The number of examples is called batch size.</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph:</span><br><span class="line">        n.forward()</span><br><span class="line">        <span class="comment"># each node execute forward, get self.value based on the topological sort result.</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph[::<span class="number">-1</span>]:</span><br><span class="line">        n.backward()</span><br><span class="line">    <span class="comment"># return outputnode.value</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### v -&gt; a -&gt; C</span></span><br><span class="line"><span class="comment">##  b -&gt; C</span></span><br><span class="line"><span class="comment">##  b -&gt; v - a -&gt; C</span></span><br><span class="line"><span class="comment">## v -&gt; v -&gt; a -&gt; C</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topological_sort</span><span class="params">(feed_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sort generic nodes in topological order using Kahn's Algorithm.</span></span><br><span class="line"><span class="string">    'feed_dict': A dictionary where the key is a 'Input' node and the value is the respective value feed to that node.</span></span><br><span class="line"><span class="string">    Returns a list of sorted nodes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    input_nodes = [n <span class="keyword">for</span> n <span class="keyword">in</span> feed_dict.keys()]</span><br><span class="line">    </span><br><span class="line">    G = &#123;&#125;</span><br><span class="line">    nodes = [n <span class="keyword">for</span> n <span class="keyword">in</span> input_nodes]</span><br><span class="line">    <span class="keyword">while</span> len(nodes)&gt;<span class="number">0</span>:</span><br><span class="line">        n = nodes.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> G:</span><br><span class="line">            G[n] = &#123;<span class="string">'in'</span>:set(),<span class="string">'out'</span>:set()&#125;</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> n.outputs:</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">not</span> <span class="keyword">in</span> G:</span><br><span class="line">                G[m] = &#123;<span class="string">'in'</span>:set(),<span class="string">'out'</span>:set()&#125;</span><br><span class="line">            G[n][<span class="string">'out'</span>].add(m)</span><br><span class="line">            G[m][<span class="string">'in'</span>].add(n)</span><br><span class="line">            nodes.append(m)</span><br><span class="line">    </span><br><span class="line">    L =[]</span><br><span class="line">    S = set(input_nodes)</span><br><span class="line">    <span class="keyword">while</span> len(S) &gt;<span class="number">0</span>:</span><br><span class="line">        n = S.pop()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> isinstance(n,Input):</span><br><span class="line">            n.value= feed_dict[n]</span><br><span class="line">            <span class="comment">## if n is Input Node,setn'value as</span></span><br><span class="line">            <span class="comment">## feed_dict[n]</span></span><br><span class="line">            <span class="comment">## else, n's value is caculate as its</span></span><br><span class="line">            <span class="comment">## inbounds</span></span><br><span class="line">        L.append(n)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> n.outputs:</span><br><span class="line">            G[n][<span class="string">'out'</span>].remove(m)</span><br><span class="line">            G[m][<span class="string">'in'</span>].remove(n)</span><br><span class="line">            <span class="comment"># if no other incoming edges add to S</span></span><br><span class="line">            <span class="keyword">if</span> len(G[m][<span class="string">'in'</span>]) == <span class="number">0</span>:</span><br><span class="line">                S.add(m)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_update</span><span class="params">(trainables,learning_rate =<span class="number">1e-2</span>)</span>:</span></span><br><span class="line">    <span class="comment">#there are so many other update / optimigation methods</span></span><br><span class="line">    <span class="comment">#such as Adam,Mom,</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> trainables:</span><br><span class="line">        t.value += <span class="number">-1</span> * learning_rate * t.gradients[t]</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\partial{node}{input_i}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = load_boston()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">losses = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Check out the new network architecture and dataset!</span></span><br><span class="line"><span class="string">Notice that the weights and biases are </span></span><br><span class="line"><span class="string">generated randomly.</span></span><br><span class="line"><span class="string">No need to change anything,but feel free to tweak</span></span><br><span class="line"><span class="string">to test your network, play around with the epoches, batch size,etc!</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle,resample</span><br><span class="line"><span class="comment">#from minflow import *</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Load data</span></span><br><span class="line">data =load_boston()</span><br><span class="line">X_ = data[<span class="string">'data'</span>]</span><br><span class="line">_ = data[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize data</span></span><br><span class="line"></span><br><span class="line">X_ = (X_ - np.mean(X_,axis=<span class="number">0</span>)) / np.std(X_,axis =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">n_features =X_.shape[<span class="number">1</span>]</span><br><span class="line">n_hidden = <span class="number">10</span></span><br><span class="line">W1_ = np.random.randn(n_features,n_hidden)</span><br><span class="line">b1_ = np.zeros(n_hidden)</span><br><span class="line">W2_ = np.random.randn(n_hidden,<span class="number">1</span>)</span><br><span class="line">b2_ = np.zeros(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neural network</span></span><br><span class="line">X,y = Input(),Input()</span><br><span class="line">W1,b1=Input(),Input()</span><br><span class="line">W2,b2 =Input(),Input()</span><br><span class="line"></span><br><span class="line">l1 = Linear(X,W1,b1)</span><br><span class="line">s1 = Sigmoid(l1)</span><br><span class="line">l2 = Linear(s1,W2,b2)</span><br><span class="line">cost = MSE(y,l2)</span><br><span class="line"></span><br><span class="line">feed_dict = &#123;</span><br><span class="line">    X:X_,</span><br><span class="line">    y:y_,</span><br><span class="line">    W1:W1_,</span><br><span class="line">    b1:b1_,</span><br><span class="line">    W2:W2_,</span><br><span class="line">    b2:b2_</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5000</span></span><br><span class="line"><span class="comment"># Total number of examples</span></span><br><span class="line">m = X_.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">steps_per_epoch = m // batch_size</span><br><span class="line"></span><br><span class="line">graph = topological_sort(feed_dict)</span><br><span class="line">trainables = [W1,b1,W2,b2]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total number of examples = &#123;&#125;"</span>.format(m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(steps_per_epoch):</span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        <span class="comment"># Randomly sample a batch of examples</span></span><br><span class="line">        X_batch,y_batch =resample(X_,y_,n_samples=batch_size)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reset valueof X and y Inputs</span></span><br><span class="line">        X.value = X_batch</span><br><span class="line">        y.value = y_batch</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        _ = <span class="literal">None</span></span><br><span class="line">        forward_and_backward(_,graph) <span class="comment">#set output node not important.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        rate =<span class="number">1e-2</span></span><br><span class="line">        </span><br><span class="line">        sgd_update(trainables,rate)</span><br><span class="line">        </span><br><span class="line">        loss += graph[<span class="number">-1</span>].value</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:&#123;&#125;,Loss:&#123;:.3f&#125;"</span>.format(i+<span class="number">1</span>,loss/steps_per_epoch))</span><br><span class="line">        losses.append(loss)</span><br></pre></td></tr></table></figure>
<pre><code>Total number of examples = 506
Epoch:1,Loss:171.205
Epoch:101,Loss:8.215
Epoch:201,Loss:7.937
Epoch:301,Loss:7.301
Epoch:401,Loss:6.069
Epoch:501,Loss:5.735
Epoch:601,Loss:4.524
Epoch:701,Loss:4.418
Epoch:801,Loss:4.473
Epoch:901,Loss:4.510
Epoch:1001,Loss:3.484
Epoch:1101,Loss:4.627
Epoch:1201,Loss:4.473
Epoch:1301,Loss:4.152
Epoch:1401,Loss:4.831
Epoch:1501,Loss:4.992
Epoch:1601,Loss:4.500
Epoch:1701,Loss:4.706
Epoch:1801,Loss:3.927
Epoch:1901,Loss:4.712
Epoch:2001,Loss:4.262
Epoch:2101,Loss:3.968
Epoch:2201,Loss:4.792
Epoch:2301,Loss:4.106
Epoch:2401,Loss:3.815
Epoch:2501,Loss:4.089
Epoch:2601,Loss:4.376
Epoch:2701,Loss:3.923
Epoch:2801,Loss:5.195
Epoch:2901,Loss:4.273
Epoch:3001,Loss:3.618
Epoch:3101,Loss:3.368
Epoch:3201,Loss:3.754
Epoch:3301,Loss:4.027
Epoch:3401,Loss:3.442
Epoch:3501,Loss:4.029
Epoch:3601,Loss:3.475
Epoch:3701,Loss:3.861
Epoch:3801,Loss:4.248
Epoch:3901,Loss:3.697
Epoch:4001,Loss:4.466
Epoch:4101,Loss:4.746
Epoch:4201,Loss:4.145
Epoch:4301,Loss:3.594
Epoch:4401,Loss:4.501
Epoch:4501,Loss:3.719
Epoch:4601,Loss:4.005
Epoch:4701,Loss:4.331
Epoch:4801,Loss:4.092
Epoch:4901,Loss:3.657
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(outputNode,graph)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph:</span><br><span class="line">        n.forward()</span><br><span class="line">    <span class="keyword">return</span> outputNode.value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">forward(l2,graph)</span><br></pre></td></tr></table></figure>
<pre><code>array([[18.41675144],
       [29.47806568],
       [15.32327987],
       [48.89022023],
       [49.92947466],
       [45.52560889],
       [30.32152378],
       [13.889439  ],
       [20.52297098],
       [38.29474339],
       [19.54218589],
       [10.72223761],
       [10.13966276],
       [19.20474889],
       [ 9.715787  ],
       [26.87621257]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(len(losses)),losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x1fd125539c8&gt;]
</code></pre><p><img src="output_10_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W2.value</span><br></pre></td></tr></table></figure>
<pre><code>array([[3.78502026],
       [5.48520715],
       [6.6856144 ],
       [5.89625598],
       [5.81552044],
       [3.22412022],
       [8.54504219],
       [2.76550267],
       [9.40747634],
       [5.60113079]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_ = data[<span class="string">'data'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,
       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,
       4.980e+00])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model =Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units = <span class="number">64</span>,activation=<span class="string">'sigmoid'</span>,input_dim=<span class="number">13</span>))</span><br><span class="line">model.add(Dense(units = <span class="number">30</span>,activation=<span class="string">'sigmoid'</span>,input_dim=<span class="number">64</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss = <span class="string">'mse'</span>,optimizer=<span class="string">'sgd'</span>,metrics = [<span class="string">'mse'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(X_,y_,epochs=<span class="number">5000</span>,batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5000
506/506 [==============================] - 1s 3ms/step - loss: 158.0243 - mse: 158.0243
Epoch 2/5000
506/506 [==============================] - 0s 91us/step - loss: 81.3887 - mse: 81.3887
Epoch 3/5000
506/506 [==============================] - 0s 87us/step - loss: 79.5072 - mse: 79.5072
Epoch 4/5000
506/506 [==============================] - 0s 
Epoch 2700/5000
506/506 [==============================] - 0s 99us/step - loss: 85.0554 - mse: 85.0554
Epoch 2701/5000
506/506 [==============================] - 0s 142us/step - loss: 84.8442 - mse: 84.8442
Epoch 2702/5000
506/506 [==============================] - 0s 103us/step - loss: 85.1862 - mse: 85.1862
Epoch 2703/5000
506/506 [==============================] - 0s 97us/step - loss: 84.6198 - mse: 84.6198
Epoch 2704/5000
506/506 [==============================] - 0s 89us/step - loss: 85.3531 - mse: 85.3531
Epoch 2705/5000
506/506 [==============================] - 0s 101us/step - loss: 84.8638 - mse: 84.8638
Epoch 2706/5000
506/506 [==============================] - 0s 87us/step - loss: 84.6398 - mse: 84.6398
Epoch 2707/5000
506/506 [==============================] - 0s 154us/step - loss: 84.8038 - mse: 84.8038
Epoch 2708/5000
506/506 [==============================] - 0s 99us/step - loss: 84.6299 - mse: 84.6299
Epoch 2709/5000
506/506 [==============================] - 0s 101us/step - loss: 84.8330 - mse: 84.8330
Epoch 2710/5000
506/506 [==============================] - 0s 130us/step - loss: 85.0682 - mse: 85.0682
Epoch 2711/5000
506/506 [==============================] - 0s 111us/step - loss: 84.6372 - mse: 84.6372
Epoch 2712/5000
506/506 [==============================] - 0s 101us/step - loss: 84.6715 - mse: 84.6715
Epoch 2713/5000
506/506 [==============================] - 0s 128us/step - loss: 85.0228 - mse: 85.0228
Epoch 2714/5000
506/506 [==============================] - 0s 93us/step - loss: 84.8349 - mse: 84.8349
Epoch 2715/5000
506/506 [==============================] - 0s 109us/step - loss: 84.7732 - mse: 84.7732
Epoch 2716/5000
506/506 [==============================] - 0s 113us/step - loss: 84.7862 - mse: 84.7862
Epoch 2717/5000
506/506 [==============================] - 0s 113us/step - loss: 84.5297 - mse: 84.5297
Epoch 2718/5000
506/506 [==============================] - 0s 107us/step - loss: 84.7425 - mse: 84.7425
Epoch 2719/5000
506/506 [==============================] - 0s 103us/step - loss: 85.2987 - mse: 85.2987
Epoch 2720/5000
506/506 [==============================] - 0s 107us/step - loss: 84.6666 - mse: 84.6666
Epoch 2721/5000
506/506 [==============================] - 0s 103us/step - loss: 
506/506 [==============================] - 0s 105us/step - loss: 85.1947 - mse: 85.1947
Epoch 3139/5000
506/506 [==============================] - 0s 97us/step - loss: 84.9491 - mse: 84.9491
Epoch 3140/5000
506/506 [==============================] - 0s 99us/step - loss: 85.3199 - mse: 85.3198
Epoch 3141/5000
 32/506 [&gt;.............................] - ETA: 0s - loss: 114.7459 - mse: 114.7459
</code></pre>]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Machinglearing Model,the first step</title>
    <url>/2020/01/31/Machinglearing-Model-the-first-step/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_data = np.random.random((<span class="number">20</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_data</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<pre><code>array([[0.50296664, 0.62445093],
       [0.19994622, 0.19373156],
       [0.14250226, 0.61334931],
       [0.70048398, 0.75160392],
       [0.74448897, 0.7320275 ],
       [0.85976709, 0.84319014],
       [0.73253413, 0.12288743],
       [0.88371578, 0.84136921],
       [0.97180754, 0.79078425],
       [0.05776667, 0.92731363],
       [0.03322522, 0.36021126],
       [0.30821425, 0.57943347],
       [0.16970345, 0.72740845],
       [0.5127129 , 0.44245729],
       [0.37546157, 0.477542  ],
       [0.96229987, 0.98167783],
       [0.79974288, 0.20093964],
       [0.65953995, 0.83289056],
       [0.83061176, 0.10022954],
       [0.57372662, 0.74422547]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = random_data[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y = random_data[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assuming_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 在我们的日常生活中是常见的</span></span><br><span class="line">    <span class="comment"># 体重-&gt; 高血压的概率</span></span><br><span class="line">    <span class="comment"># 收入-&gt; 买阿玛尼的概率</span></span><br><span class="line">    <span class="comment"># 其实都是一种潜在的函数关系 + 一个随机变化</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">13.4</span> *x + <span class="number">5</span> + random.randint(<span class="number">-5</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = [assuming_function(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x21e39725708&gt;
</code></pre><p><img src="output_11_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = np.array(y)</span><br></pre></td></tr></table></figure>
<h3 id="Regression-gt-Real-Number"><a href="#Regression-gt-Real-Number" class="headerlink" title="Regression -&gt; Real Number"></a>Regression -&gt; Real Number</h3><h3 id="Classification-gt-0-0-0-1-0-1-0-0"><a href="#Classification-gt-0-0-0-1-0-1-0-0" class="headerlink" title="Classification -&gt; [0,0,0,1],[0,1,0,0]"></a>Classification -&gt; [0,0,0,1],[0,1,0,0]</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>array([ 9.73975302,  6.67927941,  3.90953023, 10.38648538, 13.97615214,
       20.520879  , 10.81595738, 14.84179151, 19.02222101,  8.77407336,
        0.44521789, 13.13007093,  4.27402617,  7.87035283,  9.03118504,
       13.89481826, 18.71655464, 15.83783527, 16.1301976 , 10.68793665])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg = LinearRegression().fit(X.reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.score(X.reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br></pre></td></tr></table></figure>
<pre><code>0.7115853500059341
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.coef_</span><br></pre></td></tr></table></figure>
<pre><code>array([14.50455278])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.intercept_</span><br></pre></td></tr></table></figure>
<pre><code>3.441324165071637
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> reg.coef_ * x + reg.intercept_</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X,y)</span><br><span class="line">plt.plot(X,f(X),color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x21e3cf5c8c8&gt;]
</code></pre><p><img src="output_22_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>array([0.50296664, 0.19994622, 0.14250226, 0.70048398, 0.74448897,
       0.85976709, 0.73253413, 0.88371578, 0.97180754, 0.05776667,
       0.03322522, 0.30821425, 0.16970345, 0.5127129 , 0.37546157,
       0.96229987, 0.79974288, 0.65953995, 0.83061176, 0.57372662])
</code></pre><h3 id="How-to-implement-a-KNN-model"><a href="#How-to-implement-a-KNN-model" class="headerlink" title="How to implement a KNN model"></a>How to implement a KNN model</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    <span class="comment"># 直接存储X,y即可</span></span><br><span class="line">    <span class="keyword">return</span> [(Xi,yi) <span class="keyword">for</span> Xi, yi <span class="keyword">in</span> zip(X,y)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cosine</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cosine(x1,x2)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prdict</span><span class="params">(x,k=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment">#在predicate 的时候，需要做大量的计算</span></span><br><span class="line">    most_similars =sorted(model(X,y),key = <span class="keyword">lambda</span> xi:distance(xi[<span class="number">0</span>],x))[:k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># -&gt; regression:numerical -&gt; most_similars(y)</span></span><br><span class="line">    <span class="comment"># -&gt;classification:categorical -&gt; most_similar(y)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 已经获得了最相似的数据集</span></span><br><span class="line">    <span class="comment"># 然后呢，Counter()-&gt; most_common()-&gt;就可以获得出现最多的这个y了</span></span><br></pre></td></tr></table></figure>
<h3 id="不是简简单单的学一个算法，看到背后的思维方式："><a href="#不是简简单单的学一个算法，看到背后的思维方式：" class="headerlink" title="不是简简单单的学一个算法，看到背后的思维方式："></a>不是简简单单的学一个算法，看到背后的思维方式：</h3><blockquote>
<p>贝叶斯，线性回归，决策树，KNN</p>
</blockquote>
<h3 id="新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。"><a href="#新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。" class="headerlink" title="新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。"></a>新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。</h3><h3 id="How-to-implement-a-Decision-Tree"><a href="#How-to-implement-a-Decision-Tree" class="headerlink" title="How to implement a Decision Tree"></a>How to implement a Decision Tree</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><script type="math/tex; mode=display">Entropy = -\sum_i^n Pr(x_i) log(Pr(x_i))</script><h3 id="Gini-纯度"><a href="#Gini-纯度" class="headerlink" title="Gini 纯度"></a>Gini 纯度</h3><script type="math/tex; mode=display">Gini= 1 - \sum_{i=1}^J P_i^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">! pip install icecream</span><br></pre></td></tr></table></figure>
<pre><code>Collecting icecream
  Downloading https://files.pythonhosted.org/packages/8c/ec/821ef939e8e4f4306e7263afa7e2ce0b4c5da9e6e53d1cc97b01606035f8/icecream-2.0.0-py2.py3-none-any.whl
Requirement already satisfied: colorama&gt;=0.3.9 in d:\anaconda\lib\site-packages (from icecream) (0.4.1)
Collecting asttokens&gt;=2.0.1 (from icecream)
  Downloading https://files.pythonhosted.org/packages/e8/18/41e95b4a6b4fd3ae704e672da5d070272518995f580be79d772be312c4af/asttokens-2.0.3-py2.py3-none-any.whl
Requirement already satisfied: pygments&gt;=2.2.0 in d:\anaconda\lib\site-packages (from icecream) (2.4.2)
Collecting executing&gt;=0.3.1 (from icecream)
  Downloading https://files.pythonhosted.org/packages/79/a1/f85482473b12b2b0e1fa10da84d4280930dbd6e4e149cedf7ae91f894138/executing-0.4.1.tar.gz
Requirement already satisfied: six in d:\anaconda\lib\site-packages (from asttokens&gt;=2.0.1-&gt;icecream) (1.12.0)
Building wheels for collected packages: executing
  Building wheel for executing (setup.py): started
  Building wheel for executing (setup.py): finished with status &#39;done&#39;
  Created wheel for executing: filename=executing-0.4.1-cp37-none-any.whl size=8302 sha256=2fce6277eb7197756482de660a24f6d2c80bb965838cbbb515106f385e4ccad3
  Stored in directory: C:\Users\tb\AppData\Local\pip\Cache\wheels\b0\71\dc\c1bdcd4b384c4458b639dfa905bc093979b8779f2e0df78792
Successfully built executing
Installing collected packages: asttokens, executing, icecream
Successfully installed asttokens-2.0.3 executing-0.4.1 icecream-2.0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(elements)</span>:</span></span><br><span class="line">    <span class="string">'''群体混乱程度'''</span></span><br><span class="line">    counter = Counter(elements)</span><br><span class="line">    probs = [counter[c] / len(elements) <span class="keyword">for</span> c <span class="keyword">in</span> set(elements)]</span><br><span class="line">    ic(probs)</span><br><span class="line">    <span class="keyword">return</span> - sum(p * np.log(p) <span class="keyword">for</span> p <span class="keyword">in</span> probs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [1.0]





-0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]





0.5623351446188083
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]





0.5623351446188083
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.5, 0.25]





1.0397207708399179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.5, 0.25]





1.0397207708399179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.25, 0.25, 0.25]





1.3862943611198906
</code></pre><h3 id="决策树怎么来决定，哪一个特征来进行分割呢？"><a href="#决策树怎么来决定，哪一个特征来进行分割呢？" class="headerlink" title="决策树怎么来决定，哪一个特征来进行分割呢？"></a>决策树怎么来决定，哪一个特征来进行分割呢？</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mock_data = &#123;</span><br><span class="line">    <span class="string">'gender'</span>:[<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'M'</span>,<span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'income'</span>:[<span class="string">'+10'</span>,<span class="string">'-10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'-10'</span>],</span><br><span class="line">    <span class="string">'family_number'</span>:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">    <span class="comment">#'pet':[1,1,1,0,0,0,1],</span></span><br><span class="line">    <span class="string">'bought'</span>:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = pd.DataFrame.from_dict(mock_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>F</td>
      <td>+10</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>6</td>
      <td>M</td>
      <td>-10</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>如果我们来了一个新的case:::[F,-10,2,1]-&gt;?</p>
<p>:::[F,+10,2,0]-&gt;?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split_bt_gender:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]) + entropy([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split_by_income:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) + entropy([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#split_by_family_number</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])+entropy([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split_by_some_feature:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])+entropy([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| probs: [0.6, 0.4]
ic| probs: [1.0]
ic| probs: [0.6, 0.4]
ic| probs: [1.0]
ic| probs: [1.0]
ic| probs: [1.0]


1.198849312913621
0.6730116670092565
0.6730116670092565
-0.0
</code></pre><h3 id="决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。"><a href="#决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。" class="headerlink" title="决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。"></a>决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(mock_data[<span class="string">'family_number'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{1, 2}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(mock_data[<span class="string">'gender'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;F&#39;, &#39;M&#39;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_1 = dataset[dataset[<span class="string">'family_number'</span>]==<span class="number">1</span>][<span class="string">'bought'</span>].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_1</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, 0, 0, 0]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_2 =dataset[dataset[<span class="string">'family_number'</span>]!=<span class="number">1</span>][<span class="string">'bought'</span>].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_2</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data =dataset[dataset[<span class="string">'family_number'</span>]==<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data[splited_data[<span class="string">'income'</span>] == <span class="string">'+10'</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="gt-根据信息熵，我们得到了一个决策过程："><a href="#gt-根据信息熵，我们得到了一个决策过程：" class="headerlink" title="===&gt; 根据信息熵，我们得到了一个决策过程："></a>===&gt; 根据信息熵，我们得到了一个决策过程：</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一步：我们观察他的家庭成员：</span><br><span class="line">         如果他的家庭成员是2人，那么就会购买，如果不是2人，我们继续决策，进入下一步</span><br><span class="line">第二部：我们观察他的收入情况：</span><br><span class="line">         如果他的收入是&#39;+10&#39;,那么他有 3&#x2F;4 的概率会购买，如果是&#39;-10&#39;，那么，他肯定不买</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy(sub_split_1)</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.6, 0.4]





0.6730116670092565
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy(sub_split_2)</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [1.0]





-0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(dataset.columns.to_list()) - &#123;<span class="string">'bought'</span>&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;family_number&#39;, &#39;gender&#39;, &#39;income&#39;, &#39;pet&#39;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_the_optimal_spilter</span><span class="params">(training_data:pd.DataFrame,target:str)</span> -&gt; str:</span></span><br><span class="line">    x_fields = set(training_data.columns.tolist()) - &#123;target&#125;</span><br><span class="line">    </span><br><span class="line">    spliter = <span class="literal">None</span></span><br><span class="line">    min_entropy = float(<span class="string">'inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> x_fields:</span><br><span class="line">        ic(f)</span><br><span class="line">        values = set(training_data[f])</span><br><span class="line">        ic(values)</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> values:</span><br><span class="line">            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()</span><br><span class="line">            ic(sub_spliter_1)</span><br><span class="line">            <span class="comment"># split by the current feature and one value</span></span><br><span class="line">            </span><br><span class="line">            entropy_1 =entropy(sub_spliter_1)</span><br><span class="line">            ic(entropy_1)</span><br><span class="line">            </span><br><span class="line">            sub_spliter_2 = training_data[training_data[f] !=v][target].tolist()</span><br><span class="line">            ic(sub_spliter_2)</span><br><span class="line">            </span><br><span class="line">            entropy_2 = entropy(sub_spliter_2)</span><br><span class="line">            ic(entropy_2)</span><br><span class="line">            </span><br><span class="line">            entropy_v =entropy_1 +entropy_2</span><br><span class="line">            </span><br><span class="line">            ic(entropy_v)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> entropy_v &lt;= min_entropy:</span><br><span class="line">                min_entropy = entropy_v</span><br><span class="line">                spliter =(f,v)</span><br><span class="line">    print(<span class="string">' spliter is: &#123;&#125;'</span>.format(spliter))</span><br><span class="line">    print(<span class="string">' the min entropy is: &#123;&#125;'</span>.format(min_entropy))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> spliter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(training_data=dataset,target=<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;pet&#39;
ic| values: {0, 1}
ic| sub_spliter_1: [0, 0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: -0.0
ic| sub_spliter_1: [1, 1, 1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [0, 0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: -0.0
ic| f: &#39;family_number&#39;
ic| values: {1, 2}
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: [1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6730116670092565
ic| sub_spliter_1: [1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_2: 0.6730116670092565
ic| entropy_v: 0.6730116670092565
ic| f: &#39;income&#39;
ic| values: {&#39;-10&#39;, &#39;+10&#39;}
ic| sub_spliter_1: [1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_2: 0.6730116670092565
ic| entropy_v: 0.6730116670092565
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: [1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6730116670092565
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 1, 1, 0]
ic| probs: [0.25, 0.75]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: [0, 0, 1]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| entropy_2: 0.6365141682948128
ic| entropy_v: 1.198849312913621
ic| sub_spliter_1: [0, 0, 1]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| entropy_1: 0.6365141682948128
ic| sub_spliter_2: [1, 1, 1, 0]
ic| probs: [0.25, 0.75]
ic| entropy_2: 0.5623351446188083
ic| entropy_v: 1.198849312913621


 spliter is: (&#39;pet&#39;, 1)
 the min entropy is: -0.0





(&#39;pet&#39;, 1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>F</td>
      <td>+10</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>6</td>
      <td>M</td>
      <td>-10</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">1</span> ],<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;family_number&#39;
ic| values: {1}
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.6730116670092565
ic| f: &#39;income&#39;
ic| values: {&#39;-10&#39;, &#39;+10&#39;}
ic| sub_spliter_1: [1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_2: 0.5623351446188083
ic| entropy_v: 0.5623351446188083
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: [1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 1, 0]
ic| probs: [0.3333333333333333, 0.6666666666666666]
ic| entropy_1: 0.6365141682948128
ic| sub_spliter_2: [0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6365141682948128
ic| sub_spliter_1: [0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0]
ic| probs: [0.3333333333333333, 0.6666666666666666]
ic| entropy_2: 0.6365141682948128
ic| entropy_v: 0.6365141682948128


 spliter is: (&#39;income&#39;, &#39;+10&#39;)
 the min entropy is: 0.5623351446188083





(&#39;income&#39;, &#39;+10&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fm_n_1 = dataset[dataset[<span class="string">'family_number'</span>] == <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fm_n_1[fm_n_1[<span class="string">'income'</span>]==<span class="string">'+10'</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(fm_n_1[fm_n_1[<span class="string">'income'</span>] == <span class="string">'+10'</span>],<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;family_number&#39;
ic| values: {1}
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;income&#39;
ic| values: {&#39;+10&#39;}
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 0]
ic| probs: [0.5, 0.5]
ic| entropy_1: 0.6931471805599453
ic| sub_spliter_2: [0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6931471805599453
ic| sub_spliter_1: [0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 0]
ic| probs: [0.5, 0.5]
ic| entropy_2: 0.6931471805599453
ic| entropy_v: 0.6931471805599453


 spliter is: (&#39;income&#39;, &#39;+10&#39;)
 the min entropy is: 0.5623351446188083





(&#39;income&#39;, &#39;+10&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-Methods"><a href="#Evaluation-Methods" class="headerlink" title="Evaluation Methods"></a>Evaluation Methods</h3><h4 id="1-Accuracy"><a href="#1-Accuracy" class="headerlink" title="1.Accuracy"></a>1.Accuracy</h4><h4 id="2-Precision"><a href="#2-Precision" class="headerlink" title="2. Precision"></a>2. Precision</h4><h4 id="3-Recall"><a href="#3-Recall" class="headerlink" title="3.Recall"></a>3.Recall</h4><h4 id="3-5-F1-Score-F2-Score"><a href="#3-5-F1-Score-F2-Score" class="headerlink" title="3.5 F1 Score, F2 Score"></a>3.5 F1 Score, F2 Score</h4><h4 id="4-AUC"><a href="#4-AUC" class="headerlink" title="4. AUC"></a>4. AUC</h4><h3 id="判断是不是垃圾邮件，是的话程序输出1，不是输出0"><a href="#判断是不是垃圾邮件，是的话程序输出1，不是输出0" class="headerlink" title="判断是不是垃圾邮件，是的话程序输出1，不是输出0"></a>判断是不是垃圾邮件，是的话程序输出1，不是输出0</h3><h3 id="给了十个数据，这10个数据的真实情况是："><a href="#给了十个数据，这10个数据的真实情况是：" class="headerlink" title="给了十个数据，这10个数据的真实情况是："></a>给了十个数据，这10个数据的真实情况是：</h3><h3 id="gt-1-1-1-1-0-1-0-0-1-1"><a href="#gt-1-1-1-1-0-1-0-0-1-1" class="headerlink" title="-&gt;[1,1,1,1,0,1,0,0,1,1]"></a>-&gt;[1,1,1,1,0,1,0,0,1,1]</h3><h3 id="F-x"><a href="#F-x" class="headerlink" title="F(x)"></a>F(x)</h3><h3 id="gt-1-1-1-1-1-1-1-1-0-1"><a href="#gt-1-1-1-1-1-1-1-1-0-1" class="headerlink" title="-&gt;[1,1,1,1,1,1,1,1,0,1]"></a>-&gt;[1,1,1,1,1,1,1,1,0,1]</h3><h3 id="Accuracy-预测的标签中预测正确的值的个数-总的预测的个数"><a href="#Accuracy-预测的标签中预测正确的值的个数-总的预测的个数" class="headerlink" title="Accuracy:预测的标签中预测正确的值的个数/总的预测的个数"></a>Accuracy:预测的标签中预测正确的值的个数/总的预测的个数</h3><p>—&gt; 6/10</p>
<h3 id="Precision-所有说“是”的预测而且预测正确-所有说“是”的预测个数"><a href="#Precision-所有说“是”的预测而且预测正确-所有说“是”的预测个数" class="headerlink" title="Precision: 所有说“是”的预测而且预测正确/所有说“是”的预测个数"></a>Precision: 所有说“是”的预测而且预测正确/所有说“是”的预测个数</h3><p>—&gt; 6/9</p>
<h3 id="Recall-所有说“是”的预测而且预测正确-所有真正标签是“是”"><a href="#Recall-所有说“是”的预测而且预测正确-所有真正标签是“是”" class="headerlink" title="Recall:所有说“是”的预测而且预测正确 / 所有真正标签是“是”"></a>Recall:所有说“是”的预测而且预测正确 / 所有真正标签是“是”</h3><p>—&gt; 6/7</p>
<h3 id="Recall-和-Precision-其实往往是互相-tradeoff"><a href="#Recall-和-Precision-其实往往是互相-tradeoff" class="headerlink" title="Recall 和 Precision 其实往往是互相 tradeoff"></a>Recall 和 Precision 其实往往是互相 tradeoff</h3><h3 id="F1-Score-frac-2precisionrecall-precision-recall"><a href="#F1-Score-frac-2precisionrecall-precision-recall" class="headerlink" title="F1 Score = $ \frac{2precisionrecall}{precision + recall} $"></a>F1 Score = $ \frac{2<em>precision</em>recall}{precision + recall} $</h3><h3 id="—-gt-AOC-AUC"><a href="#—-gt-AOC-AUC" class="headerlink" title="—&gt;AOC / AUC"></a>—&gt;AOC / AUC</h3><h3 id="到这一步就无法分割了"><a href="#到这一步就无法分割了" class="headerlink" title="到这一步就无法分割了"></a>到这一步就无法分割了</h3><h3 id="A-simple-example-of-kmeans"><a href="#A-simple-example-of-kmeans" class="headerlink" title="A simple example of kmeans"></a>A simple example of kmeans</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X1 = [random.randint(<span class="number">0</span>,<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">X2 = [random.randint(<span class="number">0</span>,<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X1,X2)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x21e43481048&gt;
</code></pre><p><img src="output_87_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_data = [[x1,x2] <span class="keyword">for</span> x1,x2 <span class="keyword">in</span> zip(X1,X2)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster = KMeans(n_clusters=<span class="number">6</span>,max_iter=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.fit(training_data)</span><br></pre></td></tr></table></figure>
<pre><code>KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=500,
       n_clusters=6, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;,
       random_state=None, tol=0.0001, verbose=0)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.cluster_centers_</span><br></pre></td></tr></table></figure>
<pre><code>array([[85.5       , 41.78571429],
       [27.57894737, 18.89473684],
       [79.90909091, 84.22727273],
       [14.6       , 76.6       ],
       [51.06666667, 65.33333333],
       [62.73333333, 22.13333333]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.labels_</span><br></pre></td></tr></table></figure>
<pre><code>array([1, 3, 2, 5, 0, 1, 5, 0, 1, 2, 4, 3, 3, 4, 4, 0, 1, 0, 5, 0, 1, 4,
       1, 3, 4, 3, 5, 1, 1, 4, 2, 2, 3, 4, 2, 2, 5, 3, 2, 1, 4, 0, 3, 5,
       2, 4, 3, 2, 1, 3, 4, 1, 2, 5, 0, 5, 5, 0, 0, 2, 2, 1, 2, 2, 0, 0,
       4, 1, 5, 3, 5, 0, 2, 5, 3, 3, 5, 5, 5, 2, 1, 2, 2, 4, 1, 2, 4, 4,
       2, 1, 0, 1, 2, 3, 1, 2, 0, 1, 3, 4])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">centers = defaultdict(list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> label, location <span class="keyword">in</span> zip(cluster.labels_,training_data):</span><br><span class="line">    centers[label].append(location)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">color = [<span class="string">'red'</span>,<span class="string">'green'</span>,<span class="string">'grey'</span>,<span class="string">'black'</span>,<span class="string">'yellow'</span>,<span class="string">'orange'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,c <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">    <span class="keyword">for</span> location <span class="keyword">in</span> centers[c]:</span><br><span class="line">        plt.scatter(*location,c=color[i])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> center <span class="keyword">in</span> cluster.cluster_centers_:</span><br><span class="line">    plt.scatter(*center,s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_96_0.png" alt="png"></p>
<h3 id="Kmeans-的计算复杂度"><a href="#Kmeans-的计算复杂度" class="headerlink" title="Kmeans 的计算复杂度"></a>Kmeans 的计算复杂度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">()</span>:</span> <span class="keyword">return</span> np.sqrt((x1 - x2)**<span class="number">2</span> +(y1 - y2)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">O(I*N*k*d)</script><p>N:10000 k:100 d:500 I:500 -&gt;10**(5+2+2+2)=&gt;10^11 ==&gt;100亿</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>AI for NLP</title>
    <url>/2020/01/16/AI%20for%20NLP/</url>
    <content><![CDATA[<h2 id="Build-Graph"><a href="#Build-Graph" class="headerlink" title="Build Graph"></a>Build Graph</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">coordination_source = <span class="string">"""</span></span><br><span class="line"><span class="string">&#123;name:'兰州', geoCoord:[103.73, 36.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'嘉峪关', geoCoord:[98.17, 39.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'西宁', geoCoord:[101.74, 36.56]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'成都', geoCoord:[104.06, 30.67]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'石家庄', geoCoord:[114.48, 38.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'拉萨', geoCoord:[102.73, 25.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'贵阳', geoCoord:[106.71, 26.57]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'武汉', geoCoord:[114.31, 30.52]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'郑州', geoCoord:[113.65, 34.76]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'济南', geoCoord:[117, 36.65]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南京', geoCoord:[118.78, 32.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'合肥', geoCoord:[117.27, 31.86]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'杭州', geoCoord:[120.19, 30.26]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南昌', geoCoord:[115.89, 28.68]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'福州', geoCoord:[119.3, 26.08]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'广州', geoCoord:[113.23, 23.16]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'长沙', geoCoord:[113, 28.21]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'海口', geoCoord:[110.35, 20.02]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'沈阳', geoCoord:[123.38, 41.8]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'长春', geoCoord:[125.35, 43.88]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'哈尔滨', geoCoord:[126.63, 45.75]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'太原', geoCoord:[112.53, 37.87]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'西安', geoCoord:[108.95, 34.27]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'台湾', geoCoord:[121.30, 25.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'北京', geoCoord:[116.46, 39.92]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'上海', geoCoord:[121.48, 31.22]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'重庆', geoCoord:[106.54, 29.59]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'天津', geoCoord:[117.2, 39.13]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'呼和浩特', geoCoord:[111.65, 40.82]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南宁', geoCoord:[108.33, 22.84]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'西藏', geoCoord:[91.11, 29.97]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'银川', geoCoord:[106.27, 38.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'乌鲁木齐', geoCoord:[87.68, 43.77]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'香港', geoCoord:[114.17, 22.28]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'澳门', geoCoord:[113.54, 22.19]&#125;</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.findall(<span class="string">"[\d\.]+"</span>,<span class="string">"&#123;name:'澳门', geoCoord:[113.54, 22.19]&#125;"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;113.54&#39;, &#39;22.19&#39;]
</code></pre><h3 id="Get-data-from-source-using-regular-expression"><a href="#Get-data-from-source-using-regular-expression" class="headerlink" title="Get data from source using regular expression"></a>Get data from source using regular expression</h3><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure>
<h2 id="regular-expression"><a href="#regular-expression" class="headerlink" title="regular expression"></a>regular expression</h2><p>[a-z][A-Z]<sup><a href="#fn_a" id="reffn_a">a</a></sup>: negation colou?r: ? zero or onr of its previous character<br>colou?r: ? zero or onr of its previous character</p>
<ul>
<li><p>: zero or more of its previous character +: one or more.:match any single character  </p>
</li>
<li><p>: ^: start of the line </p>
</li>
<li>: $:end of the line</li>
<li>: | [cat|dog] : cat or dog</li>
<li>(da): make the string da like a character</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = <span class="string">"color or colour"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.compile(<span class="string">"colou?r"</span>)</span><br><span class="line">pattern.findall(l)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;color&#39;, &#39;colour&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_city_info</span><span class="params">(city_coordination)</span>:</span></span><br><span class="line">    city_location = &#123;&#125;</span><br><span class="line">    first = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> city_coordination.split(<span class="string">'\n'</span>):</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">'//'</span>): <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.strip()==<span class="string">""</span>:<span class="keyword">continue</span>    </span><br><span class="line">        city =re.findall(<span class="string">"name:'(\w+)'"</span>,line)[<span class="number">0</span>]</span><br><span class="line">        x_y = re.findall(<span class="string">"Coord:\[(\d+.\d+),\s(\d+.\d+)\]"</span>,line)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> first ==<span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"x_y: "</span>,x_y)</span><br><span class="line">        x_y = tuple(map(float,x_y))</span><br><span class="line">        city_location[city] = x_y</span><br><span class="line">        <span class="keyword">if</span> first ==<span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"city:"</span>,city )</span><br><span class="line">            print(<span class="string">"x_y: "</span>,x_y)</span><br><span class="line">            first =<span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> city_location</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_info = get_city_info(coordination_source)</span><br></pre></td></tr></table></figure>
<pre><code>x_y:  (&#39;103.73&#39;, &#39;36.03&#39;)
city: 兰州
x_y:  (103.73, 36.03)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_info</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;兰州&#39;: (103.73, 36.03),
 &#39;嘉峪关&#39;: (98.17, 39.47),
 &#39;西宁&#39;: (101.74, 36.56),
 &#39;成都&#39;: (104.06, 30.67),
 &#39;石家庄&#39;: (114.48, 38.03),
 &#39;拉萨&#39;: (102.73, 25.04),
 &#39;贵阳&#39;: (106.71, 26.57),
 &#39;武汉&#39;: (114.31, 30.52),
 &#39;郑州&#39;: (113.65, 34.76),
 &#39;济南&#39;: (117.0, 36.65),
 &#39;南京&#39;: (118.78, 32.04),
 &#39;合肥&#39;: (117.27, 31.86),
 &#39;杭州&#39;: (120.19, 30.26),
 &#39;南昌&#39;: (115.89, 28.68),
 &#39;福州&#39;: (119.3, 26.08),
 &#39;广州&#39;: (113.23, 23.16),
 &#39;长沙&#39;: (113.0, 28.21),
 &#39;沈阳&#39;: (123.38, 41.8),
 &#39;长春&#39;: (125.35, 43.88),
 &#39;哈尔滨&#39;: (126.63, 45.75),
 &#39;太原&#39;: (112.53, 37.87),
 &#39;西安&#39;: (108.95, 34.27),
 &#39;北京&#39;: (116.46, 39.92),
 &#39;上海&#39;: (121.48, 31.22),
 &#39;重庆&#39;: (106.54, 29.59),
 &#39;天津&#39;: (117.2, 39.13),
 &#39;呼和浩特&#39;: (111.65, 40.82),
 &#39;南宁&#39;: (108.33, 22.84),
 &#39;银川&#39;: (106.27, 38.47),
 &#39;乌鲁木齐&#39;: (87.68, 43.77),
 &#39;香港&#39;: (114.17, 22.28),
 &#39;澳门&#39;: (113.54, 22.19)}
</code></pre><h3 id="Compute-distance-between-cities"><a href="#Compute-distance-between-cities" class="headerlink" title="Compute distance between cities"></a>Compute distance between cities</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">geo_distance</span><span class="params">(origin, destination)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate the Haversine distance.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    origin: tuple of float</span></span><br><span class="line"><span class="string">        (lat,long)</span></span><br><span class="line"><span class="string">    destination: tuple of float</span></span><br><span class="line"><span class="string">        (lat,long)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    distance_in_km: float</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; origin = (48,1372,11.5756) #Munich</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; destination = (52.5186,13.4083) #Berlin</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; round(distance(origin,destination),1)</span></span><br><span class="line"><span class="string">    504.2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    lat1,lon1 = origin</span><br><span class="line">    lat2,lon2 = destination</span><br><span class="line">    radius =<span class="number">6371</span> <span class="comment"># km</span></span><br><span class="line">    </span><br><span class="line">    dlat = math.radians(lat2 - lat1)</span><br><span class="line">    dlon = math.radians(lon2 - lon1)</span><br><span class="line">    a = (math.sin(dlat / <span class="number">2</span>) * math.sin(dlat / <span class="number">2</span>) +</span><br><span class="line">         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *</span><br><span class="line">         math.sin(dlon / <span class="number">2</span>) * math.sin(dlon / <span class="number">2</span>))</span><br><span class="line">    c = <span class="number">2</span> * math.atan2(math.sqrt(a), math.sqrt(<span class="number">1</span> - a))</span><br><span class="line">    d = radius * c</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_city_distance</span><span class="params">(city1,city2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> geo_distance(city_info[city1],city_info[city2])</span><br><span class="line"></span><br><span class="line">get_city_distance(<span class="string">"上海"</span>,<span class="string">"北京"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>727.52769688981
</code></pre><h3 id="Draw-the-graph"><a href="#Draw-the-graph" class="headerlink" title="Draw the graph"></a>Draw the graph</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_graph =nx.Graph()</span><br><span class="line">city_graph.add_nodes_from(list(city_info.keys()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.draw(city_graph,city_info,with_labels=<span class="literal">True</span>,node_size=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/output_18_0.png" alt="png"></p>
<h3 id="Build-connection-between-Let’s-assume-that-two-cities-are-connected-if-their-distance-is-less-than-700km"><a href="#Build-connection-between-Let’s-assume-that-two-cities-are-connected-if-their-distance-is-less-than-700km" class="headerlink" title="Build connection between. Let’s assume that two cities are connected if their distance is less than 700km"></a>Build connection between. Let’s assume that two cities are connected if their distance is less than 700km</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">threshold = <span class="number">700</span> <span class="comment"># defined the threshold</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_connection</span><span class="params">(city_info)</span>:</span></span><br><span class="line">    cities_connection = defaultdict(list)</span><br><span class="line">    cities = list(city_info.keys())</span><br><span class="line">    <span class="keyword">for</span> c1 <span class="keyword">in</span> cities:</span><br><span class="line">        <span class="keyword">for</span> c2 <span class="keyword">in</span> cities:</span><br><span class="line">            <span class="keyword">if</span> c1 == c2 :</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> get_city_distance(c1,c2) &lt; threshold:</span><br><span class="line">                cities_connection[c1].append(c2)</span><br><span class="line">    <span class="keyword">return</span> cities_connection</span><br><span class="line"></span><br><span class="line">cities_connection = build_connection(city_info)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cities_connection</span><br></pre></td></tr></table></figure>
<pre><code>defaultdict(list,
            {&#39;兰州&#39;: [&#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;嘉峪关&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;],
             &#39;西宁&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;重庆&#39;, &#39;银川&#39;],
             &#39;成都&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;石家庄&#39;: [&#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;拉萨&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;贵阳&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;贵阳&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;武汉&#39;: [&#39;石家庄&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;郑州&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;济南&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;南京&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;],
             &#39;合肥&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;杭州&#39;: [&#39;武汉&#39;, &#39;济南&#39;, &#39;南京&#39;, &#39;合肥&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;北京&#39;, &#39;上海&#39;, &#39;天津&#39;],
             &#39;南昌&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;福州&#39;: [&#39;武汉&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;上海&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;广州&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;长沙&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;沈阳&#39;: [&#39;长春&#39;, &#39;哈尔滨&#39;, &#39;上海&#39;],
             &#39;长春&#39;: [&#39;沈阳&#39;, &#39;哈尔滨&#39;],
             &#39;哈尔滨&#39;: [&#39;沈阳&#39;, &#39;长春&#39;],
             &#39;太原&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;银川&#39;,
              &#39;澳门&#39;],
             &#39;西安&#39;: [&#39;兰州&#39;,
              &#39;成都&#39;,
              &#39;石家庄&#39;,
              &#39;贵阳&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;重庆&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;,
              &#39;银川&#39;],
             &#39;北京&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;上海&#39;: [&#39;济南&#39;, &#39;南京&#39;, &#39;合肥&#39;, &#39;杭州&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;沈阳&#39;, &#39;天津&#39;],
             &#39;重庆&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;呼和浩特&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;天津&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;呼和浩特&#39;],
             &#39;呼和浩特&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;重庆&#39;,
              &#39;天津&#39;,
              &#39;银川&#39;],
             &#39;南宁&#39;: [&#39;兰州&#39;,
              &#39;成都&#39;,
              &#39;拉萨&#39;,
              &#39;贵阳&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;西安&#39;,
              &#39;重庆&#39;,
              &#39;银川&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;银川&#39;: [&#39;兰州&#39;,
              &#39;西宁&#39;,
              &#39;成都&#39;,
              &#39;拉萨&#39;,
              &#39;贵阳&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;重庆&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;],
             &#39;香港&#39;: [&#39;武汉&#39;, &#39;郑州&#39;, &#39;合肥&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;广州&#39;, &#39;长沙&#39;, &#39;南宁&#39;, &#39;澳门&#39;],
             &#39;澳门&#39;: [&#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;]})
</code></pre><h3 id="Draw-connection-graph"><a href="#Draw-connection-graph" class="headerlink" title="Draw connection graph"></a>Draw connection graph</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cities_connection_graph = nx.Graph(cities_connection)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.draw(cities_connection_graph,city_info,with_labels = <span class="literal">True</span>, node_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/output_26_0.png" alt="png"></p>
<h3 id="BFS-1-version"><a href="#BFS-1-version" class="headerlink" title="BFS 1 version"></a>BFS 1 version</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_1</span><span class="params">(graph, start, destination)</span>:</span></span><br><span class="line">    pathes = [[start]] <span class="comment"># list 用来存储待搜索路径</span></span><br><span class="line">    visited = set() <span class="comment">#set 用来存车已搜索节点</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> pathes:</span><br><span class="line">        path= pathes.pop(<span class="number">0</span>) <span class="comment">#提取第一条路径</span></span><br><span class="line">        froniter = path[<span class="number">-1</span>] <span class="comment">#提取即将要探索的节点</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> froniter <span class="keyword">in</span> visited: <span class="keyword">continue</span> <span class="comment">#检查如果该点已经探索过，则不用探索</span></span><br><span class="line">        </span><br><span class="line">        successors = graph[froniter]</span><br><span class="line">        <span class="keyword">for</span> city <span class="keyword">in</span> successors: <span class="comment"># 遍历子节点</span></span><br><span class="line">            <span class="keyword">if</span> city <span class="keyword">in</span> path: <span class="keyword">continue</span> <span class="comment"># 检查会不会形成环</span></span><br><span class="line">            </span><br><span class="line">            new_path = path +[city]</span><br><span class="line">            pathes.append(new_path) <span class="comment">#bfs #将新路径加到list里面</span></span><br><span class="line">            <span class="comment"># pathes = [new_path] + pathes #dfs</span></span><br><span class="line">            <span class="keyword">if</span> city ==destination: <span class="comment">#检查是否到达目的地</span></span><br><span class="line">                <span class="keyword">return</span> new_path</span><br><span class="line">        visited.add(froniter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_1(cities_connection,<span class="string">"上海"</span>,<span class="string">"石家庄"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;上海&#39;, &#39;济南&#39;, &#39;石家庄&#39;]
</code></pre><h3 id="Optimal-search-using-variation-of-BFS"><a href="#Optimal-search-using-variation-of-BFS" class="headerlink" title="Optimal search using variation of BFS"></a>Optimal search using variation of BFS</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_2</span><span class="params">(graph,start,destination,search_strategy)</span>:</span></span><br><span class="line">    pathes = [[start]]</span><br><span class="line">    visited =set() <span class="comment"># !</span></span><br><span class="line">    <span class="keyword">while</span> pathes:</span><br><span class="line">        path = pathes.pop(<span class="number">0</span>)</span><br><span class="line">        froniter =path[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> froniter <span class="keyword">in</span> visited:<span class="keyword">continue</span> <span class="comment"># !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> froniter == destination:</span><br><span class="line">            <span class="keyword">return</span> path</span><br><span class="line">        successors = graph[froniter]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> city <span class="keyword">in</span> successors:</span><br><span class="line">            <span class="keyword">if</span> city <span class="keyword">in</span> path:<span class="keyword">continue</span> <span class="comment">#check loop</span></span><br><span class="line">            </span><br><span class="line">            new_path = path + [city]</span><br><span class="line">            pathes.append(new_path)</span><br><span class="line">        </span><br><span class="line">        pathes =search_strategy(pathes)</span><br><span class="line">        visited.add(froniter) <span class="comment"># !</span></span><br><span class="line">        <span class="comment">#</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_by_distance</span><span class="params">(pathes)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_distance_of_path</span><span class="params">(path)</span>:</span></span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(path[:<span class="number">-1</span>]):</span><br><span class="line">            distance += get_city_distance(path[i],path[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> distance</span><br><span class="line">    <span class="keyword">return</span> sorted(pathes,key =get_distance_of_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distance_of_path</span><span class="params">(path)</span>:</span></span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(path[:<span class="number">-1</span>]):</span><br><span class="line">            distance += get_city_distance(path[i],path[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> distance</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_distance_of_path([<span class="string">"北京"</span>,<span class="string">"济南"</span>,<span class="string">"上海"</span>])</span><br></pre></td></tr></table></figure>
<pre><code>752.66259009181
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_2(cities_connection,<span class="string">"北京"</span>,<span class="string">"上海"</span>,search_strategy=<span class="keyword">lambda</span> x:x)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;北京&#39;, &#39;济南&#39;, &#39;上海&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_2(cities_connection,<span class="string">"北京"</span>,<span class="string">"上海"</span>,search_strategy=sort_by_distance)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;北京&#39;, &#39;天津&#39;, &#39;上海&#39;]
</code></pre><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = load_boston()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dataset</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y=dataset[<span class="string">'data'</span>],dataset[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>
<pre><code>(506, 13)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure>
<pre><code>(506,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>(13,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,
       6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,
       1.7800e+01, 3.9690e+02, 9.1400e+00])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset.feature_names</span><br></pre></td></tr></table></figure>
<pre><code>array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;,
       &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[<span class="string">'DESCR'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&quot;.. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000&#39;s\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic\nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics\n...&#39;, Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n&quot;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_rm = x[:,<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plot the RM with respect to y</span></span><br><span class="line">plt.scatter(X_rm,y)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x11955a9f148&gt;
</code></pre><p><img src="/images/output_49_1.png" alt="png"></p>
<h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><h3 id="Assume-that-the-target-function-is-a-linear-function"><a href="#Assume-that-the-target-function-is-a-linear-function" class="headerlink" title="Assume that the target function is a linear function"></a>Assume that the target function is a linear function</h3><script type="math/tex; mode=display">y = k*rm +b</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define target function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(rm,k,b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> k*rm + b</span><br></pre></td></tr></table></figure>
<h3 id="Define-mean-square-loss"><a href="#Define-mean-square-loss" class="headerlink" title="Define mean square loss"></a>Define mean square loss</h3><script type="math/tex; mode=display">loss = \frac{1}{n} \sum{(y_i - \hat{y_i})}^2</script><script type="math/tex; mode=display">loss = \frac{1}{n}\sum{(y_i) - (kx_i + b_i))}^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum((y_i - y_hat_i)**<span class="number">2</span> <span class="keyword">for</span> y_i,y_hat_i <span class="keyword">in</span> zip(list(y),list(y_hat)))/len(list(y))</span><br></pre></td></tr></table></figure>
<h3 id="Define-partial-derivatives"><a href="#Define-partial-derivatives" class="headerlink" title="Define partial derivatives"></a>Define partial derivatives</h3><script type="math/tex; mode=display">\frac {\partial{loss}}{\partial{k}} = -\frac{2}{n}\sum(y_i -\hat{y_i})x_i</script><script type="math/tex; mode=display">\frac{\partial{loss}}{\partial{b}} = -\frac{2}{n}\sum(y_i - \hat{y_i})</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#define partial derivative</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_derivative_k</span><span class="params">(x,y,y_hat)</span>:</span></span><br><span class="line">    n = len(y)</span><br><span class="line">    gradient = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_i,y_i,y_hat_i <span class="keyword">in</span> zip(list(x),list(y),list(y_hat)):</span><br><span class="line">        gradient += (y_i - y_hat_i) * x_i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>/n * gradient</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_derivative_b</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    n =len(y)</span><br><span class="line">    gradient = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y_i,y_hat_i <span class="keyword">in</span> zip(list(y),list(y_hat)):</span><br><span class="line">        gradient+= (y_i-y_hat_i)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>/n* gradient</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#initialized parameters</span></span><br><span class="line">k= random.random()*<span class="number">200</span><span class="number">-100</span> <span class="comment"># -100 100</span></span><br><span class="line">b= random.random()*<span class="number">200</span><span class="number">-100</span> <span class="comment"># -100 100</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">iteration_num =<span class="number">200</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iteration_num):</span><br><span class="line">    price_use_current_parameters = [price(r,k,b) <span class="keyword">for</span> r <span class="keyword">in</span> X_rm] <span class="comment"># \hat&#123;y&#125;</span></span><br><span class="line">    </span><br><span class="line">    current_loss = loss(y,price_use_current_parameters)</span><br><span class="line">    losses.append(current_loss)</span><br><span class="line">    <span class="comment">#print("Iteration &#123;&#125;,the loss is &#123;&#125;,parameters k is &#123;&#125; and b is &#123;&#125;".format(i,current_loss,k,b))</span></span><br><span class="line">    </span><br><span class="line">    k_gradient = partial_derivative_k(X_rm,y,price_use_current_parameters)</span><br><span class="line">    b_gradient = partial_derivative_b(y,price_use_current_parameters)</span><br><span class="line">    </span><br><span class="line">    k = k + (<span class="number">-1</span> * k_gradient) * learning_rate</span><br><span class="line">    b = b + (<span class="number">-1</span> * b_gradient) * learning_rate</span><br><span class="line"></span><br><span class="line">best_k =k </span><br><span class="line">best_b =b</span><br><span class="line">print(<span class="string">"best_k is&#123;&#125;,best_b is &#123;&#125;"</span>.format(best_k,best_b))</span><br></pre></td></tr></table></figure>
<pre><code>best_k is12.41253129958806,best_b is -55.72859329657179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(list(range(iteration_num)),losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x119550b6fc8&gt;]
</code></pre><p><img src="/images/output_62_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">price_use_best_paramters = [price(r,best_k,best_b) <span class="keyword">for</span> r <span class="keyword">in</span> X_rm]</span><br><span class="line"></span><br><span class="line">plt.scatter(X_rm,y)</span><br><span class="line">plt.scatter(X_rm,price_use_current_parameters)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x11955a2d188&gt;
</code></pre><p><img src="/images/output_63_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Syntax-Tree</title>
    <url>/2020/01/11/Syntax-Tree/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simple_grammar =<span class="string">"""</span></span><br><span class="line"><span class="string">sentence =&gt; noun_phrase verb_phrase</span></span><br><span class="line"><span class="string">noun_phrase =&gt; Article Adj* noun</span></span><br><span class="line"><span class="string">Adj* =&gt; null | Adj Adj*</span></span><br><span class="line"><span class="string">verb_phrase =&gt; verb noun_phrase</span></span><br><span class="line"><span class="string">Article =&gt; 一个 | 这个</span></span><br><span class="line"><span class="string">noun =&gt; 女人 | 篮球 | 桌子 | 小猫</span></span><br><span class="line"><span class="string">verb =&gt; 看着 | 坐着 | 听见 | 看着</span></span><br><span class="line"><span class="string">Adj =&gt; 蓝色的 | 好看的 | 小小的</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">another_grammar = <span class="string">"""</span></span><br><span class="line"><span class="string">#</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adj</span><span class="params">()</span>:</span><span class="keyword">return</span> random.choice(<span class="string">'蓝色的|好看的|小小的'</span>.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adj_star</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> random.choice([<span class="keyword">lambda</span> : <span class="string">''</span>,<span class="keyword">lambda</span> : adj()+adj_star()])()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj_star()</span><br></pre></td></tr></table></figure>
<pre><code>&#39;蓝色的好看的小小的好看的小小的蓝色的&#39;
</code></pre><a id="more"></a>
<h1 id="but-the-question-is"><a href="#but-the-question-is" class="headerlink" title="but the question is ?"></a>but the question is ?</h1><p>如果我们更换了语法，会发现所有写过的过程都需要重写</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj_grammar =<span class="string">"""</span></span><br><span class="line"><span class="string">Adj* =&gt; null | Adj Adj*</span></span><br><span class="line"><span class="string">Adj =&gt; 蓝色的 | 好看的 | 小小的 </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_grammar</span><span class="params">(grammar_str,split = <span class="string">'=&gt;'</span>,line_split=<span class="string">'\n'</span>)</span>:</span></span><br><span class="line">    grammar = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> grammar_str.split(line_split):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.strip(): <span class="keyword">continue</span></span><br><span class="line">        exp,stmt = line.split(split)</span><br><span class="line">        grammar[exp.strip()]=[s.split() <span class="keyword">for</span> s <span class="keyword">in</span> stmt.split(<span class="string">'|'</span>)]</span><br><span class="line">    <span class="keyword">return</span> grammar</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grammar=create_grammar(adj_grammar)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grammar[<span class="string">'Adj*'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[[&#39;null&#39;], [&#39;Adj&#39;, &#39;Adj*&#39;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">choice = random.choice</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(gram,target)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> target <span class="keyword">not</span> <span class="keyword">in</span> gram: <span class="keyword">return</span> target <span class="comment">#means target is terminal expression</span></span><br><span class="line">    </span><br><span class="line">    expand = [generate(gram,t) <span class="keyword">for</span> t <span class="keyword">in</span> choice(gram[target])]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([e <span class="keyword">if</span> e != <span class="string">'/n'</span> <span class="keyword">else</span> <span class="string">'\n'</span> <span class="keyword">for</span> e <span class="keyword">in</span> expand <span class="keyword">if</span> e != <span class="string">'null'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_grammar =create_grammar(simple_grammar)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_grammar</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;sentence&#39;: [[&#39;noun_phrase&#39;, &#39;verb_phrase&#39;]],
 &#39;noun_phrase&#39;: [[&#39;Article&#39;, &#39;Adj*&#39;, &#39;noun&#39;]],
 &#39;Adj*&#39;: [[&#39;null&#39;], [&#39;Adj&#39;, &#39;Adj*&#39;]],
 &#39;verb_phrase&#39;: [[&#39;verb&#39;, &#39;noun_phrase&#39;]],
 &#39;Article&#39;: [[&#39;一个&#39;], [&#39;这个&#39;]],
 &#39;noun&#39;: [[&#39;女人&#39;], [&#39;篮球&#39;], [&#39;桌子&#39;], [&#39;小猫&#39;]],
 &#39;verb&#39;: [[&#39;看着&#39;], [&#39;坐着&#39;], [&#39;听见&#39;], [&#39;看着&#39;]],
 &#39;Adj&#39;: [[&#39;蓝色的&#39;], [&#39;好看的&#39;], [&#39;小小的&#39;]]}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generate(gram=example_grammar,target=<span class="string">'sentence'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;这个女人看着一个蓝色的蓝色的好看的桌子&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在西部世界里，一个 “人类”的语言可以定义为：</span></span><br><span class="line"></span><br><span class="line">human = <span class="string">"""</span></span><br><span class="line"><span class="string">human = 自己 寻找 活动</span></span><br><span class="line"><span class="string">自己 = 我 | 俺 | 我们</span></span><br><span class="line"><span class="string">寻找 = 找找 | 想找点</span></span><br><span class="line"><span class="string">活动 = 乐子 | 玩的</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#一个接待员的语言可以定义为</span></span><br><span class="line"></span><br><span class="line">host = <span class="string">"""</span></span><br><span class="line"><span class="string">host = 寒暄 报数 询问 业务相关 结尾</span></span><br><span class="line"><span class="string">报数 = 我是 数字 号 ,</span></span><br><span class="line"><span class="string">数字 = 单个数字 | 数字 单个数字</span></span><br><span class="line"><span class="string">单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9</span></span><br><span class="line"><span class="string">寒暄 = 称谓 打招呼 | 打招呼</span></span><br><span class="line"><span class="string">称谓 = 人称 ，</span></span><br><span class="line"><span class="string">人称 = tb | 神仙 | 傻狗</span></span><br><span class="line"><span class="string">打招呼 = 你好 | 您好</span></span><br><span class="line"><span class="string">询问 = 请问你要 | 您需要</span></span><br><span class="line"><span class="string">业务相关 = 玩玩 具体业务</span></span><br><span class="line"><span class="string">玩玩 = null</span></span><br><span class="line"><span class="string">具体业务 = 喝酒 | 打牌 | 打猎 | 赌博 | 踢球 | 飞翔</span></span><br><span class="line"><span class="string">结尾 = 吗？</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    print(generate(create_grammar(human,<span class="string">'='</span>),<span class="string">'human'</span>))</span><br><span class="line">    print(generate(create_grammar(host,<span class="string">'='</span>),<span class="string">'host'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>我想找点玩的
神仙，您好我是5号,您需要打牌吗？
我们想找点玩的
傻狗，你好我是4号,请问你要踢球吗？
俺想找点玩的
你好我是6号,请问你要打猎吗？
俺找找玩的
你好我是7723号,请问你要打猎吗？
我们想找点玩的
神仙，你好我是2号,您需要打牌吗？
俺找找玩的
神仙，你好我是66号,您需要打牌吗？
我找找玩的
傻狗，您好我是21号,您需要打牌吗？
我们想找点玩的
你好我是6号,您需要飞翔吗？
我们找找玩的
您好我是5号,请问你要打牌吗？
我找找玩的
您好我是9号,请问你要喝酒吗？
俺想找点乐子
你好我是6号,请问你要飞翔吗？
我们想找点玩的
你好我是8号,请问你要飞翔吗？
我找找乐子
您好我是99号,您需要踢球吗？
我想找点乐子
神仙，你好我是95号,您需要飞翔吗？
俺找找玩的
您好我是8号,请问你要飞翔吗？
俺找找乐子
傻狗，你好我是45号,请问你要喝酒吗？
俺找找乐子
您好我是68号,您需要赌博吗？
我们想找点乐子
神仙，你好我是3号,您需要打猎吗？
我们想找点乐子
佟博，您好我是521号,请问你要打猎吗？
我找找乐子
神仙，您好我是2189号,您需要赌博吗？
</code></pre><h1 id="希望能够生成最合理的一句话？"><a href="#希望能够生成最合理的一句话？" class="headerlink" title="希望能够生成最合理的一句话？"></a>希望能够生成最合理的一句话？</h1><h2 id="Data-Driven"><a href="#Data-Driven" class="headerlink" title="Data Driven"></a>Data Driven</h2><p>我们的目标是，希望能够在做一个程序，然后，当输入的数据变化的时候，我们的程序不用重写。Generalizatoin.<br>AI? 如何能够自动化解决巨额问题，我们找到一个方法后，输入变了，我们这个方法，不用变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simple_programming = <span class="string">"""</span></span><br><span class="line"><span class="string">programming =&gt; if_stmt | assign | while_loop</span></span><br><span class="line"><span class="string">while_loop =&gt; while ( cond ) &#123; change_line stmt change_line &#125;</span></span><br><span class="line"><span class="string">if_stmt =&gt; if ( cond ) &#123; change_line stmt change_line &#125; | if ( cond ) &#123; change_line stmt change_line &#125; else &#123; change_line stmt change_line &#125;</span></span><br><span class="line"><span class="string">change_line =&gt; /n</span></span><br><span class="line"><span class="string">cond =&gt; var op var</span></span><br><span class="line"><span class="string">op =&gt; | == | &lt; | &gt;= | &lt;=</span></span><br><span class="line"><span class="string">stmt =&gt; assign | if_stmt</span></span><br><span class="line"><span class="string">assign =&gt; var = var</span></span><br><span class="line"><span class="string">var =&gt; var _ num | words</span></span><br><span class="line"><span class="string">words =&gt; words _ word | word</span></span><br><span class="line"><span class="string">word =&gt; name | info | student | lib | database</span></span><br><span class="line"><span class="string">nums =&gt; nums num | num</span></span><br><span class="line"><span class="string">num =&gt; 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(generate(gram=create_grammar(simple_programming,split=<span class="string">'=&gt;'</span>),target=<span class="string">'programming'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>if(lib_name_info&gt;=lib_student){
if(name_2&lt;=name_database){
if(lib&lt;info_lib_database_7){
database=lib_lib
}else{
info_6=info
}
}
}else{
info_0=name_5
}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_print</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="comment">#utility ttool function</span></span><br><span class="line">    lines = line.split(<span class="string">'\n'</span>)</span><br><span class="line">    </span><br><span class="line">    code_lines = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i , sen <span class="keyword">in</span> enumerate(lines):</span><br><span class="line">        <span class="keyword">if</span> i &lt; len(lines) /<span class="number">2</span>:</span><br><span class="line">            <span class="comment">#print()</span></span><br><span class="line">            code_lines.append(i * <span class="string">"  "</span> + sen)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code_lines.append((len(lines) -i )* <span class="string">" "</span> +sen)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> code_lines</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generated_programming = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    generated_programming += pretty_print(generate(gram=create_grammar(simple_programming,split=<span class="string">'=&gt;'</span>),target=<span class="string">'programming'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> generated_programming:</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>if(student==name){
  if(database_student&lt;=lib){
    if(student_student_lib_databasename_name){
      info=info
        }else{
          if(info==name){
            if(lib&lt;info_3){
              if(student_lib_lib_9&gt;=database){
       info_6=student_database_lib_lib
      }
     }
    }
   }
  }
 }
while(lib_0&lt;name_name_name_info){
  info_lib_database_name=info
 }
database_info_database_3=lib_7_5_0_8_5_7_3
database_0=info
while(lib_1_9&lt;=student_lib_student_lib){
  if(student_name&lt;info_database_0){
    if(database&lt;=student_database){
      if(name&gt;=name){
        info_info=student_info_lib_name_0
          }
            }else{
              if(lib_database_5_0==name_2){
       lib_database_database_4=lib_5
      }
     }
    }else{
   info=database_database_database_0
  }
 }
while(lib_database_lib&lt;=student_6){
  if(studentstudent_2){
    name_database_7=lib_name_name_7
      }else{
        if(info_1&gt;=database_4_8){
          info=info_1_3_7
            }else{
              if(name_9_8_9&lt;database){
                if(database_info_lib_3_8database_7){
                  if(name_7&gt;=lib_name_student){
                    if(student_lib_info_info_2_4_8&lt;lib_6_2_2){
                      student_lib_database=student_info_3
           }else{
          info_lib=student_database
         }
        }
       }
      }else{
     database=lib_student_info_name_student_3_1_7_3
    }
   }
  }
 }
if(student&gt;=name_student_database_lib_lib){
  name_database_3_4=lib_name_info_info_name_student_info_name
    }else{
  lib_database=info_lib_student
 }
name=name_5_2_3_2_4_5_3_2_0_3_7
info_6=info_student_1
info=lib_9
if(student&gt;=database_2){
  name_4_5=student
 }
if(student_3_3_6&lt;=name_info_database_8){
  if(student_6&gt;=database_student_4){
    if(info_2_2_7==info_6_4){
      name_8_1=info_student_lib
        }else{
          database_5=database_1
            }
              }
                }else{
                  if(lib==database_lib){
                    lib_0_9_1=lib_0
          }else{
         if(lib_database&gt;=info_student_0){
        student_name_8=name
       }else{
      if(info_name_lib_name_student_database_lib_name&lt;=student_2){
     student_info_0_9_9_1=database
    }
   }
  }
 }
while(lib_3_8&lt;=database_0){
  if(name_9==lib){
    if(lib_name_lib_5info){
      if(lib_name_database_3lib_2){
        database_info_info=student_1
          }
            }
              }else{
       if(name&lt;database_5_1_5_8){
      student_student=info
     }else{
    info_info_7=student_database_database_info_name_1_6
   }
  }
 }
database_database=info_student_database_lib_student
lib=database
student_database_database=name_6
while(student_database&lt;info){
  if(info_student_database_student_lib_name_info&lt;=info){
    lib_4_5_2=name_info
      }else{
   lib_2=database_info_name
  }
 }
while(name_3name_lib){
  info=lib
 }
while(info_libinfo_4_4_3){
  info_name_info_lib_9_9=database_0_4_3
 }
if(lib_database_name&lt;student_name_0){
  if(name_9_4_4_6student_info_student){
    if(name_0lib_1){
      lib_lib=database_student_student
        }
          }else{
            if(infostudent){
              lib_0=student
                }
                  }
                    }else{
                      if(student_name_info_0_9info_lib_9){
                        database_name_student=name
                          }else{
                            if(student&lt;=name){
                              database=info_info_lib_name_lib_8_7
                                }else{
                                  if(name_namedatabase_lib_name_8_6){
                 if(info==name){
                info_7_9_5_1_6=info_database
               }else{
              if(name_student_info_8&lt;=student_database){
             info_name_database_database_database=lib_1
            }else{
           if(name&lt;=info_info_student_student_database_info_lib_lib_info){
          student_lib=name_student
         }
        }
       }
      }else{
     info_6_8_3=info_9_3_7
    }
   }
  }
 }
</code></pre><h1 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h1><script type="math/tex; mode=display">language\_model(string) = Probality(String)\in(0,1)</script><script type="math/tex; mode=display">Pro(w_1 w_2 w_3 w_4) = Pr(w_1 | w_2 w_3 w_4) * Pr(w_2 | w_3 w_4) *Pr(w_3 | w_4)*Pr(w_4)</script><p>how to get $ Pr(w_1 |w_2 w_3 w_4)$?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.choice(range(<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<pre><code>86
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename=<span class="string">'/Users/24768/sqlResult_1558435.csv'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">content =pd.read_csv(filename,encoding=<span class="string">'gb18030'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">content.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>author</th>
      <th>source</th>
      <th>content</th>
      <th>feature</th>
      <th>title</th>
      <th>url</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>89617</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"37"...</td>
      <td>小米MIUI 9首批机型曝光：共计15款</td>
      <td>http://www.cnbeta.com/articles/tech/623597.htm</td>
    </tr>
    <tr>
      <td>1</td>
      <td>89616</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"15"...</td>
      <td>骁龙835在Windows 10上的性能表现有望改善</td>
      <td>http://www.cnbeta.com/articles/tech/623599.htm</td>
    </tr>
    <tr>
      <td>2</td>
      <td>89615</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\r\n...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"18"...</td>
      <td>一加手机5细节曝光：3300mAh、充半小时用1天</td>
      <td>http://www.cnbeta.com/articles/tech/623601.htm</td>
    </tr>
    <tr>
      <td>3</td>
      <td>89614</td>
      <td>NaN</td>
      <td>新华社</td>
      <td>这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\r\n</td>
      <td>{"type":"国际新闻","site":"环球","commentNum":"0","j...</td>
      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）</td>
      <td>http://world.huanqiu.com/hot/2017-06/10866126....</td>
    </tr>
    <tr>
      <td>4</td>
      <td>89613</td>
      <td>胡淑丽_MN7479</td>
      <td>深圳大件事</td>
      <td>（原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\r\n@深圳交警微博称：昨日清...</td>
      <td>{"type":"新闻","site":"网易热门","commentNum":"978",...</td>
      <td>44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随</td>
      <td>http://news.163.com/17/0618/00/CN617P3Q0001875...</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">articles =content[<span class="string">'content'</span>].tolist()</span><br><span class="line">print(articles[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。\r\nMIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。\r\n当然，关于MIUI 9的确切信息，我们还是等待官方消息。\r\n&#39;, &#39;骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考虑性能而去屏蔽掉小核心。相反，他们正联手微软，找到一种适合桌面平台的、兼顾性能和功耗的完美方案。\r\n报道称，微软已经拿到了一些新的源码，以便Windows 10更好地理解big.little架构。\r\n资料显示，骁龙835作为一款集成了CPU、GPU、基带、蓝牙/Wi-Fi的SoC，比传统的Wintel方案可以节省至少30%的PCB空间。\r\n按计划，今年Q4，华硕、惠普、联想将首发骁龙835 Win10电脑，预计均是二合一形态的产品。\r\n当然，高通骁龙只是个开始，未来也许还能见到三星Exynos、联发科、华为麒麟、小米澎湃等进入Windows 10桌面平台。\r\n&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(articles)</span><br></pre></td></tr></table></figure>
<pre><code>89611
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">token</span><span class="params">(string)</span>:</span></span><br><span class="line">    <span class="comment"># we will learn the regular expression next course</span></span><br><span class="line">    <span class="keyword">return</span> re.findall(<span class="string">'\w+'</span>,string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">with_jieba_cut = Counter(jieba.cut(articles[<span class="number">110</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Building prefix dict from the default dictionary ...
Dumping model to file cache C:\Users\24768\AppData\Local\Temp\jieba.cache
Loading model cost 0.957 seconds.
Prefix dict has been built succesfully.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">with_jieba_cut.most_common()[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;，&#39;, 88),
 (&#39;的&#39;, 73),
 (&#39;。&#39;, 39),
 (&#39;\r\n&#39;, 27),
 (&#39;了&#39;, 20),
 (&#39;们&#39;, 18),
 (&#39;工作队&#39;, 16),
 (&#39;村民&#39;, 15),
 (&#39;收割&#39;, 14),
 (&#39;、&#39;, 12)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">''</span>.join(token(articles[<span class="number">110</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&#39;在外国名著麦田里的守望者中作者想要守护麦田里如自己内心一般纯真的孩子们而驻村干部们也在这个炎热的夏天里撸袖子上阵真正做起了村民们的麦田守望者三夏时节不等人你看到了吗不停翻涌起伏仿若铺陈至天边的金黄麦浪中那若隐若现的人影是自治区新闻出版广电局驻和田市肖尔巴格乡合尼村工作队的队员与工作队组织的青年志愿者在这个炎热的夏季他们深入田间地头帮助村民们收割小麦扛起收麦机麦田中的每个人都显得兴致勃勃一天下来就近22亩小麦收割完毕志愿者麦麦提亚森擦去满脸的汗水高兴地告诉驻村队员我们青年志愿者应该多做贡献为村里的脱贫致富出把力工作队带着我们为村里的老人服务看到那些像我爷爷奶奶一样的老人赞许感谢的目光我体会到了帮助他人的快乐自治区新闻出版广电局驻村工作队孙敏艾力依布拉音麦收时节我们在一起6月中旬的和田墨玉麦田金黄静待收割6月14日15日两天自治区高级人民法院驻和田地区墨玉县吐外特乡罕勒克艾日克村工作队与48名村民志愿者一道帮助村里29户有需要的村民进行小麦收割工作田间地头罕勒克艾日克村志愿队的红旗迎风飘扬格外醒目10余台割麦机一起轰鸣男人们在用机器收割小麦的同时几名妇女也加入到志愿队构成了一道美丽的麦收风景休息空闲工作队员和村民们坐在树荫下田埂上互相问好聊天语言交流有困难就用手势动作比划着聊天有趣地交流方式不时引来阵阵欢笑大家在一同享受丰收和喜悦也一同增进着彼此的情感和友谊自治区高级人民法院驻村工作队周春梅艾地艾木阿不拉细看稻菽千重浪6月15日自治区煤田灭火工程局的干部职工们再一次跋涉1000多公里来到了叶城县萨依巴格乡阿亚格欧尔达贝格村见到了自己的亲戚现场处处都透出掩盖不住的喜悦一声声亲切的谢谢一个个结实的拥抱都透露出浓浓的亲情没坐一会儿在嘘寒问暖中大家了解到在麦收的关键时刻部分村民家中却存在收割难的问题小麦成熟期短收获的时间集中天气的变化对小麦最终产量的影响极大如果不能及时收割会有不小损失的于是大家几乎立刻就决定要帮助亲戚们收割麦子在茂密的麦地里干部们每人手持一把镰刀一字排开挽起衣袖卷起裤腿挥舞着镰刀进行着无声的竞赛骄阳似火汗如雨下但这都挡不住大家的热情随着此起彼伏的镰刀割倒麦子的刷刷声响不一会一束束沉甸甸的麦穗就被整齐地堆放了起来当看到自己亲手收割的金黄色麦穗被一簇簇地打成捆运送到晒场每个人的脸上都露出了灿烂的笑容自治区煤田灭火工程局驻村工作队马浩南这是一个收获多多的季节6月13日清晨6时许和田地区民丰县若雅乡特开墩村的麦田里已经传来马达轰鸣声原来是自治区质监局驻村工作队趁着天气尚且凉爽开始了麦田的收割工作忙碌间隙志愿者队伍搬来清凉的水村民们拎来鲜甜的西瓜抹一把汗水吃一牙西瓜甜蜜的汁水似乎流进了每一个人的心里说起割麦子对于生活在这片土地上的村民来说是再平常不过的事但是对于工作队队员们来说却是陌生的自治区质监局驻民丰县若克雅乡博斯坦村工作队队员们一开始觉得十几个人一起收割二亩地应该会挺快的结果却一点不简单镰刀拿到自己手里割起来考验才真正的开始大家弓着腰弯着腿亦步亦趋手上挥舞着镰刀时刻注意不要让镰刀割到自己脚下还要留心不要把套种的玉米苗踩伤不一会儿就已经汗流浃背了抬头看看身边的村民早就远远地割到前面去了只有今年已经56岁的工作队队长李树刚有割麦经验多少给队员们挽回了些面子赶不上村民们割麦子的速度更不要说搞定收割机这台大家伙了现代化的机械收割能成倍提升小麦的收割速度李树刚说不过能有这样的体验拉近和村民的距离也是很难得的体验自治区质监局驻村工作队王辉马君刚我们是麦田的守护者为了应对麦收新疆银监局驻和田县塔瓦库勒乡也先巴扎村工作队一早就从经济支援和人力支援两方面做好了准备一方面工作队帮村里购入了5台小麦收割机另一边还组织村干部青年团员等组成了6支近百人的收割先锋突击队帮助村民们抢收麦子看着及时归仓的麦子村民们喜得合不拢嘴纷纷摘下自家杏树上的杏子送给工作队金黄的麦穗温暖了村民们的心香甜的杏子温暖了工作队员的心麦子加杏子拉近了村民和队员们的心新疆银监局驻村工作队王继发免责声明本文仅代表作者个人观点与环球网无关其原创性以及文中陈述文字和内容未经本站证实对本文以及其中全部或者部分内容文字的真实性完整性及时性本站不作任何保证或承诺请读者仅作参考并请自行核实相关内容&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">articles_clean = [<span class="string">''</span>.join(token(str(a))) <span class="keyword">for</span> a <span class="keyword">in</span> articles]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(articles_clean)</span><br></pre></td></tr></table></figure>
<pre><code>89611
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'article_tb.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> articles_clean:</span><br><span class="line">        f.write(a + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(string)</span>:</span> <span class="keyword">return</span> list(jieba.cut(string))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate((open(<span class="string">'article_tb.txt'</span>))):</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> ==<span class="number">0</span>: print(i)</span><br><span class="line">    <span class="keyword">if</span> i &gt;<span class="number">100000</span>:<span class="keyword">break</span></span><br><span class="line">    TOKEN.extend(cut(line))</span><br></pre></td></tr></table></figure>
<pre><code>0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
40200
40300
40400
40500
40600
40700
40800
40900
41000
41100
41200
41300
41400
41500
41600
41700
41800
41900
42000
42100
42200
42300
42400
42500
42600
42700
42800
42900
43000
43100
43200
43300
43400
43500
43600
43700
43800
43900
44000
44100
44200
44300
44400
44500
44600
44700
44800
44900
45000
45100
45200
45300
45400
45500
45600
45700
45800
45900
46000
46100
46200
46300
46400
46500
46600
46700
46800
46900
47000
47100
47200
47300
47400
47500
47600
47700
47800
47900
48000
48100
48200
48300
48400
48500
48600
48700
48800
48900
49000
49100
49200
49300
49400
49500
49600
49700
49800
49900
50000
50100
50200
50300
50400
50500
50600
50700
50800
50900
51000
51100
51200
51300
51400
51500
51600
51700
51800
51900
52000
52100
52200
52300
52400
52500
52600
52700
52800
52900
53000
53100
53200
53300
53400
53500
53600
53700
53800
53900
54000
54100
54200
54300
54400
54500
54600
54700
54800
54900
55000
55100
55200
55300
55400
55500
55600
55700
55800
55900
56000
56100
56200
56300
56400
56500
56600
56700
56800
56900
57000
57100
57200
57300
57400
57500
57600
57700
57800
57900
58000
58100
58200
58300
58400
58500
58600
58700
58800
58900
59000
59100
59200
59300
59400
59500
59600
59700
59800
59900
60000
60100
60200
60300
60400
60500
60600
60700
60800
60900
61000
61100
61200
61300
61400
61500
61600
61700
61800
61900
62000
62100
62200
62300
62400
62500
62600
62700
62800
62900
63000
63100
63200
63300
63400
63500
63600
63700
63800
63900
64000
64100
64200
64300
64400
64500
64600
64700
64800
64900
65000
65100
65200
65300
65400
65500
65600
65700
65800
65900
66000
66100
66200
66300
66400
66500
66600
66700
66800
66900
67000
67100
67200
67300
67400
67500
67600
67700
67800
67900
68000
68100
68200
68300
68400
68500
68600
68700
68800
68900
69000
69100
69200
69300
69400
69500
69600
69700
69800
69900
70000
70100
70200
70300
70400
70500
70600
70700
70800
70900
71000
71100
71200
71300
71400
71500
71600
71700
71800
71900
72000
72100
72200
72300
72400
72500
72600
72700
72800
72900
73000
73100
73200
73300
73400
73500
73600
73700
73800
73900
74000
74100
74200
74300
74400
74500
74600
74700
74800
74900
75000
75100
75200
75300
75400
75500
75600
75700
75800
75900
76000
76100
76200
76300
76400
76500
76600
76700
76800
76900
77000
77100
77200
77300
77400
77500
77600
77700
77800
77900
78000
78100
78200
78300
78400
78500
78600
78700
78800
78900
79000
79100
79200
79300
79400
79500
79600
79700
79800
79900
80000
80100
80200
80300
80400
80500
80600
80700
80800
80900
81000
81100
81200
81300
81400
81500
81600
81700
81800
81900
82000
82100
82200
82300
82400
82500
82600
82700
82800
82900
83000
83100
83200
83300
83400
83500
83600
83700
83800
83900
84000
84100
84200
84300
84400
84500
84600
84700
84800
84900
85000
85100
85200
85300
85400
85500
85600
85700
85800
85900
86000
86100
86200
86300
86400
86500
86600
86700
86800
86900
87000
87100
87200
87300
87400
87500
87600
87700
87800
87900
88000
88100
88200
88300
88400
88500
88600
88700
88800
88900
89000
89100
89200
89300
89400
89500
89600
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add,mul</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reduce(add,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<pre><code>23
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count =Counter(TOKEN)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count.most_common(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;的&#39;, 703716),
 (&#39;n&#39;, 382020),
 (&#39;在&#39;, 263597),
 (&#39;月&#39;, 189330),
 (&#39;日&#39;, 166300),
 (&#39;新华社&#39;, 142462),
 (&#39;和&#39;, 134061),
 (&#39;年&#39;, 123106),
 (&#39;了&#39;, 121938),
 (&#39;是&#39;, 100909),
 (&#39;\n&#39;, 89611),
 (&#39;１&#39;, 88187),
 (&#39;０&#39;, 84945),
 (&#39;外代&#39;, 83268),
 (&#39;中&#39;, 73926),
 (&#39;中国&#39;, 71179),
 (&#39;２&#39;, 70521),
 (&#39;2017&#39;, 69894),
 (&#39;记者&#39;, 62147),
 (&#39;二线&#39;, 61998),
 (&#39;将&#39;, 61420),
 (&#39;与&#39;, 58309),
 (&#39;等&#39;, 58162),
 (&#39;为&#39;, 57019),
 (&#39;5&#39;, 54578),
 (&#39;照片&#39;, 52271),
 (&#39;4&#39;, 51626),
 (&#39;对&#39;, 50317),
 (&#39;上&#39;, 47452),
 (&#39;也&#39;, 47401),
 (&#39;有&#39;, 45767),
 (&#39;５&#39;, 40857),
 (&#39;说&#39;, 39017),
 (&#39;发展&#39;, 37632),
 (&#39;他&#39;, 37194),
 (&#39;３&#39;, 36906),
 (&#39;以&#39;, 36867),
 (&#39;国际&#39;, 35842),
 (&#39;nn&#39;, 35330),
 (&#39;４&#39;, 34659),
 (&#39;比赛&#39;, 32232),
 (&#39;６&#39;, 30575),
 (&#39;到&#39;, 30109),
 (&#39;人&#39;, 29572),
 (&#39;从&#39;, 29489),
 (&#39;6&#39;, 29002),
 (&#39;都&#39;, 28027),
 (&#39;不&#39;, 27963),
 (&#39;后&#39;, 27393),
 (&#39;当日&#39;, 27186),
 (&#39;就&#39;, 26684),
 (&#39;并&#39;, 26568),
 (&#39;国家&#39;, 26439),
 (&#39;７&#39;, 26386),
 (&#39;企业&#39;, 26147),
 (&#39;进行&#39;, 25987),
 (&#39;3&#39;, 25491),
 (&#39;美国&#39;, 25485),
 (&#39;举行&#39;, 25389),
 (&#39;被&#39;, 25277),
 (&#39;北京&#39;, 25245),
 (&#39;体育&#39;, 24873),
 (&#39;2&#39;, 24376),
 (&#39;1&#39;, 24182),
 (&#39;这&#39;, 24118),
 (&#39;新&#39;, 23828),
 (&#39;但&#39;, 23385),
 (&#39;比&#39;, 23229),
 (&#39;个&#39;, 23081),
 (&#39;足球&#39;, 22554),
 (&#39;表示&#39;, 22134),
 (&#39;经济&#39;, 22006),
 (&#39;我&#39;, 21940),
 (&#39;一个&#39;, 21932),
 (&#39;９&#39;, 21920),
 (&#39;还&#39;, 21861),
 (&#39;合作&#39;, 21567),
 (&#39;要&#39;, 21045),
 (&#39;n5&#39;, 20946),
 (&#39;已&#39;, 20882),
 (&#39;摄&#39;, 20837),
 (&#39;８&#39;, 20701),
 (&#39;工作&#39;, 20700),
 (&#39;n4&#39;, 20658),
 (&#39;选手&#39;, 19986),
 (&#39;我们&#39;, 19982),
 (&#39;市场&#39;, 19001),
 (&#39;一路&#39;, 18978),
 (&#39;一带&#39;, 18907),
 (&#39;建设&#39;, 18634),
 (&#39;让&#39;, 18609),
 (&#39;日电&#39;, 18384),
 (&#39;通过&#39;, 18159),
 (&#39;多&#39;, 17760),
 (&#39;时&#39;, 17750),
 (&#39;完&#39;, 17424),
 (&#39;于&#39;, 17421),
 (&#39;问题&#39;, 17338),
 (&#39;更&#39;, 17275),
 (&#39;项目&#39;, 17260)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">frequiences = [f <span class="keyword">for</span> w,f <span class="keyword">in</span> words_count.most_common(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x =[ i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#内嵌画图</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x,frequiences)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x16f678b8b48&gt;]
</code></pre><p><img src="/images/output_58_1.png" alt="png"></p>
<p>NLP比较重要的规律，在很大的一个 text corpus, 文字集合中，出现频率第二多的单词，是出现频率第一多单词频率的1/2，出现频率滴N多的的单词是第一多的1/N</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x,np.log(frequiences))</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x16efa34bf08&gt;]
</code></pre><p><img src="/images/output_61_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_1</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> words_count[word] / len(TOKEN)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_1(<span class="string">'我们'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.0011341645999654677
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN[:<span class="number">10</span>]</span><br><span class="line">len(TOKEN[:<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>17618253
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN = [str(t) <span class="keyword">for</span> t <span class="keyword">in</span> TOKEN]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN_2_GRAM = [<span class="string">''</span>.join(TOKEN[i:i+<span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(TOKEN[:<span class="number">-2</span>]))]</span><br><span class="line">len(TOKEN[:<span class="number">-2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>17618252
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN_2_GRAM[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;此外自&#39;, &#39;自本周&#39;, &#39;本周6&#39;, &#39;6月&#39;, &#39;月12&#39;, &#39;12日起&#39;, &#39;日起除&#39;, &#39;除小米&#39;, &#39;小米手机&#39;, &#39;手机6&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count_2 =Counter(TOKEN_2_GRAM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_2</span><span class="params">(word1,word2)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> word1+word2 <span class="keyword">in</span> words_count_2: <span class="keyword">return</span> words_count_2[word1+word2] / len(TOKEN_2_GRAM)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / len(TOKEN_2_GRAM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_2(<span class="string">'我们'</span>,<span class="string">'在'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>3.0536514065072974e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_2(<span class="string">'去'</span>,<span class="string">'吃饭'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>7.946304775297799e-07
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probablity</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    words = cut(sentence)</span><br><span class="line">    sentence_prob =<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i , word <span class="keyword">in</span>  enumerate(words[:<span class="number">-1</span>]):</span><br><span class="line">        next_ =words[i+<span class="number">1</span>]</span><br><span class="line">        probablity =prob_2(word,next_)</span><br><span class="line">        sentence_prob *= probablity</span><br><span class="line">    sentence_prob*=prob_1(word[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> sentence_prob</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_probablity(<span class="string">'fd是一个小可爱'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>5.193529218987722e-22
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_probablity(<span class="string">'小敏今天抽到了一个iphone'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>9.08660601226132e-36
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sen <span class="keyword">in</span> [generate(gram=example_grammar,target=<span class="string">'sentence'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]:</span><br><span class="line">    print(<span class="string">'sentence: &#123;&#125; with Prb: &#123;&#125;'</span>.format(sen,get_probablity(sen)))</span><br></pre></td></tr></table></figure>
<pre><code>sentence: 一个好看的小猫坐着一个小猫 with Prb: 3.288492351824868e-47
sentence: 这个蓝色的小小的好看的女人看着一个好看的蓝色的好看的小猫 with Prb: 1.4336960791945416e-97
sentence: 一个桌子看着这个篮球 with Prb: 1.6316301805544588e-31
sentence: 一个小小的蓝色的好看的小猫听见一个小猫 with Prb: 5.413402114895177e-62
sentence: 这个女人听见一个女人 with Prb: 4.894890541663377e-31
sentence: 这个好看的小小的女人看着这个好看的篮球 with Prb: 7.764808720237364e-62
sentence: 这个桌子看着一个好看的篮球 with Prb: 2.948877435423709e-42
sentence: 这个小小的好看的篮球看着这个小小的小小的小小的小猫 with Prb: 1.4874477576323182e-68
sentence: 这个篮球坐着一个好看的好看的桌子 with Prb: 4.8451281079684025e-59
sentence: 这个女人听见这个好看的篮球 with Prb: 2.948877435423709e-42
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">need_compared = [</span><br><span class="line">    <span class="string">"今天晚上请你吃饭，我们一起吃火锅 明天晚上请你吃饭，我们一起吃烧烤"</span>,</span><br><span class="line">    <span class="string">"真实一只可爱的小猫 真是一只可爱的小猫"</span>,</span><br><span class="line">    <span class="string">"今天晚上去飞翔 今天晚上飞翔去我"</span>,</span><br><span class="line">    <span class="string">"洋葱奶昔来一杯 养乐多绿来一杯"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> need_compared:</span><br><span class="line">    s1,s2 =s.split()</span><br><span class="line">    p1,p2 = get_probablity(s1),get_probablity(s2)</span><br><span class="line">    </span><br><span class="line">    better = s1 <span class="keyword">if</span> p1 &gt; p2 <span class="keyword">else</span> s2</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125; is more possible'</span>.format(better))</span><br><span class="line">    print(<span class="string">'-'</span>*<span class="number">4</span> + <span class="string">' &#123;&#125; with probablity &#123;&#125;'</span>.format(s1,p1))</span><br><span class="line">    print(<span class="string">'-'</span>*<span class="number">4</span> + <span class="string">' &#123;&#125; with probablity &#123;&#125;'</span>.format(s2,p2))</span><br></pre></td></tr></table></figure>
<pre><code>今天晚上请你吃饭，我们一起吃火锅 is more possible
---- 今天晚上请你吃饭，我们一起吃火锅 with probablity 6.195089781185323e-58
---- 明天晚上请你吃饭，我们一起吃烧烤 with probablity 4.512004197519248e-64
真是一只可爱的小猫 is more possible
---- 真实一只可爱的小猫 with probablity 5.014470336000062e-27
---- 真是一只可爱的小猫 with probablity 5.014470336000062e-27
今天晚上去飞翔 is more possible
---- 今天晚上去飞翔 with probablity 1.1061245254231737e-24
---- 今天晚上飞翔去我 with probablity 2.3020387757829988e-31
养乐多绿来一杯 is more possible
---- 洋葱奶昔来一杯 with probablity 1.1160363145085983e-25
---- 养乐多绿来一杯 with probablity 1.9662609030163744e-18
</code></pre><h1 id="Data-Driven-1"><a href="#Data-Driven-1" class="headerlink" title="Data Driven"></a>Data Driven</h1><h2 id="假如你做了很久的数据预处理"><a href="#假如你做了很久的数据预处理" class="headerlink" title="假如你做了很久的数据预处理"></a>假如你做了很久的数据预处理</h2><h2 id="AI的问题里边，65-都在做数据预处理"><a href="#AI的问题里边，65-都在做数据预处理" class="headerlink" title="AI的问题里边，65%都在做数据预处理"></a>AI的问题里边，65%都在做数据预处理</h2><h2 id="我们养成一个习惯，把数据存到硬盘里"><a href="#我们养成一个习惯，把数据存到硬盘里" class="headerlink" title="我们养成一个习惯，把数据存到硬盘里"></a>我们养成一个习惯，把数据存到硬盘里</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Syntax-Tree</tag>
        <tag>Language Model</tag>
      </tags>
  </entry>
  <entry>
    <title>python BinarySearchTree</title>
    <url>/2019/11/29/python-BinarySearchTree/</url>
    <content><![CDATA[<h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p>&emsp;&emsp;最近在学python，断断续续的，感觉学的慢，就顺便写写代码，加强对python的感觉。<br>虽然学了半天还是啥都不会，但是还是要写写博客啥的，激励自己不老懒惰，不能因为眼前的</p>
<h2 id="困难而放弃进步。所以，下面我要贴代码了…"><a href="#困难而放弃进步。所以，下面我要贴代码了…" class="headerlink" title="困难而放弃进步。所以，下面我要贴代码了…."></a>困难而放弃进步。所以，下面我要贴代码了….</h2><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinarySearchTree</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.root.__iter__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self,key,val)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.root:</span><br><span class="line">            self._put(key,val,self.root)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.root = TreeNode(key,val)</span><br><span class="line">        self.size+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_put</span><span class="params">(self,key,val,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> key &lt;currentNode.key:</span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild():</span><br><span class="line">                self._put(key,val,currentNode.leftChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.leftChild = TreeNode(key,val,parent=currentNode)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> currentNode.hasRightChild():</span><br><span class="line">                self._put(key,val,currentNode.rightChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.rightChild = TreeNode(key,val, parent = currentNode)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self,k,v)</span>:</span></span><br><span class="line">        self.put(k,v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.root:</span><br><span class="line">            res = self._get(key,self.root)</span><br><span class="line">            <span class="keyword">if</span> res:</span><br><span class="line">                <span class="keyword">return</span> res.payload</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span>  <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get</span><span class="params">(self,key,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> currentNode:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> currentNode.key == key:</span><br><span class="line">            <span class="keyword">return</span> currentNode</span><br><span class="line">        <span class="keyword">elif</span> currentNode.key &lt; key:</span><br><span class="line">            <span class="keyword">return</span> self._get(key,currentNode.rightChild)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._get(key,currentNode.leftChild)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.get(key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self._get(key,self.root):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span>   <span class="comment">#if 'fff' in mytree:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.size &gt;<span class="number">1</span>:</span><br><span class="line">            nodeToRemove = self._get(key,self.root)</span><br><span class="line">            <span class="keyword">if</span> nodeToRemove:</span><br><span class="line">                self.remove(nodeToRemove)</span><br><span class="line">                self.size-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> KeyError(<span class="string">'Error,key not in tree'</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.size ==<span class="number">1</span> <span class="keyword">and</span> self.root.key == key:</span><br><span class="line">            self.root = <span class="literal">None</span></span><br><span class="line">            self.size =self.size<span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'Error,key not in tree'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__delitem__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        self.delete(key)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> currentNode.isLeaf():</span><br><span class="line">            <span class="keyword">if</span> currentNode == currentNode.parent.leftChild:</span><br><span class="line">                currentNode.parent.leftChild = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.parent.rightChild = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> currentNode.hasBothChildren():</span><br><span class="line">            succ = currentNode.findSuccessor()</span><br><span class="line">            succ.spliceOut()</span><br><span class="line">            currentNode.key = succ.key</span><br><span class="line">            currentNode.payload = succ.payload</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#this node has one child</span></span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild():</span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild():</span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.leftChild = currentNode.leftChild</span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild():</span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.rightChild = currentNode.leftChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    currentNode.replaceNodeData(currentNode.leftChild.key,currentNode.leftChild.payload,</span><br><span class="line">                                               currentNode.leftChild.leftChild,</span><br><span class="line">                                               currentNode.leftChild.rightChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild():</span><br><span class="line">                    currentNode.rightChild.parent =currentNode.parent</span><br><span class="line">                    currentNode.parent.leftChild=currentNode.rightChild</span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild():</span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.rightChild =currentNode.rightChild</span><br><span class="line"></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    currentNode.replaceNodeData(currentNode.rightChild.key,currentNode.rightChild.payload,</span><br><span class="line">                                               currentNode.rightChild.leftChild,</span><br><span class="line">                                               currentNode.rightChild.rightChild)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, key, val, left = None, right = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                        parent = None)</span>:</span></span><br><span class="line">        self.key = key</span><br><span class="line">        self.payload = val</span><br><span class="line">        self.leftChild = left</span><br><span class="line">        self.rightChild = right</span><br><span class="line">        self.parent = parent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasLeftChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.leftChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasRightChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isLeftChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.leftChild == self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isRightChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.rightChild == self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isRoot</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.parent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isLeaf</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> (self.rightChild <span class="keyword">or</span> self.leftChild)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasAnyChildren</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">or</span> self.leftChild</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasBothChildren</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">and</span> self.leftChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceNodeData</span><span class="params">(self,key,value,lc,rc)</span>:</span></span><br><span class="line">        self.key =key</span><br><span class="line">        self.payload = value</span><br><span class="line">        self.leftChild = lc</span><br><span class="line">        self.rightChild = rc</span><br><span class="line">        <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">            self.leftChild.parent =self</span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">            self.rightChild.parent = self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSuccessor</span><span class="params">(self)</span>:</span></span><br><span class="line">        succ = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">            succ =self.rightChild.findMin()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.parent:</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    succ = self.parent</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = <span class="literal">None</span></span><br><span class="line">                    succ =self.parent.findSuccessor()</span><br><span class="line">                    self.parent.rightChild = self</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> succ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMin</span><span class="params">(self)</span>:</span></span><br><span class="line">        current = self</span><br><span class="line">        <span class="keyword">while</span> current.hasLeftChild():</span><br><span class="line">            current =current.leftChild</span><br><span class="line">        <span class="keyword">return</span> current</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spliceOut</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.isLeaf():</span><br><span class="line">            <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                self.parent.leftChild = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent.rightChild = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> self.hasAnyChildren():</span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    self.parent.leftChild = self.leftChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = self.leftChild</span><br><span class="line">                self.leftChild.parent =self.parent</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    self.parent.leftChild = self.rightChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = self.rightChild</span><br><span class="line">                self.rightChild.parent =self.parent</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self:</span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.leftChild:</span><br><span class="line">                    <span class="keyword">yield</span> elem</span><br><span class="line">            <span class="keyword">yield</span> self.key</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.rightChild:</span><br><span class="line">                    <span class="keyword">yield</span> elem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">mytree = BinarySearchTree()</span><br><span class="line">mytree[<span class="number">3</span>]=<span class="string">"red"</span></span><br><span class="line">mytree[<span class="number">4</span>]=<span class="string">"blue"</span></span><br><span class="line">mytree[<span class="number">6</span>]=<span class="string">"yellow"</span></span><br><span class="line">mytree[<span class="number">2</span>]=<span class="string">"at"</span></span><br><span class="line"></span><br><span class="line">print(mytree[<span class="number">6</span>])</span><br><span class="line">print(mytree[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World ~</title>
    <url>/2019/11/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to  my own web,you can find me on github <a href="https://github.com/iiot-tbb" target="_blank" rel="noopener">iiot-tbb</a>! This is my very first post.</p>
<a id="more"></a>
<p><img src="/images/bac.jpg" alt=""></p>
]]></content>
      <tags>
        <tag>myBorn</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>论法的精神（节选）</title>
    <url>/2019/11/11/%E8%AE%BA%E6%B3%95%E7%9A%84%E7%B2%BE%E7%A5%9E%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/</url>
    <content><![CDATA[<h3 id="法与一切存在物的关系"><a href="#法与一切存在物的关系" class="headerlink" title="法与一切存在物的关系"></a>法与一切存在物的关系</h3><p>从最大限度的广义上说，法是源于客观事物性质的必然关系。从这个意义上推断，所有的</p>
<p>存在物都有属于自己的法；上帝有他的法；物质世界也有它的法；高于人类的“先知</p>
<p>圣人们”有着他们的法；畜类也有自己的法；人类拥有他们的法。</p>
<p>有些人说，世间我们看到的万物都是由一个盲目的命运所创造的，这种说法荒谬绝伦。因</p>
<p>为盲目的命运竟然创造“具有智能的创造物”，岂不是一件更为荒谬的事吗?</p>
<p>于是便有了一个最浅显的理性的存在。法就是这个浅显理性与各种存在物之间关系的总</p>
<p>和，同时也体现着所有客观存在物彼此之间的关系。</p>
<p>上帝与宇宙的关系体现在，它既是宇宙的创造者又是它的保管者：以此产生的规律，便是</p>
<p>保管时参照的规律。上帝遵循这些规律行事，因为他熟知这些规范；之所以他熟知这些规</p>
<p>范，因为正是他制定了这些规范；他之所以制定这些规范，因为这些规律与他的才智和能<br>量有着密不可分的关系。</p>
<p>如同我们看到的一样，我们所在的世界是由物质的运动而构成的，它在一个非智能的状态</p>
<p>中永恒地生存着。它的物质运动必然具有某种固定的规律。如果人们能够在自己所处的世</p>
<p>界之外再臆想出另一个世界的话，那个世界要么具有恒定的规律可循，要么便是毁灭。</p>
<p>创造本身似乎是某种随意的行为，然而其中必定蕴涵着恒定的规律，就如同无神论者的命</p>
<p>运一般。如果造世主没有这些规范就能统管世界的话，那么，这显然是荒谬的说法，因为</p>
<p>没有这些规范，世界将无法生存。<br><a id="more"></a></p>
<p>这些规律建立在恒定不变的关系之中。在两个运动的物体之间，遵循其重量和速度间的关</p>
<p>系，从而承受所有运动形式给予它们的作用力，增加、减力以及消失；每一次差别，都有</p>
<p>其均衡性；而每一次变化，都有其永恒性。</p>
<p>特殊的“智能存在物”能够拥有自己制定的法律，然而也有一些并非是他们创造的，却被</p>
<p>他们所拥有。在“智能存在物”产生之前，它们已有了存在的可能性；于是它们便有了存</p>
<p>在的关系，所以也就有了可能存在的法律。在法律制定之前，已有了产生公共关系的可能</p>
<p>性。如果在人为法限制或禁止的行为之外，就无公道可言的话，那么便意味着，当人们还</p>
<p>未画出圆圈时，所有的半径也并不相等。</p>
<p>因此，我们应该承认，在人为法确定之前，已存在着公正。例如：即使在纷繁的人类社会</p>
<p>中，遵守法律乃是天经地义；如果某些“智能存在物”从另一个“智能存在物”身上获取某</p>
<p>种利益的话，前者就应该怀有感激之情；如果某一个“智能存在物”创造了另一个“智能存</p>
<p>在物”，那么，被创造者则应该保持自己原有的依存关系；当一个“智能存在物”损害另一</p>
<p>个同类时，它自身也应该受到相应的损害；还能列出许多公正关系的例子。</p>
<p>然而却不能说智能世界和物质世界被管理得一样完备。因为，尽管智能世界有其自身的规</p>
<p>律，这些规律就性质而言是不可变化的，但是智能世界却不能像物质世界那样永恒地遵守</p>
<p>其规律，原因是某些特殊的“智能存在物”囿于其本性而导致错误；而且，从另一个方面</p>
<p>来说，他们按自己的本性我行我素。所以，他们并不是永恒地遵守自己最初级的规律；即</p>
<p>使那些规律是他们自己制定的，他们却总是不去遵守它们。</p>
<p>人们不知道，兽类是否受到一般规律的支配，还是受特殊动力的支配。无论如何，兽类与</p>
<p>上帝的关系绝不比其他的物质世界的关系更亲密；兽类的感官只限于它们彼此的关系，与</p>
<p>其他特殊存在物，或者是与它们自身的关系之中。</p>
<p>出于欲念的诱惑，兽类保留了自己的特殊的生灵特征；而且，由于同样的原因，它们还保</p>
<p>存了自身的种类。它们具有自然法，因为它们是由感官组合而成的；它们并没有自己制定</p>
<p>的法律，因为它们并不是由知识组合而成的。然而，兽类并非始终不渝地遵守它们的自然</p>
<p>法则。倒是那些看上去并非掌握知识和具备感官的植物，却较为忠实地遵守着自然法则。</p>
<p>兽类不具备我们人类所具有的最高级的智能，然而我们却不具备它们拥有的某些能力。他</p>
<p>们丝毫没有我们所拥有的种种欲望，当然，它们也没有我们所特有的种种担忧和恐惧；它</p>
<p>们和我们一样将经受死亡，但是它们却不了解死亡；它们中的大多数甚至比我们更完好地</p>
<p>保存自身，却并不像我们那样滥用情欲。</p>
<p>人，从 <strong> “物质存在” </strong> 的意义上而言，与其他物体别无二致，人受到永恒规律的支配。当人</p>
<p>作为“智能存在物”的时候，便时常诋毁上帝创造的规律，并且更改自己制定的法律。他</p>
<p>本应主宰自我；然而他毕竟是有其局限性的存在物；他与所有“有限的智能生物”一样，</p>
<p>无知与错误在所难免；他不但知之甚少，就连仅有的粗浅知识也会丧失殆尽。作为有感知</p>
<p>的生物，他受到无数情欲的支配。这样一种存在物随时都可能忘却他们的创造者；上帝便<br>以宗教的法规让他们回忆起这一切。这样的存在物随时也会忘却他自己；哲学家以道德规范警示他。为了在社会中生存，他必定要有所作为，但是，他可能将别人置之外；</p>
<p>立法者们以政治和民事法律促使他们尽其义务。</p>
]]></content>
      <tags>
        <tag>人文</tag>
        <tag>法律</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
