<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>pytorch版本的bilstm+crf的实现</title>
    <url>/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="CRF-代码实现"><a href="#CRF-代码实现" class="headerlink" title="CRF 代码实现"></a>CRF 代码实现</h2><p>最近没啥事情，想把CRF相关的内容再捋一捋，因此研究了Pytoch的CRF实现，代码如下展示，之后将会研究如何做batch版本的CRF。</p>
<p>关于CRF的实现，表面上和HMM模型基本一致，从我的角度来看，因为其观测矩阵的实现由模型给出，即 $P(Y|X)$，因此其展示的是判别模型。所以才认为其是CRF模型，<br>如果变成隐马尔可夫模型，需要建模$P(Y,X)$, 十分有趣，不同观测矩阵的给出方式决定了模型的类型。</p>
<a id="more"></a>
<p>关于代码实现部分，其关键点在于 前向概率的计算和解码的维特比算法部分。关于下文中借助了 Bi-LSTM作为CRF模型的观测矩阵的来源必不可少，这部分模型也可以由其它模型替换，比如 Transformer类型的模型。</p>
<p>对于该部分代码的讲解，我在知乎上看到一个比较好的回答，如果大家对这部分代码不是很熟悉，建议先去《统计学习方法》这本书上看一下具体的理论介绍，然后可以参考<a href="https://zhuanlan.zhihu.com/p/140479197" target="_blank" rel="noopener">PyTorch Bi-LSTM+CRF NER标注代码精读</a>这篇文章，通过理论和代码实现上的学习，我相信大家可以对该部分内容有一个较好的理解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">START_TAG = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">STOP_TAG = <span class="string">"&lt;STOP&gt;"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(vec)</span>:</span></span><br><span class="line">    _ , idx = torch.max(vec,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx.item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span><span class="params">(seq,to_ix)</span>:</span></span><br><span class="line">    idxs = [ to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq ]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs,dtype= torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减去最大值，防止在exp的时候溢出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(vec)</span>:</span></span><br><span class="line">    max_score = vec[<span class="number">0</span>,argmax(vec)]</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>,<span class="number">-1</span>).expand(<span class="number">1</span>,vec.size()[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,vocab_size,tag_to_idx,embedding_dim,hidden_dim)</span>:</span></span><br><span class="line">        super(BiLSTM_CRF,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding_dim = embedding_dim </span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.tag_to_ix = tag_to_idx</span><br><span class="line">        self.tagset_size = len(tag_to_idx)</span><br><span class="line"></span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim//<span class="number">2</span>,num_layers=<span class="number">1</span>,bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#隐藏层与tag层的映射维度变换</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim,self.tagset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#状态转移矩阵 元素i,j的值代表从j--&gt;i 的状态转移概率</span></span><br><span class="line">        self.trasitions = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size,self.tagset_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态转移加入条件，任何状态无法转移到起始状态</span></span><br><span class="line">        <span class="comment"># 且 任何状态无法从停止状态转移而来</span></span><br><span class="line">        self.trasitions.data[tag_to_idx[START_TAG],:] = <span class="number">-10000</span></span><br><span class="line">        self.trasitions.data[:,tag_to_idx[STOP_TAG]] = <span class="number">-10000</span></span><br><span class="line"></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对lstm网络层初始的ceil,hidden状态进行预设置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (torch.randn(<span class="number">2</span>,<span class="number">1</span>,self.hidden_dim //<span class="number">2</span>),</span><br><span class="line">                torch.randn(<span class="number">2</span>,<span class="number">1</span>,self.hidden_dim//<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span><span class="params">(self,feats)</span>:</span></span><br><span class="line"></span><br><span class="line">        init_alphas = torch.full((<span class="number">1</span>,self.tagset_size),<span class="number">-10000.</span>)</span><br><span class="line"></span><br><span class="line">        init_alphas[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历句子中的每一个词</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line"></span><br><span class="line">            alphas_t = [] <span class="comment"># 获取当前词对应的tag概率分布</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size): <span class="comment">#遍历所有的tag</span></span><br><span class="line">                <span class="comment"># 广播发射矩阵(观测矩阵)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 遍历得到当前的标签</span></span><br><span class="line"></span><br><span class="line">                emit_score = feat[next_tag].view(<span class="number">1</span>,<span class="number">-1</span>).expand(<span class="number">1</span>,self.tagset_size)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 转移概率的值为 第 i 个 条目从 i 过渡到 next_tag 的分数</span></span><br><span class="line">                <span class="comment"># </span></span><br><span class="line">                   </span><br><span class="line">                trans_score = self.trasitions[next_tag].view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">                next_tag_var = forward_var + trans_score + emit_score</span><br><span class="line"></span><br><span class="line">                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line">            </span><br><span class="line">            forward_var = torch.cat(alphas_t).view(<span class="number">1</span>,<span class="number">-1</span>) <span class="comment"># 将上一阶段的tag概率分布作为下一个阶段的初始状态概率分布</span></span><br><span class="line"></span><br><span class="line">        terminal_var = forward_var + self.trasitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line"></span><br><span class="line">        alpha = log_sum_exp(terminal_var)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lstm_features</span><span class="params">(self,sentence)</span>:</span></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">        embeds = self.word_embeds(sentence).view(len(sentence),<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        lstm_out, self.hidden = self.lstm(embeds, self.hidden)</span><br><span class="line"></span><br><span class="line">        lstm_out = lstm_out.view(len(sentence),self.hidden_dim)</span><br><span class="line"></span><br><span class="line">        lstm_feats = self.hidden2tag(lstm_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> lstm_feats</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span><span class="params">(self,feats,tags)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算  output feature 和tags之间的误差</span></span><br><span class="line"></span><br><span class="line">        score = torch.zeros(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]],dtype=torch.long),tags])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># score 的计算方式为 概率转移矩阵的对应tag之间的转移概率加上，观测矩阵对应位置x-y的概率值</span></span><br><span class="line">        <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(feats):</span><br><span class="line">            score = score + \</span><br><span class="line">                self.trasitions[tags[i+<span class="number">1</span>],tags[i]] +feat[tags[i+<span class="number">1</span>]]</span><br><span class="line">        score = score + self.trasitions[self.tag_to_ix[STOP_TAG],tags[<span class="number">-1</span>]]</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self,feats)</span>:</span></span><br><span class="line">        backpointers = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始 状态 变量</span></span><br><span class="line">        init_vvars = torch.full((<span class="number">1</span>,self.tagset_size), <span class="number">-10000.</span>)</span><br><span class="line">        init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        forward_var = init_vvars</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span>  feats:</span><br><span class="line">            bptrs_t = [] <span class="comment"># 保存当前节点的上一时间的节点转移概率最大值的节点</span></span><br><span class="line"></span><br><span class="line">            viterbivars_t = [] <span class="comment"># 保存当前的时间节点的概率分布</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> range(self.tagset_size):</span><br><span class="line">                <span class="comment"># next_tag_var 保存了 当前时刻维特比变量转移到</span></span><br><span class="line">                <span class="comment"># 下一个时间节点tag的概率分布</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 这里没有考虑到观测概率矩阵的概率原因在于 最大值不依赖于观测矩阵</span></span><br><span class="line">                next_tag_var = forward_var + self.trasitions[next_tag]</span><br><span class="line"></span><br><span class="line">                best_tag_id = argmax(next_tag_var)</span><br><span class="line">                bptrs_t.append(best_tag_id)</span><br><span class="line">                viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            forward_var = (torch.cat(viterbivars_t) + feat ).view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">            backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转移至 STOP_TAG</span></span><br><span class="line">        ternminal_var = forward_var + self.trasitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        best_tag_id = argmax(ternminal_var)</span><br><span class="line">        path_score = ternminal_var[<span class="number">0</span>][best_tag_id]</span><br><span class="line"></span><br><span class="line">        best_path = [best_tag_id]</span><br><span class="line">        <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> reversed(backpointers):</span><br><span class="line">            best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">            best_path.append(best_tag_id)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将 START tag 从得到的状态序列中移除</span></span><br><span class="line">        start= best_path.pop()</span><br><span class="line">        <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG] <span class="comment">#检查正确性</span></span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> path_score,best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_liklihood</span><span class="params">(self,sentence,tags)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得观测概率矩阵</span></span><br><span class="line">        feats =  self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        forward_score = self._forward_alg(feats)</span><br><span class="line"></span><br><span class="line">        gold_score = self._score_sentence(feats,tags)</span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,sentence)</span>:</span></span><br><span class="line">        lstm_feats = self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        score,tag_seq = self._viterbi_decode(lstm_feats)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score ,tag_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">training_data = [(</span><br><span class="line">    <span class="string">"the wall street journal reported today that apple corporation made money"</span>.split(),</span><br><span class="line">    <span class="string">"B I I I O O O B I O O"</span>.split()</span><br><span class="line">), (</span><br><span class="line">    <span class="string">"georgia tech is a university in georgia"</span>.split(),</span><br><span class="line">    <span class="string">"B I O O O O B"</span>.split()</span><br><span class="line">)]</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">"B"</span>:<span class="number">0</span>,<span class="string">"I"</span>:<span class="number">1</span>,<span class="string">"O"</span>:<span class="number">2</span>,START_TAG:<span class="number">3</span>,STOP_TAG:<span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">model = BiLSTM_CRF(len(word_to_ix),tag_to_ix,EMBEDDING_DIM,HIDDEN_DIM)</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr = <span class="number">0.01</span>, weight_decay= <span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>],word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]],dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> sentence,tags <span class="keyword">in</span> training_data:</span><br><span class="line"></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        sentence = prepare_sequence(sentence,word_to_ix)</span><br><span class="line"></span><br><span class="line">        targets = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> tags], dtype = torch.long)</span><br><span class="line"></span><br><span class="line">        loss = model.neg_log_liklihood(sentence,targets)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step() </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>],word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]],dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    print(model(precheck_sent))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>pytorch</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>kmeans numpy实现</title>
    <url>/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="k-means算法"><a href="#k-means算法" class="headerlink" title="k-means算法"></a>k-means算法</h2><p>k-means作为基本的无监督机器学习算法，在一些面试场景下经常会被拉起来让做手动实现，算法本身其实不难，但在面试场景下复现的准确且简洁则十分重要，因此，本篇博文实现了k-means基本算法，希望大家都可以动手自己实现一遍以保证在需要手写的时候能够快速完成。关于k-means算法的基本原理，我相信大家都应该十分的清楚，因此在这个不多展开介绍，不懂的同学请自行百度或者Google。</p>
<a id="more"></a>
<p>具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#距离计算以欧式距离为例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eulDistance</span><span class="params">(vector1,vevtor2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.power(vector1-vevtor2,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机初始化中心点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(dataset,k)</span>:</span></span><br><span class="line">    numSamples, dim = dataset.shape</span><br><span class="line">    centerid = np.zeros((k,dim))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        index = int(random.uniform(<span class="number">0</span>,numSamples))</span><br><span class="line">        centerid[i,:] =dataset[index,:]</span><br><span class="line">    <span class="keyword">return</span> centerid</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(dataset,k)</span>:</span></span><br><span class="line">    numsamples = dataset.shape[<span class="number">0</span>]</span><br><span class="line">   </span><br><span class="line">    cluster = np.mat(np.zeros((numsamples,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    cluseter_flag = <span class="literal">True</span> <span class="comment">#判断是否簇发生了改变</span></span><br><span class="line"></span><br><span class="line">    centroids = initCentroids(dataset,k)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> cluseter_flag:</span><br><span class="line">        cluseter_flag =<span class="literal">False</span></span><br><span class="line">        <span class="comment">#更新每一个样本所属的簇</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numsamples):</span><br><span class="line">            minDis = float(<span class="string">'inf'</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> cls <span class="keyword">in</span> range(k):</span><br><span class="line">                dis_tmp = eulDistance(dataset[i,:],centroids[cls,:])</span><br><span class="line">                <span class="keyword">if</span> dis_tmp &lt; minDis:</span><br><span class="line">                    minDis  = dis_tmp</span><br><span class="line">                    minIndex = cls</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cluster[i,<span class="number">0</span>]!=minIndex:</span><br><span class="line">                cluseter_flag =<span class="literal">True</span></span><br><span class="line">                cluster[i] = minIndex,minDis**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新簇的中心位置</span></span><br><span class="line">        <span class="keyword">for</span> cls <span class="keyword">in</span> range(k):</span><br><span class="line"></span><br><span class="line">            pointers = dataset[np.nonzero(cluster[:,<span class="number">0</span>].A == cls)[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">            centroids[cls,:] = np.mean(pointers,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids, cluster <span class="comment">#返回簇中心点和对应每一个样本点的簇分配情况</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotCluster</span><span class="params">(dataSet,k,centroids,cluster)</span>:</span></span><br><span class="line">    numsamples ,dim = dataSet.shape</span><br><span class="line"></span><br><span class="line">    color = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numsamples):</span><br><span class="line">        indx = int(cluster[i,<span class="number">0</span>])</span><br><span class="line">        plt.plot(dataSet[i,<span class="number">0</span>],dataSet[i,<span class="number">1</span>],color[indx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        plt.plot(centroids[i,<span class="number">0</span>],centroids[i,<span class="number">1</span>],color[i],markersize =<span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">    plt.savefig(<span class="string">"./output.png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    dataset = []</span><br><span class="line">    f = open(<span class="string">'./ceshi.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        dataset.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line">    dataset = np.mat(dataset)                </span><br><span class="line">    k = <span class="number">4</span></span><br><span class="line">    cenid,cluster = kmeans(dataset,k)</span><br><span class="line"></span><br><span class="line">    plotCluster(dataset,k,cenid,cluster)</span><br></pre></td></tr></table></figure>
<p>输出如下图所示<br><img src="output.png" alt="avatar"></p>
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Autograd 机制介绍</title>
    <url>/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="什么是-Autograd"><a href="#什么是-Autograd" class="headerlink" title="什么是 Autograd?"></a>什么是 Autograd?</h2><p>Autograd是反向自动微分系统。从概念上讲，autograd 记录一个图结构，记录执行操作时创建数据的所有操作，一个有向无环图，其叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，可以使用链式法则自动计算梯度。</p>
<a id="more"></a>
<p>更为具体的来说，Autograd方法是构造一个图结构的函数传播形式。它在计算网络的前向传播过程中构造了一个梯度的函数计算图。当完成前向过程后，进行反向传播时，计算每一个节点的梯度。在每一轮次都是即时构建这个图结构，并不用提前对图结构进行编码。按照python的函数流程，所构即所得。</p>
<h2 id="保存Tensor张量"><a href="#保存Tensor张量" class="headerlink" title="保存Tensor张量"></a>保存Tensor张量</h2><p>一些操作需要在前向传递期间保存中间结果，以便执行反向传递。例如，函数 $x\mapsto x^{2}$ 保存输入 $x$ 以计算梯度。</p>
<p>定义自定义 <code>Python</code> 函数时，您可以使用 <code>save_for_backward()</code> 在前向传递期间保存张量，并在后向传递期间使用 <code>saved_tensors</code> 检索它们。</p>
<p>对于<code>PyTorch</code>定义的操作（例如 <code>torch.pow()</code>），张量会根据需要自动保存。您可以通过查找以前缀 <code>_saved</code> 开头的属性来探索某个<code>grad_fn</code> 保存了哪些张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>)</span><br><span class="line">print(x.equal(y.grad_fn._saved_self))  <span class="comment"># True</span></span><br><span class="line">print(x <span class="keyword">is</span> y.grad_fn._saved_self)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>在前面的代码中，<code>y.grad_fn._saved_self</code> 指的是与 <code>x</code> 相同的 Tensor 对象。但情况可能并非总是如此。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.exp()</span><br><span class="line">print(y.equal(y.grad_fn._saved_result))  <span class="comment"># True</span></span><br><span class="line">print(y <span class="keyword">is</span> y.grad_fn._saved_result)  <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>在底层，为了防止引用循环，PyTorch 在保存时打包了张量，并将其解包到不同的张量中以供读取。在这里，您通过访问 y.grad_fn._saved_result 获得的张量是与 y 不同的张量对象（但它们仍然共享相同的存储）。一个张量是否会被打包到不同的张量对象中，取决于它是否是它自己的 grad_fn 的输出，这是一个可能会发生变化的实现细节，因此在操作时不应该依赖它。</p>
<h2 id="局部禁用梯度计算"><a href="#局部禁用梯度计算" class="headerlink" title="局部禁用梯度计算"></a>局部禁用梯度计算</h2><p>Python 有几种机制可以局部的禁用梯度计算：</p>
<p>要禁用整个代码块的梯度，有上下文管理器，如 no-grad 模式和 inference 模式。为了从梯度计算中更细粒度地排除子图，可以设置张量的 <code>requires_grad</code> 字段。<br>下面，除了讨论上述机制之外，我们还描述了 evaluate 模式<code>nn.Module.eval()</code>，这种方法实际上并不用于禁用梯度计算，但由于其名称，经常与这三种方法混淆。</p>
<h3 id="设置-required-grad"><a href="#设置-required-grad" class="headerlink" title="设置 required_grad"></a>设置 <code>required_grad</code></h3><p><code>requires_grad</code> 是一个标志，除非包含在 <code>nn.Parameter</code> 中，否则默认为 false，它允许从梯度计算中细粒度地排除子图。它在向前和向后传递中都生效：</p>
<p>在前向传递期间，只有至少一个输入张量需要 grad 时，才会将操作记录在后向图中。在后向传递 (.backward()) 期间，只有 requires_grad=True 的叶张量才会将梯度累积到它们的 <code>.grad</code> 字段中。</p>
<p>重要的是要注意，即使每个张量都有这个标志，设置它只对叶张量有意义（没有 grad_fn 的张量，例如，nn.Module 的参数）。非叶张量（具有 grad_fn 的张量）是具有与之关联的后向图的张量。因此，需要它们的梯度作为中间结果来计算需要 grad 的叶张量的梯度。从这个定义中，很明显所有非叶张量都会自动具有<code>require_grad=True</code>。</p>
<p>设置<code>require_grad</code>字段是有效的控制在你所用的模型中哪一部分的梯度需要被计算。比如说，当你使用预训练模型Bert类进行微调时，可以冻结这部分参数不跟随训练。当你设置<code>require_grad=False</code>时，你的这部分参数将不会在前向传播的过程中不会被保存记录，他们不会在后向传递中更新他们的 <code>.grad</code>字段，因为它们一开始就不会成为后向图的一部分。因为这是一种常见的模式，所以也可以使用 <code>nn.Module.requires_grad_()</code> 在模块级别设置<code>requires_grad</code>。当应用于模块时， .<code>requires_grad_()</code> 会影响模块的所有参数（默认情况下 <code>requires_grad=True</code> ）。</p>
<h3 id="梯度模式"><a href="#梯度模式" class="headerlink" title="梯度模式"></a>梯度模式</h3><p>除了设置 <code>requires_grad</code>之外，还可以从 Python 启用三种可能的模式，这些模式可以影响 PyTorch 中 autograd 计算的处理的方式：默认模式（grad 模式）、no-grad 模式和inference模式，所有这些都可以通过context manager和decorator。</p>
<h4 id="默认模式"><a href="#默认模式" class="headerlink" title="默认模式"></a>默认模式</h4><p>“默认模式”实际上是我们在没有启用其他模式（如 no-grad 和 inference 模式）时隐含的模式。与“no-grad mode”相比，默认模式有时也称为“grad mode”。<br>关于默认模式，最重要的一点是它是 <code>requires_grad</code> 生效的唯一模式。在其他两种模式下，<code>requires_grad</code> 总是被覆盖为 False。</p>
<h4 id="no-grad模式"><a href="#no-grad模式" class="headerlink" title="no-grad模式"></a>no-grad模式</h4><p>在no-grad模式下，所有的计算都表现为所有的输入都不需要有梯度，即使有 <code>require_grad=True</code> 的输入，no-grad 模式下的计算也永远不会记录在后向图中。<br>当您需要执行不应该由 autograd 记录的操作时启用 no-grad 模式，但您以后仍希望在 grad 模式下使用这些计算的输出。这个context-manager可以方便地禁用代码块或函数的梯度，而无需临时将张量设置为 requires_grad=False。</p>
<p>例如，在编写优化器时，no-grad 模式可能很有用：在执行训练更新时，您希望就地更新参数，而 autograd 不会记录更新。您还打算在下一次正向传递中使用更新的参数以 grad 模式进行计算。</p>
<p><code>torch.nn.init</code> 中的实现在初始化参数时也依赖于 no-grad 模式，以避免在就地更新初始化参数时进行 autograd 跟踪。</p>
<h4 id="Inference-模式"><a href="#Inference-模式" class="headerlink" title="Inference 模式"></a>Inference 模式</h4><p>推理模式是无梯度模式的极端版本​​。就像在 no-grad 模式下一样，推理模式下的计算不会记录在后向图中，但启用推理模式将允许 PyTorch 进一步加速您的模型。这种更好的运行时有一个缺点：在推理模式下创建的张量将无法用于退出推理模式后由 autograd 记录的计算。</p>
<p>当您执行不需要在后向图中记录的计算时启用推理模式，并且您不打算在稍后将由 autograd 记录的任何计算中使用在推理模式中创建的张量。</p>
<p>建议您在代码中不需要自动梯度跟踪的部分（例如，数据处理和模型评估）尝试推理模式。如果它为您的用例开箱即用，那将是免费的性能胜利。如果您在启用推理模式后遇到错误，请检查您是否没有在退出推理模式后由 autograd 记录的计算中使用在推理模式下创建的张量。如果在您的情况下无法避免此类使用，您可以随时切换回 no-grad 模式。</p>
<h3 id="评估-Evaluation-模式-nn-Module-eval"><a href="#评估-Evaluation-模式-nn-Module-eval" class="headerlink" title="评估(Evaluation)模式 (nn.Module.eval())"></a>评估(Evaluation)模式 (<code>nn.Module.eval()</code>)</h3><p>评估模式实际上并不是一种在本地禁用梯度计算的机制。在这里为什么要介绍它呢，因为它有时会被混淆为这样的机制。</p>
<p>在功能上，module.eval()（或等效的 module.train()）与 no-grad 模式和推理模式完全正交(即不存在相关性)。 model.eval() 如何影响您的模型完全取决于模型中使用的特定模块以及它们是否定义任何训练模式特定行为。</p>
<p>如果您的模型依赖于诸如 torch.nn.Dropout 和 torch.nn.BatchNorm2d 之类的模块，这些模块可能会因训练模式而有所不同，则您应当区分调用 model.eval() 和 model.train()。<br>例如，为了避免更新验证数据的 BatchNorm 运行统计信息。</p>
<p>建议您在训练时始终使用 model.train()，在评估模型（验证/测试）时始终使用 model.eval()，即使您不确定模型是否具有特定于训练模式的行为，因为您使用的某个模块可能会更新为在训练和评估模式下表现不同。</p>
<hr>
<h3 id="使用-autograd-进行原地操作"><a href="#使用-autograd-进行原地操作" class="headerlink" title="使用 autograd 进行原地操作"></a>使用 autograd 进行原地操作</h3><p>对于使用in-place的操作运算，pytorch这边不是非常建议使用，Autograd本身的缓冲区的释放和重用的效率是非常高的，极少情况下进行原地操作会显著降低内存的使用，</p>
<p>限制原地操作有两个主要原因：</p>
<ol>
<li>原地操作可能会覆盖计算梯度所需要的值。</li>
<li>原地操作需要重写计算图的操作实现，out-of-place的方法只需要分配新的内存给创造的对象，并且保存了对旧图的引用。在原地操作时，需要将所有输入的创建者更改为表示此操作的函数。这可能很棘手，特别是如果有许多张量引用相同的存储。比如，通过索引或转置创建。如果修改的输入的存储被任何其他张量引用，则in-place函数实际上会引发错误。</li>
</ol>
<h3 id="pytorch如何保证原地操作的正确性？"><a href="#pytorch如何保证原地操作的正确性？" class="headerlink" title="pytorch如何保证原地操作的正确性？"></a>pytorch如何保证原地操作的正确性？</h3><p> 每个张量都有一个版本计数器，每次它在任何操作中被标记为脏时都会递增。当一个函数保存任何用于后向计算的张量时，它们包含的张量的版本计数器也会被保存。当你访问<code>self.saved_tensor</code>时它就会被检查，如果它大于保存的值，则会引发错误。这可以确保如果您使用原地函数并且没有看到任何错误，则可以确保计算出的梯度是正确的。</p>
<h3 id="多线程-Autograd"><a href="#多线程-Autograd" class="headerlink" title="多线程 Autograd"></a>多线程 Autograd</h3><p>autograd 引擎负责运行计算反向传递所需的所有反向操作。下面将介绍在多线程环境中充分利用它的所有细节。<br>（仅与 PyTorch 1.6+ 相关，因为以前版本中的行为不同）</p>
<p>用户可以使用多线程代码（例如 Hogwild 训练）训练他们的模型，并且不会阻塞并发的反向计算，示例代码可以是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Define a train function to be used in different threads</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">()</span>:</span></span><br><span class="line">    x = torch.ones(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    y = (x + <span class="number">3</span>) * (x + <span class="number">4</span>) * <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    y.sum().backward()</span><br><span class="line">    <span class="comment"># potential optimizer update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># User write their own threading code to drive the train_fn</span></span><br><span class="line">threads = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    p = threading.Thread(target=train_fn, args=())</span><br><span class="line">    p.start()</span><br><span class="line">    threads.append(p)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> threads:</span><br><span class="line">    p.join()</span><br></pre></td></tr></table></figure>
<p>一些需要被了解的特性：</p>
<h3 id="CPU-上的并发"><a href="#CPU-上的并发" class="headerlink" title="CPU 上的并发"></a>CPU 上的并发</h3><hr>
<p>当您在 CPU 上的多个线程中通过 python 或 C++ API 运行<code>backward()</code> 或 grad() 时，将会看到额外的并发性，而不是在执行期间以特定顺序序列化所有向后调用（PyTorch 1.6 之前的行为）。</p>
<h4 id="Non-determinism"><a href="#Non-determinism" class="headerlink" title="Non-determinism"></a>Non-determinism</h4><p>如果您同时在多个线程上调用<code>backward()</code>，但使用共享输入（即Hogwild CPU 训练）。由于参数在线程之间自动共享，梯度累积可能在线程之间的反向调用中变得不确定，因为两个反向调用可能会访问并尝试累积相同的 <code>.grad</code>属性。这在技术上是不安全的，它可能会导致赛车状态，结果可能无法使用。</p>
<p>但是，如果您使用多线程方法来驱动整个训练过程但使用共享参数，那么这是预期的模式，使用多线程的用户应该牢记线程模型并且应该预期会发生这种情况。用户可以使用功能 API <code>torch.autograd.grad()</code> 来计算梯度，而不是使用 <code>backward()</code> 来避免不确定性。</p>
<h4 id="Graph-retaining"><a href="#Graph-retaining" class="headerlink" title="Graph retaining"></a>Graph retaining</h4><p>如果autograd图结构在多线程中被共享，例如图的第一部分运行在单线程上，图的第二部分采用多线程进行优化，在这种情况下，图结构的第一部分被优化，在这种情况下，不同的线程在同一个图上执行 <code>grad()</code>或 <code>backward()</code>可能会在一个线程的运行中破坏图，并且在这种情况下另一个线程将崩溃。 Autograd 将向用户发出错误，类似于在没有 <code>retain_graph=True</code> 的情况下两次调用 <code>backward()</code>，并让用户知道他们应该使用 <code>retain_graph=True</code>。</p>
<h4 id="在-Autograd节点上的线程安全。"><a href="#在-Autograd节点上的线程安全。" class="headerlink" title="在 Autograd节点上的线程安全。"></a>在 Autograd节点上的线程安全。</h4><p>由于 Autograd 允许调用者线程驱动其向后执行以实现潜在的并行性，因此我们必须确保 CPU 上的线程安全，并行反向传播共享 GraphTask 的部分/全部。</p>
<p>由于 GIL，自定义 Python <code>autograd.function</code> 是自动线程安全的。对于内置 C++ Autograd 节点（例如 AccumulateGrad、CopySlices）和自定义 autograd::Function，Autograd 引擎使用线程互斥锁来保护可能具有状态写入/读取的 autograd 节点上的线程安全。</p>
<h4 id="C-hooks上没有线程安全"><a href="#C-hooks上没有线程安全" class="headerlink" title="C++ hooks上没有线程安全"></a>C++ hooks上没有线程安全</h4><p>Autograd 依赖于用户编写线程安全的 C++ hooks。如果您希望hooks在多线程环境中正确应用，则需要编写适当的线程锁定代码以确保hooks是线程安全的。</p>
<hr>
<h2 id="对于复数的Autograd"><a href="#对于复数的Autograd" class="headerlink" title="对于复数的Autograd"></a>对于复数的Autograd</h2><p>简要概括:</p>
<ol>
<li>当你对具有复数域或者共域的函数$f(z)$进行微分时，梯度是在假设函数是更大的实值损失函数$g(input)=L$的一部分的情况下计算的，<br>$\frac{\partial L}{\partial z^{<em>}}$($z^{</em>}$代表 $z$ 的共轭)。其负值正是梯度下降算法中使用的最陡下降方向。因此说有的优化器对于复数参数都是开箱即用的。</li>
<li>此约定与 TensorFlow 的复杂微分约定相匹配，但与 JAX(计算$\frac{\partial L}{\partial z}$) 不同。</li>
<li>如果您有一个内部使用复数操作的实对实函数，则此处的约定无关紧要：您将始终获得与仅使用实数操作实现相同的结果。</li>
</ol>
]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>Autograd</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer语言模型训练</title>
    <url>/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code> module.<br>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><br>The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as<br><code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code>)<br>to draw global dependencies between input and output. The <code>nn.Transformer</code> module is now highly modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>in this tutorial) can be easily adapted/composed.</p>
<a id="more"></a>
<p><img src="transformer_architecture.jpg" alt="avatar"></p>
<h2 id="Define-the-model"><a href="#Define-the-model" class="headerlink" title="Define the model"></a>Define the model</h2><p>In this tutorial, we train <code>nn.TransformerEncoder</code> model on a<br>language modeling task. The language modeling task is to assign a<br>probability for the likelihood of a given word (or a sequence of words)<br>to follow a sequence of words. A sequence of tokens are passed to the embedding<br>layer first, followed by a positional encoding layer to account for the order<br>of the word (see the next paragraph for more details). The<br><code>nn.TransformerEncoder</code> consists of multiple layers of<br><code>nn.TransformerEncoderLayer &lt;https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer&gt;</code>. Along with the input sequence, a square<br>attention mask is required because the self-attention layers in<br><code>nn.TransformerEncoder</code> are only allowed to attend the earlier positions in<br>the sequence. For the language modeling task, any tokens on the future<br>positions should be masked. To have the actual words, the output<br>of <code>nn.TransformerEncoder</code> model is sent to the final Linear<br>layer, which is followed by a log-Softmax function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ntoken, ninp, nhead, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        <span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line">        self.model_type = <span class="string">'Transformer'</span></span><br><span class="line">        self.src_mask = <span class="literal">None</span></span><br><span class="line">        self.pos_encoder = PositionalEncoding(ninp, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)</span><br><span class="line">        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        self.ninp = ninp</span><br><span class="line">        self.decoder = nn.Linear(ninp, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_square_subsequent_mask</span><span class="params">(self, sz)</span>:</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.src_mask <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.src_mask.size(<span class="number">0</span>) != len(src):</span><br><span class="line">            device = src.device</span><br><span class="line">            mask = self._generate_square_subsequent_mask(len(src)).to(device)</span><br><span class="line">            self.src_mask = mask</span><br><span class="line"></span><br><span class="line">        src = self.encoder(src) * math.sqrt(self.ninp)</span><br><span class="line">        src = self.pos_encoder(src)</span><br><span class="line">        output = self.transformer_encoder(src, self.src_mask)</span><br><span class="line">        output = self.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sz=<span class="number">10</span></span><br><span class="line">mask=(torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">mask</span><br><span class="line">mask.float().masked_fill(mask == <span class="number">0</span>, float(<span class="string">'-inf'</span>)).masked_fill(mask == <span class="number">1</span>, float(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre><p><code>PositionalEncoding</code> module injects some information about the<br>relative or absolute position of the tokens in the sequence. The<br>positional encodings have the same dimension as the embeddings so that<br>the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of<br>different frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Load-and-batch-data"><a href="#Load-and-batch-data" class="headerlink" title="Load and batch data"></a>Load and batch data</h2><p>The training process uses Wikitext-2 dataset from <code>torchtext</code>. The<br>vocab object is built based on the train dataset and is used to numericalize<br>tokens into tensors. Starting from sequential data, the <code>batchify()</code><br>function arranges the dataset into columns, trimming off any tokens remaining<br>after the data has been divided into batches of size <code>batch_size</code>.<br>For instance, with the alphabet as the sequence (total length of 26)<br>and a batch size of 4, we would divide the alphabet into 4 sequences of<br>length 6:</p>
<script type="math/tex; mode=display">\begin{aligned}\begin{bmatrix}\text{A} & \text{B}& \text{C} & \ldots & \text{X} & \text{Y} & \text{Z}\end{bmatrix}\Rightarrow\begin{bmatrix}\begin{bmatrix}\text{A} \\ \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F} \end{bmatrix} &\begin{bmatrix}\text{G}\\\text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} &\begin{bmatrix}\text{M} \\\text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} &\begin{bmatrix}\text{S} \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix}\end{bmatrix}\end{aligned}</script><p>These columns are treated as independent by the model, which means that<br>the dependence of <code>G</code> and <code>F</code> can not be learned, but allows more<br>efficient batch processing.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">"basic_english"</span>),</span><br><span class="line">                            init_token=<span class="string">'&lt;sos&gt;'</span>,</span><br><span class="line">                            eos_token=<span class="string">'&lt;eos&gt;'</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># Divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure>
<pre><code>/home/bool_tbb/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.
  warnings.warn(&#39;{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;.format(self.__class__.__name__), UserWarning)
/home/bool_tbb/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.
  warnings.warn(&#39;Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;, UserWarning)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = TEXT.numericalize([train_txt.examples[<span class="number">0</span>].text])</span><br></pre></td></tr></table></figure>
<p>Functions to generate input and target sequence</p>
<p><code>get_batch()</code> function generates the input and target sequence for<br>the transformer model. It subdivides the source data into chunks of<br>length <code>bptt</code>. For the language modeling task, the model needs the<br>following words as <code>Target</code>. For example, with a <code>bptt</code> value of 2,<br>we’d get the following two Variables for <code>i</code> = 0:</p>
<p><img src="transformer_input_target.png" alt="avatar"></p>
<p>It should be noted that the chunks are along dimension 0, consistent<br>with the <code>S</code> dimension in the Transformer model. The batch dimension<br><code>N</code> is along dimension 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bptt = <span class="number">35</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(source, i)</span>:</span></span><br><span class="line">    seq_len = min(bptt, len(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_batch(train_data,<span class="number">0</span>)[<span class="number">0</span>][:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[    3,    25,  1849,   570,     7,     5,     5,  9258,     4,    56,
             0,     7,     6,  6634,     4,  6603,     6,     5,    65,    30],
        [   12,    66,    13,  4889,   458,     8,  1045,    21, 19094,    34,
           147,     4,     0,    10,  2280,  2294,    58,    35,  2438,  4064],
        [ 3852, 13667,  2962,    68,     6, 28374,    39,   417,     0,  2034,
            29,    88, 27804,   350,     7,    17,  4811,   902,    33,    20],
        [ 3872,     5,     9,     4,   155,     8,  1669,    32,  2634,   257,
             4,     5,     5,    11,  4568,  8205,    78,  5258,  7723, 12009],
        [  884,    91,   963,   294,     4,   548,    29,   279,    37,     4,
           391,    31,     4,  2614,   948, 13583,   405,   545,    15,    16],
        [   12,    25,     5,     5,  1688,     0,    39,    59,  8785,     0,
             6,    13,  3026,    43,    11,     6,     0,   349,  3134,  4538],
        [    3,     6,    82,  1780,    21,     6,  2158,     4,     8,     8,
            27,  1485,     0,   194,    96,   195,  3545,   101,  1150,  3486],
        [    3,    25,    13,   885,     4,  6360,    15,   670,     0,    13,
            26,    17,     5,   417,   894,    10,     5,     5,  2998,    27],
        [20003,   190,    33,  1516,  1085,    34,   680,  3597,  2475,   664,
            47,    11,   127,    63,     6,    46, 24995,    72, 10190,    26],
        [   86,  9076, 10540,     6,     9,    74,   198,     7,     6,    17,
          3134,  5312,     4,     4,     3, 25509,     5,  2034,     5,    86]])
</code></pre><h2 id="Initiate-an-instance"><a href="#Initiate-an-instance" class="headerlink" title="Initiate an instance"></a>Initiate an instance</h2><p>The model is set up with the hyperparameter below. The vocab size is<br>equal to the length of the vocab object.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ntokens = len(TEXT.vocab.stoi) <span class="comment"># the size of vocabulary</span></span><br><span class="line">emsize = <span class="number">200</span> <span class="comment"># embedding dimension</span></span><br><span class="line">nhid = <span class="number">200</span> <span class="comment"># the dimension of the feedforward network model in nn.TransformerEncoder</span></span><br><span class="line">nlayers = <span class="number">2</span> <span class="comment"># the number of nn.TransformerEncoderLayer in nn.TransformerEncoder</span></span><br><span class="line">nhead = <span class="number">2</span> <span class="comment"># the number of heads in the multiheadattention models</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># the dropout value</span></span><br><span class="line">model = nn.DataParallel(model)</span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="Run-the-model"><a href="#Run-the-model" class="headerlink" title="Run the model"></a>Run the model</h2><p><code>CrossEntropyLoss &lt;https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;</code><br>is applied to track the loss and<br><code>SGD &lt;https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD&gt;</code><br>implements stochastic gradient descent method as the optimizer. The initial<br>learning rate is set to 5.0. <code>StepLR &lt;https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR&gt;</code>is<br>applied to adjust the learn rate through epochs. During the<br>training, we use<br><code>nn.utils.clip_grad_norm\_ &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_&gt;</code><br>function to scale all the gradient together to prevent exploding.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span> <span class="comment"># learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train() <span class="comment"># Turn on the train mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> enumerate(range(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output.view(<span class="number">-1</span>, ntokens), targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">'| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | '</span></span><br><span class="line">                  <span class="string">'lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | '</span></span><br><span class="line">                  <span class="string">'loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(eval_model, data_source)</span>:</span></span><br><span class="line">    eval_model.eval() <span class="comment"># Turn on the evaluation mode</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    ntokens = len(TEXT.vocab.stoi)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            output_flat = output.view(<span class="number">-1</span>, ntokens)</span><br><span class="line">            total_loss += len(data) * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (len(data_source) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Loop over epochs. Save the model if the validation loss is the best<br>we’ve seen so far. Adjust the learning rate after each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_loss = float(<span class="string">"inf"</span>)</span><br><span class="line">epochs = <span class="number">100</span> <span class="comment"># The number of epochs</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train()</span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line">    print(<span class="string">'| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | '</span></span><br><span class="line">          <span class="string">'valid ppl &#123;:8.2f&#125;'</span>.format(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        best_model = model</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<pre><code>| epoch   1 |   200/ 2981 batches | lr 4.07 | ms/batch  9.71 | loss  5.39 | ppl   218.21
| epoch   1 |   400/ 2981 batches | lr 4.07 | ms/batch  9.50 | loss  5.39 | ppl   220.00
| epoch   1 |   600/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.20 | ppl   181.36
| epoch   1 |   800/ 2981 batches | lr 4.07 | ms/batch  9.60 | loss  5.26 | ppl   193.18
| epoch   1 |  1000/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.23 | ppl   186.05
| epoch   1 |  1200/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.26 | ppl   192.45
| epoch   1 |  1400/ 2981 batches | lr 4.07 | ms/batch  9.55 | loss  5.29 | ppl   197.86
| epoch   1 |  1600/ 2981 batches | lr 4.07 | ms/batch  9.60 | loss  5.33 | ppl   206.42
| epoch   1 |  1800/ 2981 batches | lr 4.07 | ms/batch  9.59 | loss  5.27 | ppl   193.88
| epoch   1 |  2000/ 2981 batches | lr 4.07 | ms/batch  9.70 | loss  5.30 | ppl   200.64
| epoch   1 |  2200/ 2981 batches | lr 4.07 | ms/batch  9.64 | loss  5.17 | ppl   176.64
| epoch   1 |  2400/ 2981 batches | lr 4.07 | ms/batch  9.62 | loss  5.26 | ppl   192.57
| epoch   1 |  2600/ 2981 batches | lr 4.07 | ms/batch  9.63 | loss  5.28 | ppl   195.69
| epoch   1 |  2800/ 2981 batches | lr 4.07 | ms/batch  9.64 | loss  5.21 | ppl   182.35
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 30.36s | valid loss  5.55 | valid ppl   256.32
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 3.87 | ms/batch  9.73 | loss  5.26 | ppl   192.78
| epoch   2 |   400/ 2981 batches | lr 3.87 | ms/batch  9.65 | loss  5.27 | ppl   194.78
| epoch   2 |   600/ 2981 batches | lr 3.87 | ms/batch  9.68 | loss  5.08 | ppl   160.59
| epoch   2 |   800/ 2981 batches | lr 3.87 | ms/batch  9.67 | loss  5.14 | ppl   171.46
| epoch   2 |  1000/ 2981 batches | lr 3.87 | ms/batch  9.68 | loss  5.10 | ppl   164.78
| epoch   2 |  1200/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.14 | ppl   171.25
| epoch   2 |  1400/ 2981 batches | lr 3.87 | ms/batch  9.71 | loss  5.18 | ppl   177.40
| epoch   2 |  1600/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.23 | ppl   186.59
| epoch   2 |  1800/ 2981 batches | lr 3.87 | ms/batch  9.70 | loss  5.16 | ppl   173.63
| epoch   2 |  2000/ 2981 batches | lr 3.87 | ms/batch  9.61 | loss  5.19 | ppl   179.61
| epoch   2 |  2200/ 2981 batches | lr 3.87 | ms/batch  9.61 | loss  5.06 | ppl   158.22
| epoch   2 |  2400/ 2981 batches | lr 3.87 | ms/batch  9.66 | loss  5.14 | ppl   170.97
| epoch   2 |  2600/ 2981 batches | lr 3.87 | ms/batch  9.63 | loss  5.16 | ppl   173.44
| epoch   2 |  2800/ 2981 batches | lr 3.87 | ms/batch  9.62 | loss  5.10 | ppl   163.57
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 30.54s | valid loss  5.44 | valid ppl   231.52
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 3.68 | ms/batch  9.74 | loss  5.15 | ppl   172.66
| epoch   3 |   400/ 2981 batches | lr 3.68 | ms/batch  9.81 | loss  5.16 | ppl   174.57
| epoch   3 |   600/ 2981 batches | lr 3.68 | ms/batch  9.76 | loss  4.98 | ppl   145.22
| epoch   3 |   800/ 2981 batches | lr 3.68 | ms/batch  9.69 | loss  5.04 | ppl   154.24
| epoch   3 |  1000/ 2981 batches | lr 3.68 | ms/batch  9.92 | loss  5.02 | ppl   150.68
| epoch   3 |  1200/ 2981 batches | lr 3.68 | ms/batch  9.75 | loss  5.05 | ppl   156.65
| epoch   3 |  1400/ 2981 batches | lr 3.68 | ms/batch  9.81 | loss  5.08 | ppl   161.32
| epoch   3 |  1600/ 2981 batches | lr 3.68 | ms/batch  9.87 | loss  5.13 | ppl   168.46
| epoch   3 |  1800/ 2981 batches | lr 3.68 | ms/batch  9.73 | loss  5.06 | ppl   158.11
| epoch   3 |  2000/ 2981 batches | lr 3.68 | ms/batch  9.78 | loss  5.09 | ppl   162.57
| epoch   3 |  2200/ 2981 batches | lr 3.68 | ms/batch  9.80 | loss  4.97 | ppl   143.40
| epoch   3 |  2400/ 2981 batches | lr 3.68 | ms/batch  9.84 | loss  5.05 | ppl   156.10
| epoch   3 |  2600/ 2981 batches | lr 3.68 | ms/batch  9.78 | loss  5.07 | ppl   158.92
| epoch   3 |  2800/ 2981 batches | lr 3.68 | ms/batch  9.80 | loss  5.01 | ppl   149.25
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 30.91s | valid loss  5.46 | valid ppl   234.33
-----------------------------------------------------------------------------------------
</code></pre><h2 id="Evaluate-the-model-with-the-test-dataset"><a href="#Evaluate-the-model-with-the-test-dataset" class="headerlink" title="Evaluate the model with the test dataset"></a>Evaluate the model with the test dataset</h2><p>Apply the best model to check the result with the test dataset.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br><span class="line">print(<span class="string">'| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;'</span>.format(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line">print(<span class="string">'='</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>
<pre><code>=========================================================================================
| End of training | test loss  5.48 | test ppl   238.72
=========================================================================================
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'./transformer_net.pth'</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_dict=model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(model_dict)</span><br></pre></td></tr></table></figure>
<pre><code>torch.nn.modules.module._IncompatibleKeys
</code></pre><p> <a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model" target="_blank" rel="noopener">REFERENCE:SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT</a>. </p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
        <tag>pytorch</tag>
        <tag>Transformer</tag>
        <tag>Languaage Model</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/07/29/Transformer/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h4 id="注意力计算公式"><a href="#注意力计算公式" class="headerlink" title="注意力计算公式"></a>注意力计算公式</h4><script type="math/tex; mode=display">A = Softmax(Q*K^T/\sqrt{d})*V</script><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[-1.6685, -1.7979,  0.0747]],

         [[ 1.1604,  1.1415,  0.4631]],

         [[ 1.6218, -1.3112, -0.6065]]],


        [[[ 0.2836, -0.8159, -0.4028]],

         [[-0.0721, -0.3244,  0.2214]],

         [[-0.9558,  0.5414, -0.4869]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 0.5522, -3.4192,  0.7006]],

         [[-1.5651, -1.0705,  1.7866]],

         [[-2.1893, -0.2521,  0.2480]]],


        [[[ 0.4621,  1.0492,  0.5085]],

         [[-0.3847, -1.9930,  1.6604]],

         [[-1.0364, -0.3537,  1.5496]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.matmul(x,y.transpose(<span class="number">-1</span>,<span class="number">-2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 5.2783]],

         [[-2.2107]],

         [[-3.3705]]],


        [[[-0.9298]],

         [[ 1.0419]],

         [[ 0.0446]]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3, 3, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=nn.Softmax()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop=nn.Dropout(<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop(torch.randn(<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 0.0000, -1.8719,  0.5233, -0.0000],
        [-0.0000,  0.6212,  0.2304, -0.1491],
        [-1.5584, -2.3030,  0.0000, -0.8582]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s(torch.FloatTensor([<span class="number">1</span>,-np.inf,<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-33-50788e72e9da&gt;:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  s(torch.FloatTensor([1,-np.inf,3]))





tensor([0.1192, 0.0000, 0.8808])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#计算注意力</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 attention_dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(ScaledDotProductAttention,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(attention_dropout)</span><br><span class="line">        self.softmax = nn.Softmax(dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,q,k,v,scale=None,attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        attention = torch.matmul(q,k.transpose(<span class="number">-2</span>,<span class="number">-1</span>)) <span class="comment"># 计算 Q*K^T ,交换最后两个维度的数据</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line">            attention = attention * scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mask attention. The attentions between the masked words and</span></span><br><span class="line">        <span class="comment"># other words are set to negative infinity</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention = attention.masked_fill_(attn_mask,-np.inf) </span><br><span class="line">        <span class="comment"># 这里掩码会把 Q*K^T里需要被掩盖的部分换成-inf 这样在softmax里该数值就变为零</span></span><br><span class="line">        <span class="comment"># 在Encoder里 需要掩盖住填充的0  在Decoder里除了掩盖住填充的0外 还要掩盖住后面的词</span></span><br><span class="line"></span><br><span class="line">        attention = self.softmax(attention)</span><br><span class="line">        attention = self.dropout(attention)</span><br><span class="line">        context = torch.matmul(attention,v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure>
<h4 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.Linear(<span class="number">10</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Linear(in_features=10, out_features=10, bias=True)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[ 0.4640,  0.5466, -0.6880,  0.1568],
         [ 0.8788,  0.9843, -0.4244, -1.5735],
         [ 0.1039,  1.2114,  0.7816, -0.8735]],

        [[ 1.1619, -2.5654,  0.5679, -1.1354],
         [-0.9004,  0.5074,  1.4977, -0.5807],
         [-1.3787,  0.7510, -1.1061,  1.2569]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.unsqueeze(<span class="number">2</span>).repeat(<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>).size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3, 10, 4])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># compute multi heads attention</span></span><br><span class="line">    <span class="comment"># 多头注意力的本质是由多个Wq,Wk,Wv计算出多组 Q,K,V从而得到多个向量 </span></span><br><span class="line">    <span class="comment"># 这里实现的方式是 由一个大的Wq,Wk,Wv 计算出一组大的Q,K,V 再把这个Q,K,V分成若干个</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_modl=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(MultiHeadAttention,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.dim_per_head = d_modl // num_heads <span class="comment">#计算每个头的维度</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.linear_k = nn.Linear(d_modl, d_modl)</span><br><span class="line">        self.linear_v = nn.Linear(d_modl, d_modl)</span><br><span class="line">        self.linear_q = nn.Linear(d_modl, d_modl)</span><br><span class="line"></span><br><span class="line">        self.dot_product_attention = ScaledDotProductAttention(dropout)</span><br><span class="line">        self.linear_final = nn.Linear(d_modl,d_modl)</span><br><span class="line">        self.norm = nn.LayerNorm(d_modl)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, keys, values, queries, attn_mask=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        residual = queries</span><br><span class="line">        batch_size = keys.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#generate keys,values and queries from inputs</span></span><br><span class="line">        keys = self.linear_k(keys) <span class="comment"># 计算Wk * E(输入词向量) = K</span></span><br><span class="line">        values = self.linear_v(values) <span class="comment"># Wv * E  = V</span></span><br><span class="line">        queries = self.linear_q(queries) <span class="comment">#Wq *E =Q</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#以下做的就是将Q,K,V分别拆分成num_head个 q,k,v</span></span><br><span class="line">        keys = keys.view(batch_size , <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>) </span><br><span class="line">        values = values.view(batch_size, <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        queries = queries.view(batch_size, <span class="number">-1</span>, self.num_heads, self.dim_per_head).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>,self.num_heads,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        scale = (keys.size(<span class="number">-1</span>)) ** <span class="number">-0.5</span></span><br><span class="line">        <span class="comment">#计算注意力</span></span><br><span class="line">        context = self.dot_product_attention(queries,keys,values,scale,attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将多个头的输出向量拼接合并</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>,<span class="number">2</span>).contiguous() \</span><br><span class="line">                  .view(batch_size,<span class="number">-1</span>,self.num_heads * self.dim_per_head)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization and residual network</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(residual+self.linear_final(context)) <span class="comment"># linear 将拼接够的多头 进行信息融合和映射回d维度</span></span><br></pre></td></tr></table></figure>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(\frac{pos}{1000^{2i/d_{model}}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)} = cos(\frac{pos}{1000^{2i/d_{model}}})</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">0</span>,<span class="number">10</span>).unsqueeze(<span class="number">1</span>).size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">div_term = torch.exp(torch.arange(<span class="number">0.</span>,<span class="number">512</span>,<span class="number">2</span>)*-(math.log(<span class="number">10000.0</span>)/<span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">div_term.size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([256])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe = torch.randn(<span class="number">20</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 9.0318e-01,  0.0000e+00,  1.2352e-01,  0.0000e+00, -3.0140e-01,
          0.0000e+00, -3.6465e-01,  0.0000e+00, -5.0365e-01,  0.0000e+00],
        [-4.5975e-01,  0.0000e+00,  8.5064e-01,  0.0000e+00, -2.6547e+00,
          0.0000e+00,  7.4937e-01,  0.0000e+00, -4.1507e-01,  0.0000e+00],
        [-1.4702e+00,  0.0000e+00,  4.7715e-01,  0.0000e+00,  8.0542e-01,
          0.0000e+00, -4.0687e-01,  0.0000e+00, -7.3654e-01,  0.0000e+00],
        [ 1.2496e+00,  0.0000e+00,  1.0493e+00,  0.0000e+00,  1.4115e+00,
          0.0000e+00, -4.0402e-01,  0.0000e+00,  1.9959e-01,  0.0000e+00],
        [ 4.1005e-01,  0.0000e+00, -1.3749e+00,  0.0000e+00, -9.4356e-02,
          0.0000e+00, -2.5279e-01,  0.0000e+00,  1.3641e+00,  0.0000e+00],
        [ 3.0355e-01,  0.0000e+00, -7.0061e-01,  0.0000e+00, -6.3308e-01,
          0.0000e+00,  7.0820e-02,  0.0000e+00, -6.3141e-02,  0.0000e+00],
        [-1.7276e+00,  0.0000e+00,  7.1022e-01,  0.0000e+00, -3.7692e-01,
          0.0000e+00,  5.7131e-01,  0.0000e+00, -1.0790e+00,  0.0000e+00],
        [-1.9643e+00,  0.0000e+00, -8.7474e-01,  0.0000e+00, -1.2753e+00,
          0.0000e+00,  2.8921e-01,  0.0000e+00, -1.4253e+00,  0.0000e+00],
        [ 8.4792e-01,  0.0000e+00,  2.9655e-02,  0.0000e+00, -9.0477e-02,
          0.0000e+00,  3.1047e-01,  0.0000e+00,  1.8603e+00,  0.0000e+00],
        [-5.7733e-01,  0.0000e+00, -2.1318e-01,  0.0000e+00, -2.9424e-01,
          0.0000e+00,  5.5969e-01,  0.0000e+00,  5.9077e-01,  0.0000e+00],
        [-9.6322e-01,  0.0000e+00,  8.8474e-01,  0.0000e+00,  2.2378e-01,
          0.0000e+00, -6.0010e-01,  0.0000e+00, -3.6576e-01,  0.0000e+00],
        [ 8.8694e-01,  0.0000e+00,  2.8291e-02,  0.0000e+00, -6.5218e-01,
          0.0000e+00, -3.9719e-01,  0.0000e+00, -8.0203e-01,  0.0000e+00],
        [ 4.1978e-01,  0.0000e+00, -2.4290e-01,  0.0000e+00,  7.7798e-02,
          0.0000e+00, -9.2004e-01,  0.0000e+00,  5.3866e-01,  0.0000e+00],
        [-1.0515e+00,  0.0000e+00, -1.0967e+00,  0.0000e+00, -1.0951e+00,
          0.0000e+00,  2.9280e-01,  0.0000e+00, -9.3913e-01,  0.0000e+00],
        [ 8.6279e-01,  0.0000e+00,  4.4137e-01,  0.0000e+00,  2.5958e-01,
          0.0000e+00,  7.3830e-01,  0.0000e+00,  7.2514e-01,  0.0000e+00],
        [ 1.5696e+00,  0.0000e+00, -6.6977e-01,  0.0000e+00, -1.4154e+00,
          0.0000e+00,  1.1696e+00,  0.0000e+00,  2.2280e-01,  0.0000e+00],
        [-1.2376e+00,  0.0000e+00, -1.3173e-01,  0.0000e+00,  1.9464e-01,
          0.0000e+00,  2.0106e-01,  0.0000e+00, -1.9465e-01,  0.0000e+00],
        [-8.8660e-01,  0.0000e+00, -1.7934e-01,  0.0000e+00,  1.1574e+00,
          0.0000e+00,  4.0144e-01,  0.0000e+00, -1.7495e-03,  0.0000e+00],
        [ 6.2252e-01,  0.0000e+00, -3.1496e-01,  0.0000e+00,  6.6546e-01,
          0.0000e+00, -1.8034e-01,  0.0000e+00, -7.8079e-01,  0.0000e+00],
        [ 7.3892e-01,  0.0000e+00,  1.0642e+00,  0.0000e+00, -1.8440e-01,
          0.0000e+00, -1.8549e+00,  0.0000e+00, -1.6177e+00,  0.0000e+00]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe[:,<span class="number">1</span>::<span class="number">2</span>]=<span class="number">0</span><span class="comment">#从第二个维度的第一个数据开始，为2表示每两个取其中第一个，简单说就是隔一行取一个</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">2</span>]=[<span class="number">1</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>[1, 0, 3]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pe[:,<span class="number">0</span>::<span class="number">8</span>].size()</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([20, 2])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#compute position encoding</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(PositionalEncoding,self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_seq_len,d_model) <span class="comment">#初始化位置向量</span></span><br><span class="line">        position = torch.arange(<span class="number">0.</span>,max_seq_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>,d_model,<span class="number">2</span>)*-(math.log(<span class="number">10000.0</span>)/d_model)) <span class="comment">#计算分母</span></span><br><span class="line"></span><br><span class="line">        pe[:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment">#计算位置编码向量里偶数位子的数值</span></span><br><span class="line">        pe[:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment">#计算位置编码里奇数位置的数值</span></span><br><span class="line"></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">"pe"</span>,pe)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        x = x + Variable(self.pe[:,:x.size(<span class="number">1</span>)],requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h4 id="前向-层归一"><a href="#前向-层归一" class="headerlink" title="前向+层归一"></a>前向+层归一</h4><script type="math/tex; mode=display">Out = Layernorm(x + W_2*ReLu(W_1+bias)+bias)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalWiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="comment">#前向传播+residual connection</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(PositionalWiseFeedForward,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.w1 = nn.Linear(d_model,ffn_dim)</span><br><span class="line">        self.w2 = nn.Linear(ffn_dim,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        output = self.w2(F.relu(self.w1(x)))</span><br><span class="line">        <span class="comment"># layer normalization and residual network</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x+self.dropout(output))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2018</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(EncoderLayer,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class="line">        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        context = self.attention(x,x,x,attn_mask)</span><br><span class="line">        output = self.feed_forward(context)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Encoder,self).__init__()</span><br><span class="line">        <span class="comment">#以下代码是建立num_layer层 </span></span><br><span class="line">        self.encoder_layers = nn.ModuleList(</span><br><span class="line">                            [EncoderLayer(d_model,num_heads,ffn_dim,dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = PositionalEncoding(d_model, max_seq_len,dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, seq_embedding)</span>:</span></span><br><span class="line"></span><br><span class="line">        embedding = seq_embedding(x)</span><br><span class="line">        output = self.pos_embedding(embedding)</span><br><span class="line">        self_attention_mask = padding_mask(x,x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> encoder <span class="keyword">in</span> self.encoder_layers:</span><br><span class="line">            output = encoder(output,self_attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.norm(output)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(DecoderLayer,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class="line">        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_inputs, enc_outputs, self_attn_mask = None,context_attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        dec_ouput  = self.attention(dec_inputs, dec_inputs, dec_inputs ,self_attn_mask)</span><br><span class="line"></span><br><span class="line">        dec_ouput = self.attention(enc_outputs, enc_outputs,dec_ouput, context_attn_mask)</span><br><span class="line"></span><br><span class="line">        dec_ouput = self.feed_forward(dec_ouput)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_ouput</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_seq_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model  = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 )</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Decoder,self).__init__()</span><br><span class="line">        self.device = device</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        self.decoder_layers = nn.ModuleList(</span><br><span class="line">            [DecoderLayer(d_model,num_heads,ffn_dim,dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br><span class="line"></span><br><span class="line">        self.seq_embedding = nn.Embedding(vocab_size, d_model, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.pos_embedding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line">        self.linear = nn.Linear(d_model, vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, enc_output, seq_embedding, context_attn_mask = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        embedding = seq_embedding(inputs)</span><br><span class="line">        output =  embedding + self.pos_embedding(embedding)</span><br><span class="line"></span><br><span class="line">        self_attention_padding_mask = padding_mask(inputs, inputs)</span><br><span class="line">        seq_mask = sequence_mask(inputs).to(self.device)</span><br><span class="line">        self_attn_mask = torch.gt((self_attention_padding_mask+seq_mask), <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> decoder <span class="keyword">in</span> self.decoder_layers:</span><br><span class="line">            output = decoder(output, enc_output,self_attn_mask,context_attn_mask)</span><br><span class="line"></span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">nn.Embedding??</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#Build transformer model</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_len,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_layers = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 stack_layers= <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 d_model = <span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads = <span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ffn_dim = <span class="number">2048</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout = <span class="number">0.2</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(vocab_size, max_len,num_layers,d_model,num_heads,ffn_dim,dropout)</span><br><span class="line">        self.decoder = Decoder(vocab_size, max_len,device, num_layers,d_model,num_heads, ffn_dim, dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size,d_model)</span><br><span class="line">        self.linear = nn.Linear(d_model, vocab_size, bias = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#self.softmax = nn.Softmax(dim = 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src_seq, dec_tgt,dec_in)</span>:</span>                           <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">        context_attn_mask_dec = padding_mask(dec_tgt, src_seq)</span><br><span class="line"></span><br><span class="line">        en_output = self.encoder(src_seq,self.embedding)</span><br><span class="line"></span><br><span class="line">        dec_output = self.decoder(dec_tgt, en_output,self.embedding,context_attn_mask_dec)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span><span class="params">(seq_k, seq_q)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pad sentence</span></span><br><span class="line">    len_q = seq_q.size(<span class="number">1</span>)</span><br><span class="line">    pad_mask = seq_k.eq(<span class="number">0</span>)</span><br><span class="line">    pad_mask = pad_mask.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>,len_q,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pad_mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">padding_mask(inputs,inputs)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True],
         [False, False, False,  True,  True,  True,  True,  True]],

        [[False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True],
         [False, False,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True,  True,  True,  True]],

        [[False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True],
         [False, False, False, False,  True,  True,  True,  True]]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span><span class="params">(seq)</span>:</span></span><br><span class="line"></span><br><span class="line">    batch_size , seq_len = seq.size()</span><br><span class="line">    mask = torch.triu(torch.ones((seq_len, seq_len),dtype = torch.uint8),<span class="comment">#上三角矩阵，加上diagnoal</span></span><br><span class="line">                      diagonal = <span class="number">1</span>)</span><br><span class="line">    mask = mask.unsqueeze(<span class="number">0</span>).expand(batch_size, <span class="number">-1</span>,<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sequence_mask(inputs)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]],

        [[0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Language Model</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP学习笔记</title>
    <url>/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="NLP中如何发掘模型的可解释性"><a href="#NLP中如何发掘模型的可解释性" class="headerlink" title="NLP中如何发掘模型的可解释性"></a>NLP中如何发掘模型的可解释性</h2><p>可解释性在AI的模型设计中十分重要。需要防止模型存在偏见和缺陷带来的伦理问题，并且帮助决策者理解如何正确地使用我们的模型。越是严苛的场景，越需要模型提供证明它们是如何运作且避免错误的证据。如实时性较强的无人驾驶领域，黑盒模型无法让人们信服其工作的安全性。</p>
<p>通常深度学习模型就像一个黑匣子，它能预测出很好的结果，但是你并不知道它为什么会预测出这样的结果。想知道它是如何工作的，那么得尝试打开这个黑匣子，解释模型的意义十分必要。</p>
<a id="more"></a>
<h3 id="现有方法："><a href="#现有方法：" class="headerlink" title="现有方法："></a>现有方法：</h3><h4 id="通用性思路："><a href="#通用性思路：" class="headerlink" title="通用性思路："></a>通用性思路：</h4><p>建模前：选用可解释性模型，如：决策树模型、线性回归、逻辑回归、广义线性回归、广义加性模型、贝叶斯实例模型等<br>建模后：使用可解释性方法，主要是针对具有黑箱性质的深度学习模型而言的，主要分为以下几类的工作：隐层分析方法、 模拟/代理模型、敏感性分析方法</p>
<p><a href="https://arxiv.org/abs/1602.04938" target="_blank" rel="noopener">LIME：通过局部线性逼近解释模型预测的方法</a></p>
<p><a href="https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf" target="_blank" rel="noopener">激活最大化：了解哪种输入模式产生最大的模型响应</a></p>
<p><a href="https://distill.pub/2017/feature-visualization/" target="_blank" rel="noopener">特征可视化：</a></p>
<p><a href="http://www.interpretable-ml.org/nips2017workshop/papers/04.pdf" target="_blank" rel="noopener">将DNN层嵌入到低维可解释的空间中:</a></p>
<p><a href="https://deepmind.com/blog/cognitive-psychology/" target="_blank" rel="noopener">采用认知心理学的方法：</a></p>
<p>不确定性估计方法</p>
<p>对于NLP，BERT模型的可视化</p>
<p><a href="https://www.jianshu.com/p/df7906a2a28e" target="_blank" rel="noopener">这里是一篇详细介绍BERT可视化的文章</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/78745325" target="_blank" rel="noopener">面向可解释的NLP：北大、哈工大等提出文本分类的生成性解释框架</a></p>
<h3 id="在Seq2Seq和注意力机制中如何可视化模型细节"><a href="#在Seq2Seq和注意力机制中如何可视化模型细节" class="headerlink" title="在Seq2Seq和注意力机制中如何可视化模型细节"></a>在Seq2Seq和注意力机制中如何可视化模型细节</h3><p><a href="https://zhuanlan.zhihu.com/p/51428657" target="_blank" rel="noopener">大致参考</a></p>
<h3 id="对抗样本能否运用到自然语言处理模型中"><a href="#对抗样本能否运用到自然语言处理模型中" class="headerlink" title="对抗样本能否运用到自然语言处理模型中"></a>对抗样本能否运用到自然语言处理模型中</h3><p><a href="https://www.secrss.com/articles/11247" target="_blank" rel="noopener">AI最前线：清华大学开源对抗样本必读论文列表</a></p>
<p>自然语言处理方面的研究在近几年取得了惊人的进步，深度神经网络模型已经取代了许多传统的方法。但是，当前提出的许多自然语言处理模型并不能够反映文本的多样特征。因此，许多研究者认为应该开辟新的研究方法，特别是利用近几年较为流行的对抗样本生成和防御的相关研究方法。</p>
<p>使用对抗样本生成和防御的自然语言处理研究可以基本概括为以下三种：1. 用未察觉的扰动迷惑模型，并评价模型在这种情况下的表现；2. 有意的改变深度神经网络的输出；3. 检测深度神经网络是否过于敏感或过于稳定，并寻找防御攻击的方法。</p>
<p>Jia 和 Liang 首先考虑在深度神经网络中采用对抗样本生成（或者「对抗攻击」，两者皆可）方法完成文本处理相关任务。他们的研究在自然语言处理社区很快获得了研究方面的关注。</p>
<p>然而，由于图片和文本数据内在的不同，用于图像的对抗攻击方法无法直接应用与文本数据上。首先，图像数据（例如像素值）是连续的，但文本数据是离散的。其次，仅仅对像素值进行微小的改变就可以造成图像数据的扰动，而且这种扰动是很难被人眼差距的。但是对于文本的对抗攻击中，小的扰动很容易被察觉，但人类同样能「猜出」本来表达的意义。因此 NLP 模型需要对可辨识的特征鲁棒，而不像视觉只需要对「不太重要」的特征鲁棒。</p>
<p><img src="image1.png" alt="image.png"></p>
<p>DeepWordBug 的深度网络攻击示例。选自 arXiv：1902.07285</p>
<p>与图像领域一样，有进攻就会有防御，目前也有很多研究尝试构建更鲁棒的自然语言处理模型。例如在 CMU 的一篇对抗性拼写错误论文（arXiv：1905.11268）中，研究者通过移除、添加或调序单词内部的字符，以构建更稳健的文本分类模型。这些增减或调序都是一种扰动，就像人类也很可能出现这些笔误一样。通过这些扰动，模型能学会如何处理错别字，从而不至于对分类结果产生影响</p>
<p><img src="image2.png" alt="image.png"></p>
<p>对抗性拼写错误导致的情感误分类，与通过字识别防御手段获得的更稳健模型。选自 arXiv：1905.11268</p>
<h3 id="复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧"><a href="#复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧" class="headerlink" title="复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧"></a>复现Kaggle心脏病数据集冠军kernel，理解所用的模型可解释性技巧</h3><p><a href="https://www.kaggle.com/scratchpad/kernel4b1df8c794/edit" target="_blank" rel="noopener">kernel</a></p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>PdfMiner文档解析</title>
    <url>/2020/02/27/PdfMiner%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>PDFMiner是一个可以从PDF文档中提取信息的工具。与其他PDF相关的工具不同，它注重的完全是获取和分析文本数据。PDFMiner允许你获取某一页中文本的准确位置和一些诸如字体、行数的信息。它包括一个PDF转换器，可以把PDF文件转换成HTML等格式。它还有一个扩展的PDF解析器，可以用于除文本分析以外的其他用途。<a href="https://euske.github.io/pdfminer/" target="_blank" rel="noopener">官方主页</a></p>
<p>其特征有：1、完全使用python编写。（适用于2.4或更新版本）2、解析，分析，并转换成PDF文档。3、PDF-1.7规范的支持。（几乎）4、中日韩语言和垂直书写脚本支持。5、各种字体类型（Type1、TrueType、Type3，和CID）的支持。6、基本加密（RC4）的支持。7、PDF与HTML转换。8、纲要（TOC）的提取。9、标签内容提取。10、通过分组文本块重建原始的布局。<br>如果你的Python有安装pip模块，就可以通过命令“python pip install pdfminer”，自动安装pdfminer。<br><a id="more"></a><br>解析pdf文件用到的类：</p>
<ul>
<li>PDFParser：从一个文件中获取数据</li>
<li>PDFDocument：保存获取的数据，和PDFParser是相互关联的</li>
<li>PDFPageInterpreter处理页面内容</li>
<li>PDFDevice将其翻译成你需要的格式</li>
<li>PDFResourceManager用于存储共享资源，如字体或图像。</li>
</ul>
<p><strong>pdfminer之间的关系图：</strong></p>
<hr>
<p><img src="pdfminer.png" alt="avatar"></p>
<p>Layout布局分析返回的PDF文档中的每个页面LTPage对象。这个对象和页内包含的子对象，形成一个树结构。如图所示<br><img src="pdfminer2.png" alt="avatar"></p>
<p><em>pdf转换成text的核心代码</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFResourceManager,PDFPageInterpreter</span><br><span class="line"><span class="keyword">from</span> pdfminer.converter <span class="keyword">import</span> TextConverter</span><br><span class="line"><span class="keyword">from</span> pdfminer.layout <span class="keyword">import</span> LAParams</span><br><span class="line"><span class="keyword">from</span> pdfminer.pdfpage <span class="keyword">import</span> PDFPage</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_pdf_to_txt</span><span class="params">(path)</span>:</span></span><br><span class="line">    rsrcmgr =PDFResourceManager()</span><br><span class="line">    retstr = StringIO()</span><br><span class="line">    codec =<span class="string">'utf-8'</span></span><br><span class="line">    laparams =LAParams()</span><br><span class="line">    device = TextConverter(rsrcmgr, retstr,laparams = laparams)</span><br><span class="line">    fp =open(path,<span class="string">'rb'</span>)</span><br><span class="line">    interpreter =PDFPageInterpreter(rsrcmgr, device)</span><br><span class="line">    password = <span class="string">""</span></span><br><span class="line">    maxpages = <span class="number">0</span></span><br><span class="line">    caching =<span class="literal">True</span></span><br><span class="line">    pagenos= set()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> PDFPage.get_pages(fp,pagenos,maxpages=maxpages,password=password,caching=caching,check_extractable=<span class="literal">True</span>):</span><br><span class="line">        interpreter.process_page(page)</span><br><span class="line"></span><br><span class="line">    text = retstr.getvalue()</span><br><span class="line"></span><br><span class="line">    fp.close()</span><br><span class="line">    device.close()</span><br><span class="line">    retstr.close()</span><br><span class="line">    <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>
<p><em>保存成文本文件</em><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveTxt</span><span class="params">(txt, filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename[:<span class="number">-3</span>]+<span class="string">'txt'</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        print(<span class="string">'openTxt:'</span> + filename[:<span class="number">-3</span>]+<span class="string">'txt'</span>)</span><br><span class="line">        f.write(txt)</span><br></pre></td></tr></table></figure><br><em>转换一个文件夹中的所有pdf文件</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traversal</span><span class="params">(rootdir)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> parent, dirnames, filenames <span class="keyword">in</span> os.walk(rootdir):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            filenameFull = os.path.join(parent, filename)</span><br><span class="line">            <span class="keyword">if</span> (filenameFull.endswith(<span class="string">'pdf'</span>) <span class="keyword">or</span> filenameFull.endswith(<span class="string">'PDF'</span>)):</span><br><span class="line">                txt = readPDF(filenameFull)</span><br><span class="line">                saveTxt(txt.replace(<span class="string">u'\xa9'</span>, <span class="string">u''</span>).replace(<span class="string">u'\xa0'</span>,<span class="string">u''</span>).replace(<span class="string">u'\xad'</span>,<span class="string">u''</span>).replace(<span class="string">u'\u037e'</span>,<span class="string">u''</span>), filenameFull)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    rootdir = <span class="string">'./'</span></span><br><span class="line">    traversal(rootdir)</span><br></pre></td></tr></table></figure>
<p>我们拿其中的一篇pdf文件做解析[PDF] WHO R&amp;D Blueprint novel Coronavirus prospects for evaluating cross-reactivity of nCoV with SARS-CoV January 24, 2020, Geneva, Switzerland.pdf</p>
<p>解析部分页面如下:<br><img src="pdf.png" alt="avatar"></p>
<p>我们通过解析：可以将此pdf转换为纯文本形式：<br><img src="text.png" alt="avatar"></p>
<p>通过对比发现，可以将PDF文件合理的解析成纯文本文件。</p>
]]></content>
      <tags>
        <tag>PDF解析</tag>
      </tags>
  </entry>
  <entry>
    <title>全连接神经网络-FNN</title>
    <url>/2020/02/27/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="comment">#该类为所有其他图节点类的父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,inputs=[])</span>:</span></span><br><span class="line">        <span class="comment">#定义每个节点的输入和输出</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.outputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每个节点都是其输入节点的输出节点</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs:</span><br><span class="line">            n.outputs.append(self)</span><br><span class="line">            <span class="comment"># set 'self' node as inbound_nodes's outbound_nodes</span></span><br><span class="line">            </span><br><span class="line">        self.value = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        self.gradients = &#123;&#125;</span><br><span class="line">        <span class="comment"># keys are the inputs to this node,and</span></span><br><span class="line">        <span class="comment">#their values are the partials of this node with</span></span><br><span class="line">        <span class="comment"># respect to that input.</span></span><br><span class="line">        <span class="comment"># \partial&#123;node&#125;&#123;input_i&#125;</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播函数，继承该类的其他类会覆写该函数</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string">        Compute the output value based on 'inbound_nodes' and store the</span></span><br><span class="line"><span class="string">        result in self.value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">raise</span> <span class="built_in">NotImplemented</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播函数，继承该类的其他类会覆写该函数</span></span><br><span class="line">        <span class="keyword">raise</span> <span class="built_in">NotImplemented</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Input</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#输入节点，包括神经网络输入节点，权重节点，和偏差节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        An Input node has no inbound nodes,</span></span><br><span class="line"><span class="string">        So no need to pass anythinto the Node instantiator.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Node.__init__(self)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, value=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Only input node is the node where the value may be passed</span></span><br><span class="line"><span class="string">        as anargument to forward().</span></span><br><span class="line"><span class="string">        All other node implementations should get the value of the </span></span><br><span class="line"><span class="string">        previous node from self.inbound_nodes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string">        val0:self.inbound_nodes[0].value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#定义节点数值</span></span><br><span class="line">        <span class="keyword">if</span> value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.value =value</span><br><span class="line">            <span class="comment">#It's is input node,when need to forward,this node initiate</span></span><br><span class="line">            <span class="comment">#self's value.</span></span><br><span class="line">        <span class="comment"># Input subclass just holds a value,such as a data feature or</span></span><br><span class="line">        <span class="comment"># model parameter(weight/bias)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#计算节点梯度</span></span><br><span class="line">        self.gradients = &#123;self:<span class="number">0</span>&#125;<span class="comment"># initialization</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            <span class="comment">#以下计算该节点的输出节点对该节点的梯度</span></span><br><span class="line">            grad_cost = n.gradients[self]</span><br><span class="line">            self.gradients[self] =grad_cost*<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># input --&gt; N1,N2</span></span><br><span class="line">            <span class="comment">#\partial L / \partial N</span></span><br><span class="line">            <span class="comment"># ==&gt; \partial L / partial N1 * \ partial N1 / \partial N</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Add</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,*nodes)</span>:</span></span><br><span class="line">        Node.__init__(self,nodes)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.value = sum(map(<span class="keyword">lambda</span> n:n.value,self.inputs))</span><br><span class="line">        <span class="comment"># when execute forward, this node cacultae value as defined</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#全连接网络层的计算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,nodes,weights,bias)</span>:</span></span><br><span class="line">        Node.__init__(self,[nodes,weights,bias])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播的计算 y=w*x + b</span></span><br><span class="line">        inputs = self.inputs[<span class="number">0</span>].value</span><br><span class="line">        weights = self.inputs[<span class="number">1</span>].value</span><br><span class="line">        bias = self.inputs[<span class="number">2</span>].value</span><br><span class="line">        </span><br><span class="line">        self.value =np.dot(inputs,weights) + bias</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播计算</span></span><br><span class="line">        <span class="comment"># initial a partial for each of the inbound_nodes.</span></span><br><span class="line">        self.gradients = &#123;n:np.zeros_like(n.value) <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            <span class="comment"># Get the partial of the cost w.r.t this node.</span></span><br><span class="line">            grad_cost = n.gradients[self]</span><br><span class="line">            </span><br><span class="line">            self.gradients[self.inputs[<span class="number">0</span>]] = np.dot(grad_cost,self.inputs[<span class="number">1</span>].value.T)</span><br><span class="line">            self.gradients[self.inputs[<span class="number">1</span>]] = np.dot(self.inputs[<span class="number">0</span>].value.T,grad_cost)</span><br><span class="line">            self.gradients[self.inputs[<span class="number">2</span>]] = np.sum(grad_cost,axis=<span class="number">0</span>,keepdims=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># WX + B / W ==&gt; X</span></span><br><span class="line">        <span class="comment"># WX + B / X ==&gt; W</span></span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment">#定义sigmod函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,node)</span>:</span></span><br><span class="line">        Node.__init__(self,[node])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1</span>+np.exp(<span class="number">-1</span>*x))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向 即sigmoid函数计算</span></span><br><span class="line">        self.x = self.inputs[<span class="number">0</span>].value <span class="comment"># [0] input is a list</span></span><br><span class="line">        self.value = self._sigmoid(self.x)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向传播计算梯度</span></span><br><span class="line">        self.partial = self._sigmoid(self.x) * (<span class="number">1</span> -self._sigmoid(self.x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># y = 1/(1+ e^-x)</span></span><br><span class="line">        <span class="comment"># y'= 1/(1 + e^-x) (1 - 1/(1 + e^-x))</span></span><br><span class="line">        </span><br><span class="line">        self.gradients = &#123;n:np.zeros_like(n.value) <span class="keyword">for</span> n <span class="keyword">in</span> self.inputs&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.outputs:</span><br><span class="line">            grad_cost = n.gradients[self] <span class="comment"># Get the partial of the cost with respect to this node</span></span><br><span class="line">            </span><br><span class="line">            self.gradients[self.inputs[<span class="number">0</span>]] = grad_cost * self.partial</span><br><span class="line">            <span class="comment"># use * to keep all the dimension same!.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSE</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="comment"># 定义平均平方误差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,y,a)</span>:</span></span><br><span class="line">        Node.__init__(self,[y,a])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#前向传播计算</span></span><br><span class="line">        y = self.inputs[<span class="number">0</span>].value.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        a = self.inputs[<span class="number">1</span>].value.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span>(y.shape == a.shape)</span><br><span class="line">        </span><br><span class="line">        self.m = self.inputs[<span class="number">0</span>].value.shape[<span class="number">0</span>]</span><br><span class="line">        self.diff = y -a</span><br><span class="line">        </span><br><span class="line">        self.value = np.mean(self.diff**<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#反向计算相应的梯度</span></span><br><span class="line">        self.gradients[self.inputs[<span class="number">0</span>]] = ( <span class="number">2</span> / self.m) * self.diff</span><br><span class="line">        self.gradients[self.inputs[<span class="number">1</span>]] = ( <span class="number">-2</span> /self.m) * self.diff</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_and_backward</span><span class="params">(outputnode,graph)</span>:</span></span><br><span class="line">    <span class="comment"># execute all the forward method of sorted_nodes.</span></span><br><span class="line">    <span class="comment">## In practice,it's common to feed in mutiple data example in each forward pass rather than just 1. Because the example can be</span></span><br><span class="line">    <span class="comment">## processed in parallel.The number of examples is called batch size.</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph:</span><br><span class="line">        n.forward()</span><br><span class="line">        <span class="comment"># each node execute forward, get self.value based on the topological sort result.</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph[::<span class="number">-1</span>]:</span><br><span class="line">        n.backward()</span><br><span class="line">    <span class="comment"># return outputnode.value</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### v -&gt; a -&gt; C</span></span><br><span class="line"><span class="comment">##  b -&gt; C</span></span><br><span class="line"><span class="comment">##  b -&gt; v - a -&gt; C</span></span><br><span class="line"><span class="comment">## v -&gt; v -&gt; a -&gt; C</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topological_sort</span><span class="params">(feed_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sort generic nodes in topological order using Kahn's Algorithm.</span></span><br><span class="line"><span class="string">    'feed_dict': A dictionary where the key is a 'Input' node and the value is the respective value feed to that node.</span></span><br><span class="line"><span class="string">    Returns a list of sorted nodes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    input_nodes = [n <span class="keyword">for</span> n <span class="keyword">in</span> feed_dict.keys()]</span><br><span class="line">    </span><br><span class="line">    G = &#123;&#125;</span><br><span class="line">    nodes = [n <span class="keyword">for</span> n <span class="keyword">in</span> input_nodes]</span><br><span class="line">    <span class="keyword">while</span> len(nodes)&gt;<span class="number">0</span>:</span><br><span class="line">        n = nodes.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> G:</span><br><span class="line">            G[n] = &#123;<span class="string">'in'</span>:set(),<span class="string">'out'</span>:set()&#125;</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> n.outputs:</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">not</span> <span class="keyword">in</span> G:</span><br><span class="line">                G[m] = &#123;<span class="string">'in'</span>:set(),<span class="string">'out'</span>:set()&#125;</span><br><span class="line">            G[n][<span class="string">'out'</span>].add(m)</span><br><span class="line">            G[m][<span class="string">'in'</span>].add(n)</span><br><span class="line">            nodes.append(m)</span><br><span class="line">    </span><br><span class="line">    L =[]</span><br><span class="line">    S = set(input_nodes)</span><br><span class="line">    <span class="keyword">while</span> len(S) &gt;<span class="number">0</span>:</span><br><span class="line">        n = S.pop()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> isinstance(n,Input):</span><br><span class="line">            n.value= feed_dict[n]</span><br><span class="line">            <span class="comment">## if n is Input Node,setn'value as</span></span><br><span class="line">            <span class="comment">## feed_dict[n]</span></span><br><span class="line">            <span class="comment">## else, n's value is caculate as its</span></span><br><span class="line">            <span class="comment">## inbounds</span></span><br><span class="line">        L.append(n)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> n.outputs:</span><br><span class="line">            G[n][<span class="string">'out'</span>].remove(m)</span><br><span class="line">            G[m][<span class="string">'in'</span>].remove(n)</span><br><span class="line">            <span class="comment"># if no other incoming edges add to S</span></span><br><span class="line">            <span class="keyword">if</span> len(G[m][<span class="string">'in'</span>]) == <span class="number">0</span>:</span><br><span class="line">                S.add(m)</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_update</span><span class="params">(trainables,learning_rate =<span class="number">1e-2</span>)</span>:</span></span><br><span class="line">    <span class="comment">#there are so many other update / optimigation methods</span></span><br><span class="line">    <span class="comment">#such as Adam,Mom,</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> trainables:</span><br><span class="line">        t.value += <span class="number">-1</span> * learning_rate * t.gradients[t]</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\partial{node}{input_i}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = load_boston()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">losses = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Check out the new network architecture and dataset!</span></span><br><span class="line"><span class="string">Notice that the weights and biases are </span></span><br><span class="line"><span class="string">generated randomly.</span></span><br><span class="line"><span class="string">No need to change anything,but feel free to tweak</span></span><br><span class="line"><span class="string">to test your network, play around with the epoches, batch size,etc!</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle,resample</span><br><span class="line"><span class="comment">#from minflow import *</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Load data</span></span><br><span class="line">data =load_boston()</span><br><span class="line">X_ = data[<span class="string">'data'</span>]</span><br><span class="line">_ = data[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize data</span></span><br><span class="line"></span><br><span class="line">X_ = (X_ - np.mean(X_,axis=<span class="number">0</span>)) / np.std(X_,axis =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">n_features =X_.shape[<span class="number">1</span>]</span><br><span class="line">n_hidden = <span class="number">10</span></span><br><span class="line">W1_ = np.random.randn(n_features,n_hidden)</span><br><span class="line">b1_ = np.zeros(n_hidden)</span><br><span class="line">W2_ = np.random.randn(n_hidden,<span class="number">1</span>)</span><br><span class="line">b2_ = np.zeros(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neural network</span></span><br><span class="line">X,y = Input(),Input()</span><br><span class="line">W1,b1=Input(),Input()</span><br><span class="line">W2,b2 =Input(),Input()</span><br><span class="line"></span><br><span class="line">l1 = Linear(X,W1,b1)</span><br><span class="line">s1 = Sigmoid(l1)</span><br><span class="line">l2 = Linear(s1,W2,b2)</span><br><span class="line">cost = MSE(y,l2)</span><br><span class="line"></span><br><span class="line">feed_dict = &#123;</span><br><span class="line">    X:X_,</span><br><span class="line">    y:y_,</span><br><span class="line">    W1:W1_,</span><br><span class="line">    b1:b1_,</span><br><span class="line">    W2:W2_,</span><br><span class="line">    b2:b2_</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5000</span></span><br><span class="line"><span class="comment"># Total number of examples</span></span><br><span class="line">m = X_.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">steps_per_epoch = m // batch_size</span><br><span class="line"></span><br><span class="line">graph = topological_sort(feed_dict)</span><br><span class="line">trainables = [W1,b1,W2,b2]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total number of examples = &#123;&#125;"</span>.format(m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#Step 4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(steps_per_epoch):</span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        <span class="comment"># Randomly sample a batch of examples</span></span><br><span class="line">        X_batch,y_batch =resample(X_,y_,n_samples=batch_size)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reset valueof X and y Inputs</span></span><br><span class="line">        X.value = X_batch</span><br><span class="line">        y.value = y_batch</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        _ = <span class="literal">None</span></span><br><span class="line">        forward_and_backward(_,graph) <span class="comment">#set output node not important.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        rate =<span class="number">1e-2</span></span><br><span class="line">        </span><br><span class="line">        sgd_update(trainables,rate)</span><br><span class="line">        </span><br><span class="line">        loss += graph[<span class="number">-1</span>].value</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:&#123;&#125;,Loss:&#123;:.3f&#125;"</span>.format(i+<span class="number">1</span>,loss/steps_per_epoch))</span><br><span class="line">        losses.append(loss)</span><br></pre></td></tr></table></figure>
<pre><code>Total number of examples = 506
Epoch:1,Loss:171.205
Epoch:101,Loss:8.215
Epoch:201,Loss:7.937
Epoch:301,Loss:7.301
Epoch:401,Loss:6.069
Epoch:501,Loss:5.735
Epoch:601,Loss:4.524
Epoch:701,Loss:4.418
Epoch:801,Loss:4.473
Epoch:901,Loss:4.510
Epoch:1001,Loss:3.484
Epoch:1101,Loss:4.627
Epoch:1201,Loss:4.473
Epoch:1301,Loss:4.152
Epoch:1401,Loss:4.831
Epoch:1501,Loss:4.992
Epoch:1601,Loss:4.500
Epoch:1701,Loss:4.706
Epoch:1801,Loss:3.927
Epoch:1901,Loss:4.712
Epoch:2001,Loss:4.262
Epoch:2101,Loss:3.968
Epoch:2201,Loss:4.792
Epoch:2301,Loss:4.106
Epoch:2401,Loss:3.815
Epoch:2501,Loss:4.089
Epoch:2601,Loss:4.376
Epoch:2701,Loss:3.923
Epoch:2801,Loss:5.195
Epoch:2901,Loss:4.273
Epoch:3001,Loss:3.618
Epoch:3101,Loss:3.368
Epoch:3201,Loss:3.754
Epoch:3301,Loss:4.027
Epoch:3401,Loss:3.442
Epoch:3501,Loss:4.029
Epoch:3601,Loss:3.475
Epoch:3701,Loss:3.861
Epoch:3801,Loss:4.248
Epoch:3901,Loss:3.697
Epoch:4001,Loss:4.466
Epoch:4101,Loss:4.746
Epoch:4201,Loss:4.145
Epoch:4301,Loss:3.594
Epoch:4401,Loss:4.501
Epoch:4501,Loss:3.719
Epoch:4601,Loss:4.005
Epoch:4701,Loss:4.331
Epoch:4801,Loss:4.092
Epoch:4901,Loss:3.657
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(outputNode,graph)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> graph:</span><br><span class="line">        n.forward()</span><br><span class="line">    <span class="keyword">return</span> outputNode.value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">forward(l2,graph)</span><br></pre></td></tr></table></figure>
<pre><code>array([[18.41675144],
       [29.47806568],
       [15.32327987],
       [48.89022023],
       [49.92947466],
       [45.52560889],
       [30.32152378],
       [13.889439  ],
       [20.52297098],
       [38.29474339],
       [19.54218589],
       [10.72223761],
       [10.13966276],
       [19.20474889],
       [ 9.715787  ],
       [26.87621257]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(range(len(losses)),losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x1fd125539c8&gt;]
</code></pre><p><img src="output_10_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W2.value</span><br></pre></td></tr></table></figure>
<pre><code>array([[3.78502026],
       [5.48520715],
       [6.6856144 ],
       [5.89625598],
       [5.81552044],
       [3.22412022],
       [8.54504219],
       [2.76550267],
       [9.40747634],
       [5.60113079]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_ = data[<span class="string">'data'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,
       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,
       4.980e+00])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line">model =Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units = <span class="number">64</span>,activation=<span class="string">'sigmoid'</span>,input_dim=<span class="number">13</span>))</span><br><span class="line">model.add(Dense(units = <span class="number">30</span>,activation=<span class="string">'sigmoid'</span>,input_dim=<span class="number">64</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.compile(loss = <span class="string">'mse'</span>,optimizer=<span class="string">'sgd'</span>,metrics = [<span class="string">'mse'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(X_,y_,epochs=<span class="number">5000</span>,batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5000
506/506 [==============================] - 1s 3ms/step - loss: 158.0243 - mse: 158.0243
Epoch 2/5000
506/506 [==============================] - 0s 91us/step - loss: 81.3887 - mse: 81.3887
Epoch 3/5000
506/506 [==============================] - 0s 87us/step - loss: 79.5072 - mse: 79.5072
Epoch 4/5000
506/506 [==============================] - 0s 
Epoch 2700/5000
506/506 [==============================] - 0s 99us/step - loss: 85.0554 - mse: 85.0554
Epoch 2701/5000
506/506 [==============================] - 0s 142us/step - loss: 84.8442 - mse: 84.8442
Epoch 2702/5000
506/506 [==============================] - 0s 103us/step - loss: 85.1862 - mse: 85.1862
Epoch 2703/5000
506/506 [==============================] - 0s 97us/step - loss: 84.6198 - mse: 84.6198
Epoch 2704/5000
506/506 [==============================] - 0s 89us/step - loss: 85.3531 - mse: 85.3531
Epoch 2705/5000
506/506 [==============================] - 0s 101us/step - loss: 84.8638 - mse: 84.8638
Epoch 2706/5000
506/506 [==============================] - 0s 87us/step - loss: 84.6398 - mse: 84.6398
Epoch 2707/5000
506/506 [==============================] - 0s 154us/step - loss: 84.8038 - mse: 84.8038
Epoch 2708/5000
506/506 [==============================] - 0s 99us/step - loss: 84.6299 - mse: 84.6299
Epoch 2709/5000
506/506 [==============================] - 0s 101us/step - loss: 84.8330 - mse: 84.8330
Epoch 2710/5000
506/506 [==============================] - 0s 130us/step - loss: 85.0682 - mse: 85.0682
Epoch 2711/5000
506/506 [==============================] - 0s 111us/step - loss: 84.6372 - mse: 84.6372
Epoch 2712/5000
506/506 [==============================] - 0s 101us/step - loss: 84.6715 - mse: 84.6715
Epoch 2713/5000
506/506 [==============================] - 0s 128us/step - loss: 85.0228 - mse: 85.0228
Epoch 2714/5000
506/506 [==============================] - 0s 93us/step - loss: 84.8349 - mse: 84.8349
Epoch 2715/5000
506/506 [==============================] - 0s 109us/step - loss: 84.7732 - mse: 84.7732
Epoch 2716/5000
506/506 [==============================] - 0s 113us/step - loss: 84.7862 - mse: 84.7862
Epoch 2717/5000
506/506 [==============================] - 0s 113us/step - loss: 84.5297 - mse: 84.5297
Epoch 2718/5000
506/506 [==============================] - 0s 107us/step - loss: 84.7425 - mse: 84.7425
Epoch 2719/5000
506/506 [==============================] - 0s 103us/step - loss: 85.2987 - mse: 85.2987
Epoch 2720/5000
506/506 [==============================] - 0s 107us/step - loss: 84.6666 - mse: 84.6666
Epoch 2721/5000
506/506 [==============================] - 0s 103us/step - loss: 
506/506 [==============================] - 0s 105us/step - loss: 85.1947 - mse: 85.1947
Epoch 3139/5000
506/506 [==============================] - 0s 97us/step - loss: 84.9491 - mse: 84.9491
Epoch 3140/5000
506/506 [==============================] - 0s 99us/step - loss: 85.3199 - mse: 85.3198
Epoch 3141/5000
 32/506 [&gt;.............................] - ETA: 0s - loss: 114.7459 - mse: 114.7459
</code></pre>]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Machinglearing Model,the first step</title>
    <url>/2020/01/31/Machinglearing-Model-the-first-step/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_data = np.random.random((<span class="number">20</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_data</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<pre><code>array([[0.50296664, 0.62445093],
       [0.19994622, 0.19373156],
       [0.14250226, 0.61334931],
       [0.70048398, 0.75160392],
       [0.74448897, 0.7320275 ],
       [0.85976709, 0.84319014],
       [0.73253413, 0.12288743],
       [0.88371578, 0.84136921],
       [0.97180754, 0.79078425],
       [0.05776667, 0.92731363],
       [0.03322522, 0.36021126],
       [0.30821425, 0.57943347],
       [0.16970345, 0.72740845],
       [0.5127129 , 0.44245729],
       [0.37546157, 0.477542  ],
       [0.96229987, 0.98167783],
       [0.79974288, 0.20093964],
       [0.65953995, 0.83289056],
       [0.83061176, 0.10022954],
       [0.57372662, 0.74422547]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = random_data[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y = random_data[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assuming_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 在我们的日常生活中是常见的</span></span><br><span class="line">    <span class="comment"># 体重-&gt; 高血压的概率</span></span><br><span class="line">    <span class="comment"># 收入-&gt; 买阿玛尼的概率</span></span><br><span class="line">    <span class="comment"># 其实都是一种潜在的函数关系 + 一个随机变化</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">13.4</span> *x + <span class="number">5</span> + random.randint(<span class="number">-5</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = [assuming_function(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x21e39725708&gt;
</code></pre><p><img src="output_11_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = np.array(y)</span><br></pre></td></tr></table></figure>
<h3 id="Regression-gt-Real-Number"><a href="#Regression-gt-Real-Number" class="headerlink" title="Regression -&gt; Real Number"></a>Regression -&gt; Real Number</h3><h3 id="Classification-gt-0-0-0-1-0-1-0-0"><a href="#Classification-gt-0-0-0-1-0-1-0-0" class="headerlink" title="Classification -&gt; [0,0,0,1],[0,1,0,0]"></a>Classification -&gt; [0,0,0,1],[0,1,0,0]</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>array([ 9.73975302,  6.67927941,  3.90953023, 10.38648538, 13.97615214,
       20.520879  , 10.81595738, 14.84179151, 19.02222101,  8.77407336,
        0.44521789, 13.13007093,  4.27402617,  7.87035283,  9.03118504,
       13.89481826, 18.71655464, 15.83783527, 16.1301976 , 10.68793665])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg = LinearRegression().fit(X.reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.score(X.reshape(<span class="number">-1</span>,<span class="number">1</span>),y)</span><br></pre></td></tr></table></figure>
<pre><code>0.7115853500059341
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.coef_</span><br></pre></td></tr></table></figure>
<pre><code>array([14.50455278])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reg.intercept_</span><br></pre></td></tr></table></figure>
<pre><code>3.441324165071637
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> reg.coef_ * x + reg.intercept_</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X,y)</span><br><span class="line">plt.plot(X,f(X),color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x21e3cf5c8c8&gt;]
</code></pre><p><img src="output_22_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>array([0.50296664, 0.19994622, 0.14250226, 0.70048398, 0.74448897,
       0.85976709, 0.73253413, 0.88371578, 0.97180754, 0.05776667,
       0.03322522, 0.30821425, 0.16970345, 0.5127129 , 0.37546157,
       0.96229987, 0.79974288, 0.65953995, 0.83061176, 0.57372662])
</code></pre><h3 id="How-to-implement-a-KNN-model"><a href="#How-to-implement-a-KNN-model" class="headerlink" title="How to implement a KNN model"></a>How to implement a KNN model</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    <span class="comment"># 直接存储X,y即可</span></span><br><span class="line">    <span class="keyword">return</span> [(Xi,yi) <span class="keyword">for</span> Xi, yi <span class="keyword">in</span> zip(X,y)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cosine</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cosine(x1,x2)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prdict</span><span class="params">(x,k=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment">#在predicate 的时候，需要做大量的计算</span></span><br><span class="line">    most_similars =sorted(model(X,y),key = <span class="keyword">lambda</span> xi:distance(xi[<span class="number">0</span>],x))[:k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># -&gt; regression:numerical -&gt; most_similars(y)</span></span><br><span class="line">    <span class="comment"># -&gt;classification:categorical -&gt; most_similar(y)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 已经获得了最相似的数据集</span></span><br><span class="line">    <span class="comment"># 然后呢，Counter()-&gt; most_common()-&gt;就可以获得出现最多的这个y了</span></span><br></pre></td></tr></table></figure>
<h3 id="不是简简单单的学一个算法，看到背后的思维方式："><a href="#不是简简单单的学一个算法，看到背后的思维方式：" class="headerlink" title="不是简简单单的学一个算法，看到背后的思维方式："></a>不是简简单单的学一个算法，看到背后的思维方式：</h3><blockquote>
<p>贝叶斯，线性回归，决策树，KNN</p>
</blockquote>
<h3 id="新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。"><a href="#新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。" class="headerlink" title="新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。"></a>新的问题，是不存在现成的解决方案的，但是，我们可以依据前人比较成熟的思维方法，我们发明新的方法。</h3><h3 id="How-to-implement-a-Decision-Tree"><a href="#How-to-implement-a-Decision-Tree" class="headerlink" title="How to implement a Decision Tree"></a>How to implement a Decision Tree</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><script type="math/tex; mode=display">Entropy = -\sum_i^n Pr(x_i) log(Pr(x_i))</script><h3 id="Gini-纯度"><a href="#Gini-纯度" class="headerlink" title="Gini 纯度"></a>Gini 纯度</h3><script type="math/tex; mode=display">Gini= 1 - \sum_{i=1}^J P_i^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">! pip install icecream</span><br></pre></td></tr></table></figure>
<pre><code>Collecting icecream
  Downloading https://files.pythonhosted.org/packages/8c/ec/821ef939e8e4f4306e7263afa7e2ce0b4c5da9e6e53d1cc97b01606035f8/icecream-2.0.0-py2.py3-none-any.whl
Requirement already satisfied: colorama&gt;=0.3.9 in d:\anaconda\lib\site-packages (from icecream) (0.4.1)
Collecting asttokens&gt;=2.0.1 (from icecream)
  Downloading https://files.pythonhosted.org/packages/e8/18/41e95b4a6b4fd3ae704e672da5d070272518995f580be79d772be312c4af/asttokens-2.0.3-py2.py3-none-any.whl
Requirement already satisfied: pygments&gt;=2.2.0 in d:\anaconda\lib\site-packages (from icecream) (2.4.2)
Collecting executing&gt;=0.3.1 (from icecream)
  Downloading https://files.pythonhosted.org/packages/79/a1/f85482473b12b2b0e1fa10da84d4280930dbd6e4e149cedf7ae91f894138/executing-0.4.1.tar.gz
Requirement already satisfied: six in d:\anaconda\lib\site-packages (from asttokens&gt;=2.0.1-&gt;icecream) (1.12.0)
Building wheels for collected packages: executing
  Building wheel for executing (setup.py): started
  Building wheel for executing (setup.py): finished with status &#39;done&#39;
  Created wheel for executing: filename=executing-0.4.1-cp37-none-any.whl size=8302 sha256=2fce6277eb7197756482de660a24f6d2c80bb965838cbbb515106f385e4ccad3
  Stored in directory: C:\Users\tb\AppData\Local\pip\Cache\wheels\b0\71\dc\c1bdcd4b384c4458b639dfa905bc093979b8779f2e0df78792
Successfully built executing
Installing collected packages: asttokens, executing, icecream
Successfully installed asttokens-2.0.3 executing-0.4.1 icecream-2.0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(elements)</span>:</span></span><br><span class="line">    <span class="string">'''群体混乱程度'''</span></span><br><span class="line">    counter = Counter(elements)</span><br><span class="line">    probs = [counter[c] / len(elements) <span class="keyword">for</span> c <span class="keyword">in</span> set(elements)]</span><br><span class="line">    ic(probs)</span><br><span class="line">    <span class="keyword">return</span> - sum(p * np.log(p) <span class="keyword">for</span> p <span class="keyword">in</span> probs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [1.0]





-0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]





0.5623351446188083
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]





0.5623351446188083
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.5, 0.25]





1.0397207708399179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.5, 0.25]





1.0397207708399179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.25, 0.25, 0.25]





1.3862943611198906
</code></pre><h3 id="决策树怎么来决定，哪一个特征来进行分割呢？"><a href="#决策树怎么来决定，哪一个特征来进行分割呢？" class="headerlink" title="决策树怎么来决定，哪一个特征来进行分割呢？"></a>决策树怎么来决定，哪一个特征来进行分割呢？</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mock_data = &#123;</span><br><span class="line">    <span class="string">'gender'</span>:[<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'M'</span>,<span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'income'</span>:[<span class="string">'+10'</span>,<span class="string">'-10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'+10'</span>,<span class="string">'-10'</span>],</span><br><span class="line">    <span class="string">'family_number'</span>:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">    <span class="comment">#'pet':[1,1,1,0,0,0,1],</span></span><br><span class="line">    <span class="string">'bought'</span>:[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = pd.DataFrame.from_dict(mock_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>F</td>
      <td>+10</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>6</td>
      <td>M</td>
      <td>-10</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>如果我们来了一个新的case:::[F,-10,2,1]-&gt;?</p>
<p>:::[F,+10,2,0]-&gt;?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split_bt_gender:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]) + entropy([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split_by_income:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) + entropy([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#split_by_family_number</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])+entropy([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># split_by_some_feature:</span></span><br><span class="line">print(entropy([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])+entropy([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.25, 0.75]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| probs: [0.6, 0.4]
ic| probs: [1.0]
ic| probs: [0.6, 0.4]
ic| probs: [1.0]
ic| probs: [1.0]
ic| probs: [1.0]


1.198849312913621
0.6730116670092565
0.6730116670092565
-0.0
</code></pre><h3 id="决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。"><a href="#决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。" class="headerlink" title="决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。"></a>决策树在选择决策过程，决策顺序的时候，其实是按照，根据这个特征，进行分割之后，数据的熵最少原则进行的。</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(mock_data[<span class="string">'family_number'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{1, 2}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(mock_data[<span class="string">'gender'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;F&#39;, &#39;M&#39;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_1 = dataset[dataset[<span class="string">'family_number'</span>]==<span class="number">1</span>][<span class="string">'bought'</span>].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_1</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, 0, 0, 0]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_2 =dataset[dataset[<span class="string">'family_number'</span>]!=<span class="number">1</span>][<span class="string">'bought'</span>].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_split_2</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data =dataset[dataset[<span class="string">'family_number'</span>]==<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splited_data[splited_data[<span class="string">'income'</span>] == <span class="string">'+10'</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="gt-根据信息熵，我们得到了一个决策过程："><a href="#gt-根据信息熵，我们得到了一个决策过程：" class="headerlink" title="===&gt; 根据信息熵，我们得到了一个决策过程："></a>===&gt; 根据信息熵，我们得到了一个决策过程：</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一步：我们观察他的家庭成员：</span><br><span class="line">         如果他的家庭成员是2人，那么就会购买，如果不是2人，我们继续决策，进入下一步</span><br><span class="line">第二部：我们观察他的收入情况：</span><br><span class="line">         如果他的收入是&#39;+10&#39;,那么他有 3&#x2F;4 的概率会购买，如果是&#39;-10&#39;，那么，他肯定不买</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy(sub_split_1)</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [0.6, 0.4]





0.6730116670092565
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">entropy(sub_split_2)</span><br></pre></td></tr></table></figure>
<pre><code>ic| probs: [1.0]





-0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set(dataset.columns.to_list()) - &#123;<span class="string">'bought'</span>&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;family_number&#39;, &#39;gender&#39;, &#39;income&#39;, &#39;pet&#39;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_the_optimal_spilter</span><span class="params">(training_data:pd.DataFrame,target:str)</span> -&gt; str:</span></span><br><span class="line">    x_fields = set(training_data.columns.tolist()) - &#123;target&#125;</span><br><span class="line">    </span><br><span class="line">    spliter = <span class="literal">None</span></span><br><span class="line">    min_entropy = float(<span class="string">'inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> x_fields:</span><br><span class="line">        ic(f)</span><br><span class="line">        values = set(training_data[f])</span><br><span class="line">        ic(values)</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> values:</span><br><span class="line">            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()</span><br><span class="line">            ic(sub_spliter_1)</span><br><span class="line">            <span class="comment"># split by the current feature and one value</span></span><br><span class="line">            </span><br><span class="line">            entropy_1 =entropy(sub_spliter_1)</span><br><span class="line">            ic(entropy_1)</span><br><span class="line">            </span><br><span class="line">            sub_spliter_2 = training_data[training_data[f] !=v][target].tolist()</span><br><span class="line">            ic(sub_spliter_2)</span><br><span class="line">            </span><br><span class="line">            entropy_2 = entropy(sub_spliter_2)</span><br><span class="line">            ic(entropy_2)</span><br><span class="line">            </span><br><span class="line">            entropy_v =entropy_1 +entropy_2</span><br><span class="line">            </span><br><span class="line">            ic(entropy_v)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> entropy_v &lt;= min_entropy:</span><br><span class="line">                min_entropy = entropy_v</span><br><span class="line">                spliter =(f,v)</span><br><span class="line">    print(<span class="string">' spliter is: &#123;&#125;'</span>.format(spliter))</span><br><span class="line">    print(<span class="string">' the min entropy is: &#123;&#125;'</span>.format(min_entropy))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> spliter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(training_data=dataset,target=<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;pet&#39;
ic| values: {0, 1}
ic| sub_spliter_1: [0, 0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: -0.0
ic| sub_spliter_1: [1, 1, 1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [0, 0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: -0.0
ic| f: &#39;family_number&#39;
ic| values: {1, 2}
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: [1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6730116670092565
ic| sub_spliter_1: [1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_2: 0.6730116670092565
ic| entropy_v: 0.6730116670092565
ic| f: &#39;income&#39;
ic| values: {&#39;-10&#39;, &#39;+10&#39;}
ic| sub_spliter_1: [1, 1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_2: 0.6730116670092565
ic| entropy_v: 0.6730116670092565
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: [1, 1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6730116670092565
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 1, 1, 0]
ic| probs: [0.25, 0.75]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: [0, 0, 1]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| entropy_2: 0.6365141682948128
ic| entropy_v: 1.198849312913621
ic| sub_spliter_1: [0, 0, 1]
ic| probs: [0.6666666666666666, 0.3333333333333333]
ic| entropy_1: 0.6365141682948128
ic| sub_spliter_2: [1, 1, 1, 0]
ic| probs: [0.25, 0.75]
ic| entropy_2: 0.5623351446188083
ic| entropy_v: 1.198849312913621


 spliter is: (&#39;pet&#39;, 1)
 the min entropy is: -0.0





(&#39;pet&#39;, 1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>F</td>
      <td>+10</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>6</td>
      <td>M</td>
      <td>-10</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>pet</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>F</td>
      <td>-10</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(dataset[dataset[<span class="string">'family_number'</span>] ==<span class="number">1</span> ],<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;family_number&#39;
ic| values: {1}
ic| sub_spliter_1: [1, 1, 0, 0, 0]
ic| probs: [0.6, 0.4]
ic| entropy_1: 0.6730116670092565
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.6730116670092565
ic| f: &#39;income&#39;
ic| values: {&#39;-10&#39;, &#39;+10&#39;}
ic| sub_spliter_1: [1]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_2: 0.5623351446188083
ic| entropy_v: 0.5623351446188083
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: [1]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 1, 0]
ic| probs: [0.3333333333333333, 0.6666666666666666]
ic| entropy_1: 0.6365141682948128
ic| sub_spliter_2: [0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6365141682948128
ic| sub_spliter_1: [0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 1, 0]
ic| probs: [0.3333333333333333, 0.6666666666666666]
ic| entropy_2: 0.6365141682948128
ic| entropy_v: 0.6365141682948128


 spliter is: (&#39;income&#39;, &#39;+10&#39;)
 the min entropy is: 0.5623351446188083





(&#39;income&#39;, &#39;+10&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fm_n_1 = dataset[dataset[<span class="string">'family_number'</span>] == <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fm_n_1[fm_n_1[<span class="string">'income'</span>]==<span class="string">'+10'</span>]</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gender</th>
      <th>income</th>
      <th>family_number</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>F</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>5</td>
      <td>M</td>
      <td>+10</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_the_optimal_spilter(fm_n_1[fm_n_1[<span class="string">'income'</span>] == <span class="string">'+10'</span>],<span class="string">'bought'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ic| f: &#39;family_number&#39;
ic| values: {1}
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;income&#39;
ic| values: {&#39;+10&#39;}
ic| sub_spliter_1: [1, 0, 0, 0]
ic| probs: [0.75, 0.25]
ic| entropy_1: 0.5623351446188083
ic| sub_spliter_2: []
ic| probs: []
ic| entropy_2: 0
ic| entropy_v: 0.5623351446188083
ic| f: &#39;gender&#39;
ic| values: {&#39;F&#39;, &#39;M&#39;}
ic| sub_spliter_1: [1, 0]
ic| probs: [0.5, 0.5]
ic| entropy_1: 0.6931471805599453
ic| sub_spliter_2: [0, 0]
ic| probs: [1.0]
ic| entropy_2: -0.0
ic| entropy_v: 0.6931471805599453
ic| sub_spliter_1: [0, 0]
ic| probs: [1.0]
ic| entropy_1: -0.0
ic| sub_spliter_2: [1, 0]
ic| probs: [0.5, 0.5]
ic| entropy_2: 0.6931471805599453
ic| entropy_v: 0.6931471805599453


 spliter is: (&#39;income&#39;, &#39;+10&#39;)
 the min entropy is: 0.5623351446188083





(&#39;income&#39;, &#39;+10&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-Methods"><a href="#Evaluation-Methods" class="headerlink" title="Evaluation Methods"></a>Evaluation Methods</h3><h4 id="1-Accuracy"><a href="#1-Accuracy" class="headerlink" title="1.Accuracy"></a>1.Accuracy</h4><h4 id="2-Precision"><a href="#2-Precision" class="headerlink" title="2. Precision"></a>2. Precision</h4><h4 id="3-Recall"><a href="#3-Recall" class="headerlink" title="3.Recall"></a>3.Recall</h4><h4 id="3-5-F1-Score-F2-Score"><a href="#3-5-F1-Score-F2-Score" class="headerlink" title="3.5 F1 Score, F2 Score"></a>3.5 F1 Score, F2 Score</h4><h4 id="4-AUC"><a href="#4-AUC" class="headerlink" title="4. AUC"></a>4. AUC</h4><h3 id="判断是不是垃圾邮件，是的话程序输出1，不是输出0"><a href="#判断是不是垃圾邮件，是的话程序输出1，不是输出0" class="headerlink" title="判断是不是垃圾邮件，是的话程序输出1，不是输出0"></a>判断是不是垃圾邮件，是的话程序输出1，不是输出0</h3><h3 id="给了十个数据，这10个数据的真实情况是："><a href="#给了十个数据，这10个数据的真实情况是：" class="headerlink" title="给了十个数据，这10个数据的真实情况是："></a>给了十个数据，这10个数据的真实情况是：</h3><h3 id="gt-1-1-1-1-0-1-0-0-1-1"><a href="#gt-1-1-1-1-0-1-0-0-1-1" class="headerlink" title="-&gt;[1,1,1,1,0,1,0,0,1,1]"></a>-&gt;[1,1,1,1,0,1,0,0,1,1]</h3><h3 id="F-x"><a href="#F-x" class="headerlink" title="F(x)"></a>F(x)</h3><h3 id="gt-1-1-1-1-1-1-1-1-0-1"><a href="#gt-1-1-1-1-1-1-1-1-0-1" class="headerlink" title="-&gt;[1,1,1,1,1,1,1,1,0,1]"></a>-&gt;[1,1,1,1,1,1,1,1,0,1]</h3><h3 id="Accuracy-预测的标签中预测正确的值的个数-总的预测的个数"><a href="#Accuracy-预测的标签中预测正确的值的个数-总的预测的个数" class="headerlink" title="Accuracy:预测的标签中预测正确的值的个数/总的预测的个数"></a>Accuracy:预测的标签中预测正确的值的个数/总的预测的个数</h3><p>—&gt; 6/10</p>
<h3 id="Precision-所有说“是”的预测而且预测正确-所有说“是”的预测个数"><a href="#Precision-所有说“是”的预测而且预测正确-所有说“是”的预测个数" class="headerlink" title="Precision: 所有说“是”的预测而且预测正确/所有说“是”的预测个数"></a>Precision: 所有说“是”的预测而且预测正确/所有说“是”的预测个数</h3><p>—&gt; 6/9</p>
<h3 id="Recall-所有说“是”的预测而且预测正确-所有真正标签是“是”"><a href="#Recall-所有说“是”的预测而且预测正确-所有真正标签是“是”" class="headerlink" title="Recall:所有说“是”的预测而且预测正确 / 所有真正标签是“是”"></a>Recall:所有说“是”的预测而且预测正确 / 所有真正标签是“是”</h3><p>—&gt; 6/7</p>
<h3 id="Recall-和-Precision-其实往往是互相-tradeoff"><a href="#Recall-和-Precision-其实往往是互相-tradeoff" class="headerlink" title="Recall 和 Precision 其实往往是互相 tradeoff"></a>Recall 和 Precision 其实往往是互相 tradeoff</h3><h3 id="F1-Score-frac-2precisionrecall-precision-recall"><a href="#F1-Score-frac-2precisionrecall-precision-recall" class="headerlink" title="F1 Score = $ \frac{2precisionrecall}{precision + recall} $"></a>F1 Score = $ \frac{2<em>precision</em>recall}{precision + recall} $</h3><h3 id="—-gt-AOC-AUC"><a href="#—-gt-AOC-AUC" class="headerlink" title="—&gt;AOC / AUC"></a>—&gt;AOC / AUC</h3><h3 id="到这一步就无法分割了"><a href="#到这一步就无法分割了" class="headerlink" title="到这一步就无法分割了"></a>到这一步就无法分割了</h3><h3 id="A-simple-example-of-kmeans"><a href="#A-simple-example-of-kmeans" class="headerlink" title="A simple example of kmeans"></a>A simple example of kmeans</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X1 = [random.randint(<span class="number">0</span>,<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">X2 = [random.randint(<span class="number">0</span>,<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X1,X2)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x21e43481048&gt;
</code></pre><p><img src="output_87_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_data = [[x1,x2] <span class="keyword">for</span> x1,x2 <span class="keyword">in</span> zip(X1,X2)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster = KMeans(n_clusters=<span class="number">6</span>,max_iter=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.fit(training_data)</span><br></pre></td></tr></table></figure>
<pre><code>KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=500,
       n_clusters=6, n_init=10, n_jobs=None, precompute_distances=&#39;auto&#39;,
       random_state=None, tol=0.0001, verbose=0)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.cluster_centers_</span><br></pre></td></tr></table></figure>
<pre><code>array([[85.5       , 41.78571429],
       [27.57894737, 18.89473684],
       [79.90909091, 84.22727273],
       [14.6       , 76.6       ],
       [51.06666667, 65.33333333],
       [62.73333333, 22.13333333]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cluster.labels_</span><br></pre></td></tr></table></figure>
<pre><code>array([1, 3, 2, 5, 0, 1, 5, 0, 1, 2, 4, 3, 3, 4, 4, 0, 1, 0, 5, 0, 1, 4,
       1, 3, 4, 3, 5, 1, 1, 4, 2, 2, 3, 4, 2, 2, 5, 3, 2, 1, 4, 0, 3, 5,
       2, 4, 3, 2, 1, 3, 4, 1, 2, 5, 0, 5, 5, 0, 0, 2, 2, 1, 2, 2, 0, 0,
       4, 1, 5, 3, 5, 0, 2, 5, 3, 3, 5, 5, 5, 2, 1, 2, 2, 4, 1, 2, 4, 4,
       2, 1, 0, 1, 2, 3, 1, 2, 0, 1, 3, 4])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">centers = defaultdict(list)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> label, location <span class="keyword">in</span> zip(cluster.labels_,training_data):</span><br><span class="line">    centers[label].append(location)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">color = [<span class="string">'red'</span>,<span class="string">'green'</span>,<span class="string">'grey'</span>,<span class="string">'black'</span>,<span class="string">'yellow'</span>,<span class="string">'orange'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,c <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">    <span class="keyword">for</span> location <span class="keyword">in</span> centers[c]:</span><br><span class="line">        plt.scatter(*location,c=color[i])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> center <span class="keyword">in</span> cluster.cluster_centers_:</span><br><span class="line">    plt.scatter(*center,s=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="output_96_0.png" alt="png"></p>
<h3 id="Kmeans-的计算复杂度"><a href="#Kmeans-的计算复杂度" class="headerlink" title="Kmeans 的计算复杂度"></a>Kmeans 的计算复杂度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">()</span>:</span> <span class="keyword">return</span> np.sqrt((x1 - x2)**<span class="number">2</span> +(y1 - y2)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">O(I*N*k*d)</script><p>N:10000 k:100 d:500 I:500 -&gt;10**(5+2+2+2)=&gt;10^11 ==&gt;100亿</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>AI for NLP</title>
    <url>/2020/01/16/AI%20for%20NLP/</url>
    <content><![CDATA[<h2 id="Build-Graph"><a href="#Build-Graph" class="headerlink" title="Build Graph"></a>Build Graph</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">coordination_source = <span class="string">"""</span></span><br><span class="line"><span class="string">&#123;name:'兰州', geoCoord:[103.73, 36.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'嘉峪关', geoCoord:[98.17, 39.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'西宁', geoCoord:[101.74, 36.56]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'成都', geoCoord:[104.06, 30.67]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'石家庄', geoCoord:[114.48, 38.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'拉萨', geoCoord:[102.73, 25.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'贵阳', geoCoord:[106.71, 26.57]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'武汉', geoCoord:[114.31, 30.52]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'郑州', geoCoord:[113.65, 34.76]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'济南', geoCoord:[117, 36.65]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南京', geoCoord:[118.78, 32.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'合肥', geoCoord:[117.27, 31.86]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'杭州', geoCoord:[120.19, 30.26]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南昌', geoCoord:[115.89, 28.68]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'福州', geoCoord:[119.3, 26.08]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'广州', geoCoord:[113.23, 23.16]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'长沙', geoCoord:[113, 28.21]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'海口', geoCoord:[110.35, 20.02]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'沈阳', geoCoord:[123.38, 41.8]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'长春', geoCoord:[125.35, 43.88]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'哈尔滨', geoCoord:[126.63, 45.75]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'太原', geoCoord:[112.53, 37.87]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'西安', geoCoord:[108.95, 34.27]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'台湾', geoCoord:[121.30, 25.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'北京', geoCoord:[116.46, 39.92]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'上海', geoCoord:[121.48, 31.22]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'重庆', geoCoord:[106.54, 29.59]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'天津', geoCoord:[117.2, 39.13]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'呼和浩特', geoCoord:[111.65, 40.82]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'南宁', geoCoord:[108.33, 22.84]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:'西藏', geoCoord:[91.11, 29.97]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'银川', geoCoord:[106.27, 38.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'乌鲁木齐', geoCoord:[87.68, 43.77]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'香港', geoCoord:[114.17, 22.28]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:'澳门', geoCoord:[113.54, 22.19]&#125;</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">re.findall(<span class="string">"[\d\.]+"</span>,<span class="string">"&#123;name:'澳门', geoCoord:[113.54, 22.19]&#125;"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;113.54&#39;, &#39;22.19&#39;]
</code></pre><h3 id="Get-data-from-source-using-regular-expression"><a href="#Get-data-from-source-using-regular-expression" class="headerlink" title="Get data from source using regular expression"></a>Get data from source using regular expression</h3><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure>
<h2 id="regular-expression"><a href="#regular-expression" class="headerlink" title="regular expression"></a>regular expression</h2><p>[a-z][A-Z]<sup><a href="#fn_a" id="reffn_a">a</a></sup>: negation colou?r: ? zero or onr of its previous character<br>colou?r: ? zero or onr of its previous character</p>
<ul>
<li><p>: zero or more of its previous character +: one or more.:match any single character  </p>
</li>
<li><p>: ^: start of the line </p>
</li>
<li>: $:end of the line</li>
<li>: | [cat|dog] : cat or dog</li>
<li>(da): make the string da like a character</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = <span class="string">"color or colour"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.compile(<span class="string">"colou?r"</span>)</span><br><span class="line">pattern.findall(l)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;color&#39;, &#39;colour&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_city_info</span><span class="params">(city_coordination)</span>:</span></span><br><span class="line">    city_location = &#123;&#125;</span><br><span class="line">    first = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> city_coordination.split(<span class="string">'\n'</span>):</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">'//'</span>): <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.strip()==<span class="string">""</span>:<span class="keyword">continue</span>    </span><br><span class="line">        city =re.findall(<span class="string">"name:'(\w+)'"</span>,line)[<span class="number">0</span>]</span><br><span class="line">        x_y = re.findall(<span class="string">"Coord:\[(\d+.\d+),\s(\d+.\d+)\]"</span>,line)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> first ==<span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"x_y: "</span>,x_y)</span><br><span class="line">        x_y = tuple(map(float,x_y))</span><br><span class="line">        city_location[city] = x_y</span><br><span class="line">        <span class="keyword">if</span> first ==<span class="literal">True</span>:</span><br><span class="line">            print(<span class="string">"city:"</span>,city )</span><br><span class="line">            print(<span class="string">"x_y: "</span>,x_y)</span><br><span class="line">            first =<span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> city_location</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_info = get_city_info(coordination_source)</span><br></pre></td></tr></table></figure>
<pre><code>x_y:  (&#39;103.73&#39;, &#39;36.03&#39;)
city: 兰州
x_y:  (103.73, 36.03)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_info</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;兰州&#39;: (103.73, 36.03),
 &#39;嘉峪关&#39;: (98.17, 39.47),
 &#39;西宁&#39;: (101.74, 36.56),
 &#39;成都&#39;: (104.06, 30.67),
 &#39;石家庄&#39;: (114.48, 38.03),
 &#39;拉萨&#39;: (102.73, 25.04),
 &#39;贵阳&#39;: (106.71, 26.57),
 &#39;武汉&#39;: (114.31, 30.52),
 &#39;郑州&#39;: (113.65, 34.76),
 &#39;济南&#39;: (117.0, 36.65),
 &#39;南京&#39;: (118.78, 32.04),
 &#39;合肥&#39;: (117.27, 31.86),
 &#39;杭州&#39;: (120.19, 30.26),
 &#39;南昌&#39;: (115.89, 28.68),
 &#39;福州&#39;: (119.3, 26.08),
 &#39;广州&#39;: (113.23, 23.16),
 &#39;长沙&#39;: (113.0, 28.21),
 &#39;沈阳&#39;: (123.38, 41.8),
 &#39;长春&#39;: (125.35, 43.88),
 &#39;哈尔滨&#39;: (126.63, 45.75),
 &#39;太原&#39;: (112.53, 37.87),
 &#39;西安&#39;: (108.95, 34.27),
 &#39;北京&#39;: (116.46, 39.92),
 &#39;上海&#39;: (121.48, 31.22),
 &#39;重庆&#39;: (106.54, 29.59),
 &#39;天津&#39;: (117.2, 39.13),
 &#39;呼和浩特&#39;: (111.65, 40.82),
 &#39;南宁&#39;: (108.33, 22.84),
 &#39;银川&#39;: (106.27, 38.47),
 &#39;乌鲁木齐&#39;: (87.68, 43.77),
 &#39;香港&#39;: (114.17, 22.28),
 &#39;澳门&#39;: (113.54, 22.19)}
</code></pre><h3 id="Compute-distance-between-cities"><a href="#Compute-distance-between-cities" class="headerlink" title="Compute distance between cities"></a>Compute distance between cities</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">geo_distance</span><span class="params">(origin, destination)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate the Haversine distance.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    origin: tuple of float</span></span><br><span class="line"><span class="string">        (lat,long)</span></span><br><span class="line"><span class="string">    destination: tuple of float</span></span><br><span class="line"><span class="string">        (lat,long)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    distance_in_km: float</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; origin = (48,1372,11.5756) #Munich</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; destination = (52.5186,13.4083) #Berlin</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; round(distance(origin,destination),1)</span></span><br><span class="line"><span class="string">    504.2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    lat1,lon1 = origin</span><br><span class="line">    lat2,lon2 = destination</span><br><span class="line">    radius =<span class="number">6371</span> <span class="comment"># km</span></span><br><span class="line">    </span><br><span class="line">    dlat = math.radians(lat2 - lat1)</span><br><span class="line">    dlon = math.radians(lon2 - lon1)</span><br><span class="line">    a = (math.sin(dlat / <span class="number">2</span>) * math.sin(dlat / <span class="number">2</span>) +</span><br><span class="line">         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *</span><br><span class="line">         math.sin(dlon / <span class="number">2</span>) * math.sin(dlon / <span class="number">2</span>))</span><br><span class="line">    c = <span class="number">2</span> * math.atan2(math.sqrt(a), math.sqrt(<span class="number">1</span> - a))</span><br><span class="line">    d = radius * c</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_city_distance</span><span class="params">(city1,city2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> geo_distance(city_info[city1],city_info[city2])</span><br><span class="line"></span><br><span class="line">get_city_distance(<span class="string">"上海"</span>,<span class="string">"北京"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>727.52769688981
</code></pre><h3 id="Draw-the-graph"><a href="#Draw-the-graph" class="headerlink" title="Draw the graph"></a>Draw the graph</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_graph =nx.Graph()</span><br><span class="line">city_graph.add_nodes_from(list(city_info.keys()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.draw(city_graph,city_info,with_labels=<span class="literal">True</span>,node_size=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/output_18_0.png" alt="png"></p>
<h3 id="Build-connection-between-Let’s-assume-that-two-cities-are-connected-if-their-distance-is-less-than-700km"><a href="#Build-connection-between-Let’s-assume-that-two-cities-are-connected-if-their-distance-is-less-than-700km" class="headerlink" title="Build connection between. Let’s assume that two cities are connected if their distance is less than 700km"></a>Build connection between. Let’s assume that two cities are connected if their distance is less than 700km</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">threshold = <span class="number">700</span> <span class="comment"># defined the threshold</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_connection</span><span class="params">(city_info)</span>:</span></span><br><span class="line">    cities_connection = defaultdict(list)</span><br><span class="line">    cities = list(city_info.keys())</span><br><span class="line">    <span class="keyword">for</span> c1 <span class="keyword">in</span> cities:</span><br><span class="line">        <span class="keyword">for</span> c2 <span class="keyword">in</span> cities:</span><br><span class="line">            <span class="keyword">if</span> c1 == c2 :</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> get_city_distance(c1,c2) &lt; threshold:</span><br><span class="line">                cities_connection[c1].append(c2)</span><br><span class="line">    <span class="keyword">return</span> cities_connection</span><br><span class="line"></span><br><span class="line">cities_connection = build_connection(city_info)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cities_connection</span><br></pre></td></tr></table></figure>
<pre><code>defaultdict(list,
            {&#39;兰州&#39;: [&#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;嘉峪关&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;],
             &#39;西宁&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;重庆&#39;, &#39;银川&#39;],
             &#39;成都&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;石家庄&#39;: [&#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;拉萨&#39;: [&#39;兰州&#39;, &#39;嘉峪关&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;贵阳&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;贵阳&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;西安&#39;, &#39;重庆&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;武汉&#39;: [&#39;石家庄&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;郑州&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;济南&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;南京&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;],
             &#39;合肥&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;杭州&#39;: [&#39;武汉&#39;, &#39;济南&#39;, &#39;南京&#39;, &#39;合肥&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;北京&#39;, &#39;上海&#39;, &#39;天津&#39;],
             &#39;南昌&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;天津&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;福州&#39;: [&#39;武汉&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;上海&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;广州&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;长沙&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;沈阳&#39;: [&#39;长春&#39;, &#39;哈尔滨&#39;, &#39;上海&#39;],
             &#39;长春&#39;: [&#39;沈阳&#39;, &#39;哈尔滨&#39;],
             &#39;哈尔滨&#39;: [&#39;沈阳&#39;, &#39;长春&#39;],
             &#39;太原&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;,
              &#39;银川&#39;,
              &#39;澳门&#39;],
             &#39;西安&#39;: [&#39;兰州&#39;,
              &#39;成都&#39;,
              &#39;石家庄&#39;,
              &#39;贵阳&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;重庆&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;,
              &#39;银川&#39;],
             &#39;北京&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;天津&#39;,
              &#39;呼和浩特&#39;],
             &#39;上海&#39;: [&#39;济南&#39;, &#39;南京&#39;, &#39;合肥&#39;, &#39;杭州&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;沈阳&#39;, &#39;天津&#39;],
             &#39;重庆&#39;: [&#39;兰州&#39;, &#39;西宁&#39;, &#39;成都&#39;, &#39;拉萨&#39;, &#39;贵阳&#39;, &#39;西安&#39;, &#39;呼和浩特&#39;, &#39;南宁&#39;, &#39;银川&#39;],
             &#39;天津&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;南京&#39;,
              &#39;合肥&#39;,
              &#39;杭州&#39;,
              &#39;南昌&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;北京&#39;,
              &#39;上海&#39;,
              &#39;呼和浩特&#39;],
             &#39;呼和浩特&#39;: [&#39;石家庄&#39;,
              &#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;济南&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;北京&#39;,
              &#39;重庆&#39;,
              &#39;天津&#39;,
              &#39;银川&#39;],
             &#39;南宁&#39;: [&#39;兰州&#39;,
              &#39;成都&#39;,
              &#39;拉萨&#39;,
              &#39;贵阳&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;西安&#39;,
              &#39;重庆&#39;,
              &#39;银川&#39;,
              &#39;香港&#39;,
              &#39;澳门&#39;],
             &#39;银川&#39;: [&#39;兰州&#39;,
              &#39;西宁&#39;,
              &#39;成都&#39;,
              &#39;拉萨&#39;,
              &#39;贵阳&#39;,
              &#39;太原&#39;,
              &#39;西安&#39;,
              &#39;重庆&#39;,
              &#39;呼和浩特&#39;,
              &#39;南宁&#39;],
             &#39;香港&#39;: [&#39;武汉&#39;, &#39;郑州&#39;, &#39;合肥&#39;, &#39;南昌&#39;, &#39;福州&#39;, &#39;广州&#39;, &#39;长沙&#39;, &#39;南宁&#39;, &#39;澳门&#39;],
             &#39;澳门&#39;: [&#39;武汉&#39;,
              &#39;郑州&#39;,
              &#39;合肥&#39;,
              &#39;南昌&#39;,
              &#39;福州&#39;,
              &#39;广州&#39;,
              &#39;长沙&#39;,
              &#39;太原&#39;,
              &#39;南宁&#39;,
              &#39;香港&#39;]})
</code></pre><h3 id="Draw-connection-graph"><a href="#Draw-connection-graph" class="headerlink" title="Draw connection graph"></a>Draw connection graph</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cities_connection_graph = nx.Graph(cities_connection)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nx.draw(cities_connection_graph,city_info,with_labels = <span class="literal">True</span>, node_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/output_26_0.png" alt="png"></p>
<h3 id="BFS-1-version"><a href="#BFS-1-version" class="headerlink" title="BFS 1 version"></a>BFS 1 version</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_1</span><span class="params">(graph, start, destination)</span>:</span></span><br><span class="line">    pathes = [[start]] <span class="comment"># list 用来存储待搜索路径</span></span><br><span class="line">    visited = set() <span class="comment">#set 用来存车已搜索节点</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> pathes:</span><br><span class="line">        path= pathes.pop(<span class="number">0</span>) <span class="comment">#提取第一条路径</span></span><br><span class="line">        froniter = path[<span class="number">-1</span>] <span class="comment">#提取即将要探索的节点</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> froniter <span class="keyword">in</span> visited: <span class="keyword">continue</span> <span class="comment">#检查如果该点已经探索过，则不用探索</span></span><br><span class="line">        </span><br><span class="line">        successors = graph[froniter]</span><br><span class="line">        <span class="keyword">for</span> city <span class="keyword">in</span> successors: <span class="comment"># 遍历子节点</span></span><br><span class="line">            <span class="keyword">if</span> city <span class="keyword">in</span> path: <span class="keyword">continue</span> <span class="comment"># 检查会不会形成环</span></span><br><span class="line">            </span><br><span class="line">            new_path = path +[city]</span><br><span class="line">            pathes.append(new_path) <span class="comment">#bfs #将新路径加到list里面</span></span><br><span class="line">            <span class="comment"># pathes = [new_path] + pathes #dfs</span></span><br><span class="line">            <span class="keyword">if</span> city ==destination: <span class="comment">#检查是否到达目的地</span></span><br><span class="line">                <span class="keyword">return</span> new_path</span><br><span class="line">        visited.add(froniter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_1(cities_connection,<span class="string">"上海"</span>,<span class="string">"石家庄"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;上海&#39;, &#39;济南&#39;, &#39;石家庄&#39;]
</code></pre><h3 id="Optimal-search-using-variation-of-BFS"><a href="#Optimal-search-using-variation-of-BFS" class="headerlink" title="Optimal search using variation of BFS"></a>Optimal search using variation of BFS</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_2</span><span class="params">(graph,start,destination,search_strategy)</span>:</span></span><br><span class="line">    pathes = [[start]]</span><br><span class="line">    visited =set() <span class="comment"># !</span></span><br><span class="line">    <span class="keyword">while</span> pathes:</span><br><span class="line">        path = pathes.pop(<span class="number">0</span>)</span><br><span class="line">        froniter =path[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> froniter <span class="keyword">in</span> visited:<span class="keyword">continue</span> <span class="comment"># !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> froniter == destination:</span><br><span class="line">            <span class="keyword">return</span> path</span><br><span class="line">        successors = graph[froniter]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> city <span class="keyword">in</span> successors:</span><br><span class="line">            <span class="keyword">if</span> city <span class="keyword">in</span> path:<span class="keyword">continue</span> <span class="comment">#check loop</span></span><br><span class="line">            </span><br><span class="line">            new_path = path + [city]</span><br><span class="line">            pathes.append(new_path)</span><br><span class="line">        </span><br><span class="line">        pathes =search_strategy(pathes)</span><br><span class="line">        visited.add(froniter) <span class="comment"># !</span></span><br><span class="line">        <span class="comment">#</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sort_by_distance</span><span class="params">(pathes)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_distance_of_path</span><span class="params">(path)</span>:</span></span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(path[:<span class="number">-1</span>]):</span><br><span class="line">            distance += get_city_distance(path[i],path[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> distance</span><br><span class="line">    <span class="keyword">return</span> sorted(pathes,key =get_distance_of_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distance_of_path</span><span class="params">(path)</span>:</span></span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,_ <span class="keyword">in</span> enumerate(path[:<span class="number">-1</span>]):</span><br><span class="line">            distance += get_city_distance(path[i],path[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> distance</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_distance_of_path([<span class="string">"北京"</span>,<span class="string">"济南"</span>,<span class="string">"上海"</span>])</span><br></pre></td></tr></table></figure>
<pre><code>752.66259009181
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_2(cities_connection,<span class="string">"北京"</span>,<span class="string">"上海"</span>,search_strategy=<span class="keyword">lambda</span> x:x)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;北京&#39;, &#39;济南&#39;, &#39;上海&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_2(cities_connection,<span class="string">"北京"</span>,<span class="string">"上海"</span>,search_strategy=sort_by_distance)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;北京&#39;, &#39;天津&#39;, &#39;上海&#39;]
</code></pre><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = load_boston()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dataset</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y=dataset[<span class="string">'data'</span>],dataset[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>
<pre><code>(506, 13)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure>
<pre><code>(506,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>(13,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,
       6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,
       1.7800e+01, 3.9690e+02, 9.1400e+00])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset.feature_names</span><br></pre></td></tr></table></figure>
<pre><code>array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;,
       &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[<span class="string">'DESCR'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&quot;.. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000&#39;s\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic\nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics\n...&#39;, Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n&quot;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_rm = x[:,<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plot the RM with respect to y</span></span><br><span class="line">plt.scatter(X_rm,y)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x11955a9f148&gt;
</code></pre><p><img src="/images/output_49_1.png" alt="png"></p>
<h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><h3 id="Assume-that-the-target-function-is-a-linear-function"><a href="#Assume-that-the-target-function-is-a-linear-function" class="headerlink" title="Assume that the target function is a linear function"></a>Assume that the target function is a linear function</h3><script type="math/tex; mode=display">y = k*rm +b</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define target function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(rm,k,b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> k*rm + b</span><br></pre></td></tr></table></figure>
<h3 id="Define-mean-square-loss"><a href="#Define-mean-square-loss" class="headerlink" title="Define mean square loss"></a>Define mean square loss</h3><script type="math/tex; mode=display">loss = \frac{1}{n} \sum{(y_i - \hat{y_i})}^2</script><script type="math/tex; mode=display">loss = \frac{1}{n}\sum{(y_i) - (kx_i + b_i))}^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum((y_i - y_hat_i)**<span class="number">2</span> <span class="keyword">for</span> y_i,y_hat_i <span class="keyword">in</span> zip(list(y),list(y_hat)))/len(list(y))</span><br></pre></td></tr></table></figure>
<h3 id="Define-partial-derivatives"><a href="#Define-partial-derivatives" class="headerlink" title="Define partial derivatives"></a>Define partial derivatives</h3><script type="math/tex; mode=display">\frac {\partial{loss}}{\partial{k}} = -\frac{2}{n}\sum(y_i -\hat{y_i})x_i</script><script type="math/tex; mode=display">\frac{\partial{loss}}{\partial{b}} = -\frac{2}{n}\sum(y_i - \hat{y_i})</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#define partial derivative</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_derivative_k</span><span class="params">(x,y,y_hat)</span>:</span></span><br><span class="line">    n = len(y)</span><br><span class="line">    gradient = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_i,y_i,y_hat_i <span class="keyword">in</span> zip(list(x),list(y),list(y_hat)):</span><br><span class="line">        gradient += (y_i - y_hat_i) * x_i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>/n * gradient</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_derivative_b</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    n =len(y)</span><br><span class="line">    gradient = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y_i,y_hat_i <span class="keyword">in</span> zip(list(y),list(y_hat)):</span><br><span class="line">        gradient+= (y_i-y_hat_i)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>/n* gradient</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#initialized parameters</span></span><br><span class="line">k= random.random()*<span class="number">200</span><span class="number">-100</span> <span class="comment"># -100 100</span></span><br><span class="line">b= random.random()*<span class="number">200</span><span class="number">-100</span> <span class="comment"># -100 100</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">iteration_num =<span class="number">200</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iteration_num):</span><br><span class="line">    price_use_current_parameters = [price(r,k,b) <span class="keyword">for</span> r <span class="keyword">in</span> X_rm] <span class="comment"># \hat&#123;y&#125;</span></span><br><span class="line">    </span><br><span class="line">    current_loss = loss(y,price_use_current_parameters)</span><br><span class="line">    losses.append(current_loss)</span><br><span class="line">    <span class="comment">#print("Iteration &#123;&#125;,the loss is &#123;&#125;,parameters k is &#123;&#125; and b is &#123;&#125;".format(i,current_loss,k,b))</span></span><br><span class="line">    </span><br><span class="line">    k_gradient = partial_derivative_k(X_rm,y,price_use_current_parameters)</span><br><span class="line">    b_gradient = partial_derivative_b(y,price_use_current_parameters)</span><br><span class="line">    </span><br><span class="line">    k = k + (<span class="number">-1</span> * k_gradient) * learning_rate</span><br><span class="line">    b = b + (<span class="number">-1</span> * b_gradient) * learning_rate</span><br><span class="line"></span><br><span class="line">best_k =k </span><br><span class="line">best_b =b</span><br><span class="line">print(<span class="string">"best_k is&#123;&#125;,best_b is &#123;&#125;"</span>.format(best_k,best_b))</span><br></pre></td></tr></table></figure>
<pre><code>best_k is12.41253129958806,best_b is -55.72859329657179
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(list(range(iteration_num)),losses)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x119550b6fc8&gt;]
</code></pre><p><img src="/images/output_62_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">price_use_best_paramters = [price(r,best_k,best_b) <span class="keyword">for</span> r <span class="keyword">in</span> X_rm]</span><br><span class="line"></span><br><span class="line">plt.scatter(X_rm,y)</span><br><span class="line">plt.scatter(X_rm,price_use_current_parameters)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x11955a2d188&gt;
</code></pre><p><img src="/images/output_63_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Syntax-Tree</title>
    <url>/2020/01/11/Syntax-Tree/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simple_grammar =<span class="string">"""</span></span><br><span class="line"><span class="string">sentence =&gt; noun_phrase verb_phrase</span></span><br><span class="line"><span class="string">noun_phrase =&gt; Article Adj* noun</span></span><br><span class="line"><span class="string">Adj* =&gt; null | Adj Adj*</span></span><br><span class="line"><span class="string">verb_phrase =&gt; verb noun_phrase</span></span><br><span class="line"><span class="string">Article =&gt; 一个 | 这个</span></span><br><span class="line"><span class="string">noun =&gt; 女人 | 篮球 | 桌子 | 小猫</span></span><br><span class="line"><span class="string">verb =&gt; 看着 | 坐着 | 听见 | 看着</span></span><br><span class="line"><span class="string">Adj =&gt; 蓝色的 | 好看的 | 小小的</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">another_grammar = <span class="string">"""</span></span><br><span class="line"><span class="string">#</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adj</span><span class="params">()</span>:</span><span class="keyword">return</span> random.choice(<span class="string">'蓝色的|好看的|小小的'</span>.split(<span class="string">'|'</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adj_star</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> random.choice([<span class="keyword">lambda</span> : <span class="string">''</span>,<span class="keyword">lambda</span> : adj()+adj_star()])()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj_star()</span><br></pre></td></tr></table></figure>
<pre><code>&#39;蓝色的好看的小小的好看的小小的蓝色的&#39;
</code></pre><a id="more"></a>
<h1 id="but-the-question-is"><a href="#but-the-question-is" class="headerlink" title="but the question is ?"></a>but the question is ?</h1><p>如果我们更换了语法，会发现所有写过的过程都需要重写</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj_grammar =<span class="string">"""</span></span><br><span class="line"><span class="string">Adj* =&gt; null | Adj Adj*</span></span><br><span class="line"><span class="string">Adj =&gt; 蓝色的 | 好看的 | 小小的 </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_grammar</span><span class="params">(grammar_str,split = <span class="string">'=&gt;'</span>,line_split=<span class="string">'\n'</span>)</span>:</span></span><br><span class="line">    grammar = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> grammar_str.split(line_split):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.strip(): <span class="keyword">continue</span></span><br><span class="line">        exp,stmt = line.split(split)</span><br><span class="line">        grammar[exp.strip()]=[s.split() <span class="keyword">for</span> s <span class="keyword">in</span> stmt.split(<span class="string">'|'</span>)]</span><br><span class="line">    <span class="keyword">return</span> grammar</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grammar=create_grammar(adj_grammar)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grammar[<span class="string">'Adj*'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[[&#39;null&#39;], [&#39;Adj&#39;, &#39;Adj*&#39;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">choice = random.choice</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(gram,target)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> target <span class="keyword">not</span> <span class="keyword">in</span> gram: <span class="keyword">return</span> target <span class="comment">#means target is terminal expression</span></span><br><span class="line">    </span><br><span class="line">    expand = [generate(gram,t) <span class="keyword">for</span> t <span class="keyword">in</span> choice(gram[target])]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([e <span class="keyword">if</span> e != <span class="string">'/n'</span> <span class="keyword">else</span> <span class="string">'\n'</span> <span class="keyword">for</span> e <span class="keyword">in</span> expand <span class="keyword">if</span> e != <span class="string">'null'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_grammar =create_grammar(simple_grammar)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_grammar</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;sentence&#39;: [[&#39;noun_phrase&#39;, &#39;verb_phrase&#39;]],
 &#39;noun_phrase&#39;: [[&#39;Article&#39;, &#39;Adj*&#39;, &#39;noun&#39;]],
 &#39;Adj*&#39;: [[&#39;null&#39;], [&#39;Adj&#39;, &#39;Adj*&#39;]],
 &#39;verb_phrase&#39;: [[&#39;verb&#39;, &#39;noun_phrase&#39;]],
 &#39;Article&#39;: [[&#39;一个&#39;], [&#39;这个&#39;]],
 &#39;noun&#39;: [[&#39;女人&#39;], [&#39;篮球&#39;], [&#39;桌子&#39;], [&#39;小猫&#39;]],
 &#39;verb&#39;: [[&#39;看着&#39;], [&#39;坐着&#39;], [&#39;听见&#39;], [&#39;看着&#39;]],
 &#39;Adj&#39;: [[&#39;蓝色的&#39;], [&#39;好看的&#39;], [&#39;小小的&#39;]]}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generate(gram=example_grammar,target=<span class="string">'sentence'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;这个女人看着一个蓝色的蓝色的好看的桌子&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在西部世界里，一个 “人类”的语言可以定义为：</span></span><br><span class="line"></span><br><span class="line">human = <span class="string">"""</span></span><br><span class="line"><span class="string">human = 自己 寻找 活动</span></span><br><span class="line"><span class="string">自己 = 我 | 俺 | 我们</span></span><br><span class="line"><span class="string">寻找 = 找找 | 想找点</span></span><br><span class="line"><span class="string">活动 = 乐子 | 玩的</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#一个接待员的语言可以定义为</span></span><br><span class="line"></span><br><span class="line">host = <span class="string">"""</span></span><br><span class="line"><span class="string">host = 寒暄 报数 询问 业务相关 结尾</span></span><br><span class="line"><span class="string">报数 = 我是 数字 号 ,</span></span><br><span class="line"><span class="string">数字 = 单个数字 | 数字 单个数字</span></span><br><span class="line"><span class="string">单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9</span></span><br><span class="line"><span class="string">寒暄 = 称谓 打招呼 | 打招呼</span></span><br><span class="line"><span class="string">称谓 = 人称 ，</span></span><br><span class="line"><span class="string">人称 = tb | 神仙 | 傻狗</span></span><br><span class="line"><span class="string">打招呼 = 你好 | 您好</span></span><br><span class="line"><span class="string">询问 = 请问你要 | 您需要</span></span><br><span class="line"><span class="string">业务相关 = 玩玩 具体业务</span></span><br><span class="line"><span class="string">玩玩 = null</span></span><br><span class="line"><span class="string">具体业务 = 喝酒 | 打牌 | 打猎 | 赌博 | 踢球 | 飞翔</span></span><br><span class="line"><span class="string">结尾 = 吗？</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    print(generate(create_grammar(human,<span class="string">'='</span>),<span class="string">'human'</span>))</span><br><span class="line">    print(generate(create_grammar(host,<span class="string">'='</span>),<span class="string">'host'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>我想找点玩的
神仙，您好我是5号,您需要打牌吗？
我们想找点玩的
傻狗，你好我是4号,请问你要踢球吗？
俺想找点玩的
你好我是6号,请问你要打猎吗？
俺找找玩的
你好我是7723号,请问你要打猎吗？
我们想找点玩的
神仙，你好我是2号,您需要打牌吗？
俺找找玩的
神仙，你好我是66号,您需要打牌吗？
我找找玩的
傻狗，您好我是21号,您需要打牌吗？
我们想找点玩的
你好我是6号,您需要飞翔吗？
我们找找玩的
您好我是5号,请问你要打牌吗？
我找找玩的
您好我是9号,请问你要喝酒吗？
俺想找点乐子
你好我是6号,请问你要飞翔吗？
我们想找点玩的
你好我是8号,请问你要飞翔吗？
我找找乐子
您好我是99号,您需要踢球吗？
我想找点乐子
神仙，你好我是95号,您需要飞翔吗？
俺找找玩的
您好我是8号,请问你要飞翔吗？
俺找找乐子
傻狗，你好我是45号,请问你要喝酒吗？
俺找找乐子
您好我是68号,您需要赌博吗？
我们想找点乐子
神仙，你好我是3号,您需要打猎吗？
我们想找点乐子
佟博，您好我是521号,请问你要打猎吗？
我找找乐子
神仙，您好我是2189号,您需要赌博吗？
</code></pre><h1 id="希望能够生成最合理的一句话？"><a href="#希望能够生成最合理的一句话？" class="headerlink" title="希望能够生成最合理的一句话？"></a>希望能够生成最合理的一句话？</h1><h2 id="Data-Driven"><a href="#Data-Driven" class="headerlink" title="Data Driven"></a>Data Driven</h2><p>我们的目标是，希望能够在做一个程序，然后，当输入的数据变化的时候，我们的程序不用重写。Generalizatoin.<br>AI? 如何能够自动化解决巨额问题，我们找到一个方法后，输入变了，我们这个方法，不用变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simple_programming = <span class="string">"""</span></span><br><span class="line"><span class="string">programming =&gt; if_stmt | assign | while_loop</span></span><br><span class="line"><span class="string">while_loop =&gt; while ( cond ) &#123; change_line stmt change_line &#125;</span></span><br><span class="line"><span class="string">if_stmt =&gt; if ( cond ) &#123; change_line stmt change_line &#125; | if ( cond ) &#123; change_line stmt change_line &#125; else &#123; change_line stmt change_line &#125;</span></span><br><span class="line"><span class="string">change_line =&gt; /n</span></span><br><span class="line"><span class="string">cond =&gt; var op var</span></span><br><span class="line"><span class="string">op =&gt; | == | &lt; | &gt;= | &lt;=</span></span><br><span class="line"><span class="string">stmt =&gt; assign | if_stmt</span></span><br><span class="line"><span class="string">assign =&gt; var = var</span></span><br><span class="line"><span class="string">var =&gt; var _ num | words</span></span><br><span class="line"><span class="string">words =&gt; words _ word | word</span></span><br><span class="line"><span class="string">word =&gt; name | info | student | lib | database</span></span><br><span class="line"><span class="string">nums =&gt; nums num | num</span></span><br><span class="line"><span class="string">num =&gt; 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(generate(gram=create_grammar(simple_programming,split=<span class="string">'=&gt;'</span>),target=<span class="string">'programming'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>if(lib_name_info&gt;=lib_student){
if(name_2&lt;=name_database){
if(lib&lt;info_lib_database_7){
database=lib_lib
}else{
info_6=info
}
}
}else{
info_0=name_5
}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_print</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="comment">#utility ttool function</span></span><br><span class="line">    lines = line.split(<span class="string">'\n'</span>)</span><br><span class="line">    </span><br><span class="line">    code_lines = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i , sen <span class="keyword">in</span> enumerate(lines):</span><br><span class="line">        <span class="keyword">if</span> i &lt; len(lines) /<span class="number">2</span>:</span><br><span class="line">            <span class="comment">#print()</span></span><br><span class="line">            code_lines.append(i * <span class="string">"  "</span> + sen)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code_lines.append((len(lines) -i )* <span class="string">" "</span> +sen)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> code_lines</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generated_programming = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    generated_programming += pretty_print(generate(gram=create_grammar(simple_programming,split=<span class="string">'=&gt;'</span>),target=<span class="string">'programming'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> generated_programming:</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure>
<pre><code>if(student==name){
  if(database_student&lt;=lib){
    if(student_student_lib_databasename_name){
      info=info
        }else{
          if(info==name){
            if(lib&lt;info_3){
              if(student_lib_lib_9&gt;=database){
       info_6=student_database_lib_lib
      }
     }
    }
   }
  }
 }
while(lib_0&lt;name_name_name_info){
  info_lib_database_name=info
 }
database_info_database_3=lib_7_5_0_8_5_7_3
database_0=info
while(lib_1_9&lt;=student_lib_student_lib){
  if(student_name&lt;info_database_0){
    if(database&lt;=student_database){
      if(name&gt;=name){
        info_info=student_info_lib_name_0
          }
            }else{
              if(lib_database_5_0==name_2){
       lib_database_database_4=lib_5
      }
     }
    }else{
   info=database_database_database_0
  }
 }
while(lib_database_lib&lt;=student_6){
  if(studentstudent_2){
    name_database_7=lib_name_name_7
      }else{
        if(info_1&gt;=database_4_8){
          info=info_1_3_7
            }else{
              if(name_9_8_9&lt;database){
                if(database_info_lib_3_8database_7){
                  if(name_7&gt;=lib_name_student){
                    if(student_lib_info_info_2_4_8&lt;lib_6_2_2){
                      student_lib_database=student_info_3
           }else{
          info_lib=student_database
         }
        }
       }
      }else{
     database=lib_student_info_name_student_3_1_7_3
    }
   }
  }
 }
if(student&gt;=name_student_database_lib_lib){
  name_database_3_4=lib_name_info_info_name_student_info_name
    }else{
  lib_database=info_lib_student
 }
name=name_5_2_3_2_4_5_3_2_0_3_7
info_6=info_student_1
info=lib_9
if(student&gt;=database_2){
  name_4_5=student
 }
if(student_3_3_6&lt;=name_info_database_8){
  if(student_6&gt;=database_student_4){
    if(info_2_2_7==info_6_4){
      name_8_1=info_student_lib
        }else{
          database_5=database_1
            }
              }
                }else{
                  if(lib==database_lib){
                    lib_0_9_1=lib_0
          }else{
         if(lib_database&gt;=info_student_0){
        student_name_8=name
       }else{
      if(info_name_lib_name_student_database_lib_name&lt;=student_2){
     student_info_0_9_9_1=database
    }
   }
  }
 }
while(lib_3_8&lt;=database_0){
  if(name_9==lib){
    if(lib_name_lib_5info){
      if(lib_name_database_3lib_2){
        database_info_info=student_1
          }
            }
              }else{
       if(name&lt;database_5_1_5_8){
      student_student=info
     }else{
    info_info_7=student_database_database_info_name_1_6
   }
  }
 }
database_database=info_student_database_lib_student
lib=database
student_database_database=name_6
while(student_database&lt;info){
  if(info_student_database_student_lib_name_info&lt;=info){
    lib_4_5_2=name_info
      }else{
   lib_2=database_info_name
  }
 }
while(name_3name_lib){
  info=lib
 }
while(info_libinfo_4_4_3){
  info_name_info_lib_9_9=database_0_4_3
 }
if(lib_database_name&lt;student_name_0){
  if(name_9_4_4_6student_info_student){
    if(name_0lib_1){
      lib_lib=database_student_student
        }
          }else{
            if(infostudent){
              lib_0=student
                }
                  }
                    }else{
                      if(student_name_info_0_9info_lib_9){
                        database_name_student=name
                          }else{
                            if(student&lt;=name){
                              database=info_info_lib_name_lib_8_7
                                }else{
                                  if(name_namedatabase_lib_name_8_6){
                 if(info==name){
                info_7_9_5_1_6=info_database
               }else{
              if(name_student_info_8&lt;=student_database){
             info_name_database_database_database=lib_1
            }else{
           if(name&lt;=info_info_student_student_database_info_lib_lib_info){
          student_lib=name_student
         }
        }
       }
      }else{
     info_6_8_3=info_9_3_7
    }
   }
  }
 }
</code></pre><h1 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h1><script type="math/tex; mode=display">language\_model(string) = Probality(String)\in(0,1)</script><script type="math/tex; mode=display">Pro(w_1 w_2 w_3 w_4) = Pr(w_1 | w_2 w_3 w_4) * Pr(w_2 | w_3 w_4) *Pr(w_3 | w_4)*Pr(w_4)</script><p>how to get $ Pr(w_1 |w_2 w_3 w_4)$?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.choice(range(<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<pre><code>86
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename=<span class="string">'/Users/24768/sqlResult_1558435.csv'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">content =pd.read_csv(filename,encoding=<span class="string">'gb18030'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">content.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>author</th>
      <th>source</th>
      <th>content</th>
      <th>feature</th>
      <th>title</th>
      <th>url</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>89617</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"37"...</td>
      <td>小米MIUI 9首批机型曝光：共计15款</td>
      <td>http://www.cnbeta.com/articles/tech/623597.htm</td>
    </tr>
    <tr>
      <td>1</td>
      <td>89616</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"15"...</td>
      <td>骁龙835在Windows 10上的性能表现有望改善</td>
      <td>http://www.cnbeta.com/articles/tech/623599.htm</td>
    </tr>
    <tr>
      <td>2</td>
      <td>89615</td>
      <td>NaN</td>
      <td>快科技@http://www.kkj.cn/</td>
      <td>此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\r\n...</td>
      <td>{"type":"科技","site":"cnbeta","commentNum":"18"...</td>
      <td>一加手机5细节曝光：3300mAh、充半小时用1天</td>
      <td>http://www.cnbeta.com/articles/tech/623601.htm</td>
    </tr>
    <tr>
      <td>3</td>
      <td>89614</td>
      <td>NaN</td>
      <td>新华社</td>
      <td>这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\r\n</td>
      <td>{"type":"国际新闻","site":"环球","commentNum":"0","j...</td>
      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）</td>
      <td>http://world.huanqiu.com/hot/2017-06/10866126....</td>
    </tr>
    <tr>
      <td>4</td>
      <td>89613</td>
      <td>胡淑丽_MN7479</td>
      <td>深圳大件事</td>
      <td>（原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\r\n@深圳交警微博称：昨日清...</td>
      <td>{"type":"新闻","site":"网易热门","commentNum":"978",...</td>
      <td>44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随</td>
      <td>http://news.163.com/17/0618/00/CN617P3Q0001875...</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">articles =content[<span class="string">'content'</span>].tolist()</span><br><span class="line">print(articles[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。\r\nMIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。\r\n当然，关于MIUI 9的确切信息，我们还是等待官方消息。\r\n&#39;, &#39;骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考虑性能而去屏蔽掉小核心。相反，他们正联手微软，找到一种适合桌面平台的、兼顾性能和功耗的完美方案。\r\n报道称，微软已经拿到了一些新的源码，以便Windows 10更好地理解big.little架构。\r\n资料显示，骁龙835作为一款集成了CPU、GPU、基带、蓝牙/Wi-Fi的SoC，比传统的Wintel方案可以节省至少30%的PCB空间。\r\n按计划，今年Q4，华硕、惠普、联想将首发骁龙835 Win10电脑，预计均是二合一形态的产品。\r\n当然，高通骁龙只是个开始，未来也许还能见到三星Exynos、联发科、华为麒麟、小米澎湃等进入Windows 10桌面平台。\r\n&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(articles)</span><br></pre></td></tr></table></figure>
<pre><code>89611
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">token</span><span class="params">(string)</span>:</span></span><br><span class="line">    <span class="comment"># we will learn the regular expression next course</span></span><br><span class="line">    <span class="keyword">return</span> re.findall(<span class="string">'\w+'</span>,string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">with_jieba_cut = Counter(jieba.cut(articles[<span class="number">110</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Building prefix dict from the default dictionary ...
Dumping model to file cache C:\Users\24768\AppData\Local\Temp\jieba.cache
Loading model cost 0.957 seconds.
Prefix dict has been built succesfully.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">with_jieba_cut.most_common()[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;，&#39;, 88),
 (&#39;的&#39;, 73),
 (&#39;。&#39;, 39),
 (&#39;\r\n&#39;, 27),
 (&#39;了&#39;, 20),
 (&#39;们&#39;, 18),
 (&#39;工作队&#39;, 16),
 (&#39;村民&#39;, 15),
 (&#39;收割&#39;, 14),
 (&#39;、&#39;, 12)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">''</span>.join(token(articles[<span class="number">110</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&#39;在外国名著麦田里的守望者中作者想要守护麦田里如自己内心一般纯真的孩子们而驻村干部们也在这个炎热的夏天里撸袖子上阵真正做起了村民们的麦田守望者三夏时节不等人你看到了吗不停翻涌起伏仿若铺陈至天边的金黄麦浪中那若隐若现的人影是自治区新闻出版广电局驻和田市肖尔巴格乡合尼村工作队的队员与工作队组织的青年志愿者在这个炎热的夏季他们深入田间地头帮助村民们收割小麦扛起收麦机麦田中的每个人都显得兴致勃勃一天下来就近22亩小麦收割完毕志愿者麦麦提亚森擦去满脸的汗水高兴地告诉驻村队员我们青年志愿者应该多做贡献为村里的脱贫致富出把力工作队带着我们为村里的老人服务看到那些像我爷爷奶奶一样的老人赞许感谢的目光我体会到了帮助他人的快乐自治区新闻出版广电局驻村工作队孙敏艾力依布拉音麦收时节我们在一起6月中旬的和田墨玉麦田金黄静待收割6月14日15日两天自治区高级人民法院驻和田地区墨玉县吐外特乡罕勒克艾日克村工作队与48名村民志愿者一道帮助村里29户有需要的村民进行小麦收割工作田间地头罕勒克艾日克村志愿队的红旗迎风飘扬格外醒目10余台割麦机一起轰鸣男人们在用机器收割小麦的同时几名妇女也加入到志愿队构成了一道美丽的麦收风景休息空闲工作队员和村民们坐在树荫下田埂上互相问好聊天语言交流有困难就用手势动作比划着聊天有趣地交流方式不时引来阵阵欢笑大家在一同享受丰收和喜悦也一同增进着彼此的情感和友谊自治区高级人民法院驻村工作队周春梅艾地艾木阿不拉细看稻菽千重浪6月15日自治区煤田灭火工程局的干部职工们再一次跋涉1000多公里来到了叶城县萨依巴格乡阿亚格欧尔达贝格村见到了自己的亲戚现场处处都透出掩盖不住的喜悦一声声亲切的谢谢一个个结实的拥抱都透露出浓浓的亲情没坐一会儿在嘘寒问暖中大家了解到在麦收的关键时刻部分村民家中却存在收割难的问题小麦成熟期短收获的时间集中天气的变化对小麦最终产量的影响极大如果不能及时收割会有不小损失的于是大家几乎立刻就决定要帮助亲戚们收割麦子在茂密的麦地里干部们每人手持一把镰刀一字排开挽起衣袖卷起裤腿挥舞着镰刀进行着无声的竞赛骄阳似火汗如雨下但这都挡不住大家的热情随着此起彼伏的镰刀割倒麦子的刷刷声响不一会一束束沉甸甸的麦穗就被整齐地堆放了起来当看到自己亲手收割的金黄色麦穗被一簇簇地打成捆运送到晒场每个人的脸上都露出了灿烂的笑容自治区煤田灭火工程局驻村工作队马浩南这是一个收获多多的季节6月13日清晨6时许和田地区民丰县若雅乡特开墩村的麦田里已经传来马达轰鸣声原来是自治区质监局驻村工作队趁着天气尚且凉爽开始了麦田的收割工作忙碌间隙志愿者队伍搬来清凉的水村民们拎来鲜甜的西瓜抹一把汗水吃一牙西瓜甜蜜的汁水似乎流进了每一个人的心里说起割麦子对于生活在这片土地上的村民来说是再平常不过的事但是对于工作队队员们来说却是陌生的自治区质监局驻民丰县若克雅乡博斯坦村工作队队员们一开始觉得十几个人一起收割二亩地应该会挺快的结果却一点不简单镰刀拿到自己手里割起来考验才真正的开始大家弓着腰弯着腿亦步亦趋手上挥舞着镰刀时刻注意不要让镰刀割到自己脚下还要留心不要把套种的玉米苗踩伤不一会儿就已经汗流浃背了抬头看看身边的村民早就远远地割到前面去了只有今年已经56岁的工作队队长李树刚有割麦经验多少给队员们挽回了些面子赶不上村民们割麦子的速度更不要说搞定收割机这台大家伙了现代化的机械收割能成倍提升小麦的收割速度李树刚说不过能有这样的体验拉近和村民的距离也是很难得的体验自治区质监局驻村工作队王辉马君刚我们是麦田的守护者为了应对麦收新疆银监局驻和田县塔瓦库勒乡也先巴扎村工作队一早就从经济支援和人力支援两方面做好了准备一方面工作队帮村里购入了5台小麦收割机另一边还组织村干部青年团员等组成了6支近百人的收割先锋突击队帮助村民们抢收麦子看着及时归仓的麦子村民们喜得合不拢嘴纷纷摘下自家杏树上的杏子送给工作队金黄的麦穗温暖了村民们的心香甜的杏子温暖了工作队员的心麦子加杏子拉近了村民和队员们的心新疆银监局驻村工作队王继发免责声明本文仅代表作者个人观点与环球网无关其原创性以及文中陈述文字和内容未经本站证实对本文以及其中全部或者部分内容文字的真实性完整性及时性本站不作任何保证或承诺请读者仅作参考并请自行核实相关内容&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">articles_clean = [<span class="string">''</span>.join(token(str(a))) <span class="keyword">for</span> a <span class="keyword">in</span> articles]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(articles_clean)</span><br></pre></td></tr></table></figure>
<pre><code>89611
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'article_tb.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> articles_clean:</span><br><span class="line">        f.write(a + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(string)</span>:</span> <span class="keyword">return</span> list(jieba.cut(string))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate((open(<span class="string">'article_tb.txt'</span>))):</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> ==<span class="number">0</span>: print(i)</span><br><span class="line">    <span class="keyword">if</span> i &gt;<span class="number">100000</span>:<span class="keyword">break</span></span><br><span class="line">    TOKEN.extend(cut(line))</span><br></pre></td></tr></table></figure>
<pre><code>0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
6900
7000
7100
7200
7300
7400
7500
7600
7700
7800
7900
8000
8100
8200
8300
8400
8500
8600
8700
8800
8900
9000
9100
9200
9300
9400
9500
9600
9700
9800
9900
10000
10100
10200
10300
10400
10500
10600
10700
10800
10900
11000
11100
11200
11300
11400
11500
11600
11700
11800
11900
12000
12100
12200
12300
12400
12500
12600
12700
12800
12900
13000
13100
13200
13300
13400
13500
13600
13700
13800
13900
14000
14100
14200
14300
14400
14500
14600
14700
14800
14900
15000
15100
15200
15300
15400
15500
15600
15700
15800
15900
16000
16100
16200
16300
16400
16500
16600
16700
16800
16900
17000
17100
17200
17300
17400
17500
17600
17700
17800
17900
18000
18100
18200
18300
18400
18500
18600
18700
18800
18900
19000
19100
19200
19300
19400
19500
19600
19700
19800
19900
20000
20100
20200
20300
20400
20500
20600
20700
20800
20900
21000
21100
21200
21300
21400
21500
21600
21700
21800
21900
22000
22100
22200
22300
22400
22500
22600
22700
22800
22900
23000
23100
23200
23300
23400
23500
23600
23700
23800
23900
24000
24100
24200
24300
24400
24500
24600
24700
24800
24900
25000
25100
25200
25300
25400
25500
25600
25700
25800
25900
26000
26100
26200
26300
26400
26500
26600
26700
26800
26900
27000
27100
27200
27300
27400
27500
27600
27700
27800
27900
28000
28100
28200
28300
28400
28500
28600
28700
28800
28900
29000
29100
29200
29300
29400
29500
29600
29700
29800
29900
30000
30100
30200
30300
30400
30500
30600
30700
30800
30900
31000
31100
31200
31300
31400
31500
31600
31700
31800
31900
32000
32100
32200
32300
32400
32500
32600
32700
32800
32900
33000
33100
33200
33300
33400
33500
33600
33700
33800
33900
34000
34100
34200
34300
34400
34500
34600
34700
34800
34900
35000
35100
35200
35300
35400
35500
35600
35700
35800
35900
36000
36100
36200
36300
36400
36500
36600
36700
36800
36900
37000
37100
37200
37300
37400
37500
37600
37700
37800
37900
38000
38100
38200
38300
38400
38500
38600
38700
38800
38900
39000
39100
39200
39300
39400
39500
39600
39700
39800
39900
40000
40100
40200
40300
40400
40500
40600
40700
40800
40900
41000
41100
41200
41300
41400
41500
41600
41700
41800
41900
42000
42100
42200
42300
42400
42500
42600
42700
42800
42900
43000
43100
43200
43300
43400
43500
43600
43700
43800
43900
44000
44100
44200
44300
44400
44500
44600
44700
44800
44900
45000
45100
45200
45300
45400
45500
45600
45700
45800
45900
46000
46100
46200
46300
46400
46500
46600
46700
46800
46900
47000
47100
47200
47300
47400
47500
47600
47700
47800
47900
48000
48100
48200
48300
48400
48500
48600
48700
48800
48900
49000
49100
49200
49300
49400
49500
49600
49700
49800
49900
50000
50100
50200
50300
50400
50500
50600
50700
50800
50900
51000
51100
51200
51300
51400
51500
51600
51700
51800
51900
52000
52100
52200
52300
52400
52500
52600
52700
52800
52900
53000
53100
53200
53300
53400
53500
53600
53700
53800
53900
54000
54100
54200
54300
54400
54500
54600
54700
54800
54900
55000
55100
55200
55300
55400
55500
55600
55700
55800
55900
56000
56100
56200
56300
56400
56500
56600
56700
56800
56900
57000
57100
57200
57300
57400
57500
57600
57700
57800
57900
58000
58100
58200
58300
58400
58500
58600
58700
58800
58900
59000
59100
59200
59300
59400
59500
59600
59700
59800
59900
60000
60100
60200
60300
60400
60500
60600
60700
60800
60900
61000
61100
61200
61300
61400
61500
61600
61700
61800
61900
62000
62100
62200
62300
62400
62500
62600
62700
62800
62900
63000
63100
63200
63300
63400
63500
63600
63700
63800
63900
64000
64100
64200
64300
64400
64500
64600
64700
64800
64900
65000
65100
65200
65300
65400
65500
65600
65700
65800
65900
66000
66100
66200
66300
66400
66500
66600
66700
66800
66900
67000
67100
67200
67300
67400
67500
67600
67700
67800
67900
68000
68100
68200
68300
68400
68500
68600
68700
68800
68900
69000
69100
69200
69300
69400
69500
69600
69700
69800
69900
70000
70100
70200
70300
70400
70500
70600
70700
70800
70900
71000
71100
71200
71300
71400
71500
71600
71700
71800
71900
72000
72100
72200
72300
72400
72500
72600
72700
72800
72900
73000
73100
73200
73300
73400
73500
73600
73700
73800
73900
74000
74100
74200
74300
74400
74500
74600
74700
74800
74900
75000
75100
75200
75300
75400
75500
75600
75700
75800
75900
76000
76100
76200
76300
76400
76500
76600
76700
76800
76900
77000
77100
77200
77300
77400
77500
77600
77700
77800
77900
78000
78100
78200
78300
78400
78500
78600
78700
78800
78900
79000
79100
79200
79300
79400
79500
79600
79700
79800
79900
80000
80100
80200
80300
80400
80500
80600
80700
80800
80900
81000
81100
81200
81300
81400
81500
81600
81700
81800
81900
82000
82100
82200
82300
82400
82500
82600
82700
82800
82900
83000
83100
83200
83300
83400
83500
83600
83700
83800
83900
84000
84100
84200
84300
84400
84500
84600
84700
84800
84900
85000
85100
85200
85300
85400
85500
85600
85700
85800
85900
86000
86100
86200
86300
86400
86500
86600
86700
86800
86900
87000
87100
87200
87300
87400
87500
87600
87700
87800
87900
88000
88100
88200
88300
88400
88500
88600
88700
88800
88900
89000
89100
89200
89300
89400
89500
89600
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add,mul</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reduce(add,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<pre><code>23
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count =Counter(TOKEN)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count.most_common(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;的&#39;, 703716),
 (&#39;n&#39;, 382020),
 (&#39;在&#39;, 263597),
 (&#39;月&#39;, 189330),
 (&#39;日&#39;, 166300),
 (&#39;新华社&#39;, 142462),
 (&#39;和&#39;, 134061),
 (&#39;年&#39;, 123106),
 (&#39;了&#39;, 121938),
 (&#39;是&#39;, 100909),
 (&#39;\n&#39;, 89611),
 (&#39;１&#39;, 88187),
 (&#39;０&#39;, 84945),
 (&#39;外代&#39;, 83268),
 (&#39;中&#39;, 73926),
 (&#39;中国&#39;, 71179),
 (&#39;２&#39;, 70521),
 (&#39;2017&#39;, 69894),
 (&#39;记者&#39;, 62147),
 (&#39;二线&#39;, 61998),
 (&#39;将&#39;, 61420),
 (&#39;与&#39;, 58309),
 (&#39;等&#39;, 58162),
 (&#39;为&#39;, 57019),
 (&#39;5&#39;, 54578),
 (&#39;照片&#39;, 52271),
 (&#39;4&#39;, 51626),
 (&#39;对&#39;, 50317),
 (&#39;上&#39;, 47452),
 (&#39;也&#39;, 47401),
 (&#39;有&#39;, 45767),
 (&#39;５&#39;, 40857),
 (&#39;说&#39;, 39017),
 (&#39;发展&#39;, 37632),
 (&#39;他&#39;, 37194),
 (&#39;３&#39;, 36906),
 (&#39;以&#39;, 36867),
 (&#39;国际&#39;, 35842),
 (&#39;nn&#39;, 35330),
 (&#39;４&#39;, 34659),
 (&#39;比赛&#39;, 32232),
 (&#39;６&#39;, 30575),
 (&#39;到&#39;, 30109),
 (&#39;人&#39;, 29572),
 (&#39;从&#39;, 29489),
 (&#39;6&#39;, 29002),
 (&#39;都&#39;, 28027),
 (&#39;不&#39;, 27963),
 (&#39;后&#39;, 27393),
 (&#39;当日&#39;, 27186),
 (&#39;就&#39;, 26684),
 (&#39;并&#39;, 26568),
 (&#39;国家&#39;, 26439),
 (&#39;７&#39;, 26386),
 (&#39;企业&#39;, 26147),
 (&#39;进行&#39;, 25987),
 (&#39;3&#39;, 25491),
 (&#39;美国&#39;, 25485),
 (&#39;举行&#39;, 25389),
 (&#39;被&#39;, 25277),
 (&#39;北京&#39;, 25245),
 (&#39;体育&#39;, 24873),
 (&#39;2&#39;, 24376),
 (&#39;1&#39;, 24182),
 (&#39;这&#39;, 24118),
 (&#39;新&#39;, 23828),
 (&#39;但&#39;, 23385),
 (&#39;比&#39;, 23229),
 (&#39;个&#39;, 23081),
 (&#39;足球&#39;, 22554),
 (&#39;表示&#39;, 22134),
 (&#39;经济&#39;, 22006),
 (&#39;我&#39;, 21940),
 (&#39;一个&#39;, 21932),
 (&#39;９&#39;, 21920),
 (&#39;还&#39;, 21861),
 (&#39;合作&#39;, 21567),
 (&#39;要&#39;, 21045),
 (&#39;n5&#39;, 20946),
 (&#39;已&#39;, 20882),
 (&#39;摄&#39;, 20837),
 (&#39;８&#39;, 20701),
 (&#39;工作&#39;, 20700),
 (&#39;n4&#39;, 20658),
 (&#39;选手&#39;, 19986),
 (&#39;我们&#39;, 19982),
 (&#39;市场&#39;, 19001),
 (&#39;一路&#39;, 18978),
 (&#39;一带&#39;, 18907),
 (&#39;建设&#39;, 18634),
 (&#39;让&#39;, 18609),
 (&#39;日电&#39;, 18384),
 (&#39;通过&#39;, 18159),
 (&#39;多&#39;, 17760),
 (&#39;时&#39;, 17750),
 (&#39;完&#39;, 17424),
 (&#39;于&#39;, 17421),
 (&#39;问题&#39;, 17338),
 (&#39;更&#39;, 17275),
 (&#39;项目&#39;, 17260)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">frequiences = [f <span class="keyword">for</span> w,f <span class="keyword">in</span> words_count.most_common(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x =[ i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#内嵌画图</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x,frequiences)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x16f678b8b48&gt;]
</code></pre><p><img src="/images/output_58_1.png" alt="png"></p>
<p>NLP比较重要的规律，在很大的一个 text corpus, 文字集合中，出现频率第二多的单词，是出现频率第一多单词频率的1/2，出现频率滴N多的的单词是第一多的1/N</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x,np.log(frequiences))</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x16efa34bf08&gt;]
</code></pre><p><img src="/images/output_61_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_1</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> words_count[word] / len(TOKEN)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_1(<span class="string">'我们'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.0011341645999654677
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN[:<span class="number">10</span>]</span><br><span class="line">len(TOKEN[:<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>17618253
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN = [str(t) <span class="keyword">for</span> t <span class="keyword">in</span> TOKEN]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN_2_GRAM = [<span class="string">''</span>.join(TOKEN[i:i+<span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(TOKEN[:<span class="number">-2</span>]))]</span><br><span class="line">len(TOKEN[:<span class="number">-2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>17618252
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TOKEN_2_GRAM[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;此外自&#39;, &#39;自本周&#39;, &#39;本周6&#39;, &#39;6月&#39;, &#39;月12&#39;, &#39;12日起&#39;, &#39;日起除&#39;, &#39;除小米&#39;, &#39;小米手机&#39;, &#39;手机6&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words_count_2 =Counter(TOKEN_2_GRAM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob_2</span><span class="params">(word1,word2)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> word1+word2 <span class="keyword">in</span> words_count_2: <span class="keyword">return</span> words_count_2[word1+word2] / len(TOKEN_2_GRAM)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / len(TOKEN_2_GRAM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_2(<span class="string">'我们'</span>,<span class="string">'在'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>3.0536514065072974e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prob_2(<span class="string">'去'</span>,<span class="string">'吃饭'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>7.946304775297799e-07
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probablity</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    words = cut(sentence)</span><br><span class="line">    sentence_prob =<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i , word <span class="keyword">in</span>  enumerate(words[:<span class="number">-1</span>]):</span><br><span class="line">        next_ =words[i+<span class="number">1</span>]</span><br><span class="line">        probablity =prob_2(word,next_)</span><br><span class="line">        sentence_prob *= probablity</span><br><span class="line">    sentence_prob*=prob_1(word[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> sentence_prob</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_probablity(<span class="string">'fd是一个小可爱'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>5.193529218987722e-22
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_probablity(<span class="string">'小敏今天抽到了一个iphone'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>9.08660601226132e-36
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sen <span class="keyword">in</span> [generate(gram=example_grammar,target=<span class="string">'sentence'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]:</span><br><span class="line">    print(<span class="string">'sentence: &#123;&#125; with Prb: &#123;&#125;'</span>.format(sen,get_probablity(sen)))</span><br></pre></td></tr></table></figure>
<pre><code>sentence: 一个好看的小猫坐着一个小猫 with Prb: 3.288492351824868e-47
sentence: 这个蓝色的小小的好看的女人看着一个好看的蓝色的好看的小猫 with Prb: 1.4336960791945416e-97
sentence: 一个桌子看着这个篮球 with Prb: 1.6316301805544588e-31
sentence: 一个小小的蓝色的好看的小猫听见一个小猫 with Prb: 5.413402114895177e-62
sentence: 这个女人听见一个女人 with Prb: 4.894890541663377e-31
sentence: 这个好看的小小的女人看着这个好看的篮球 with Prb: 7.764808720237364e-62
sentence: 这个桌子看着一个好看的篮球 with Prb: 2.948877435423709e-42
sentence: 这个小小的好看的篮球看着这个小小的小小的小小的小猫 with Prb: 1.4874477576323182e-68
sentence: 这个篮球坐着一个好看的好看的桌子 with Prb: 4.8451281079684025e-59
sentence: 这个女人听见这个好看的篮球 with Prb: 2.948877435423709e-42
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">need_compared = [</span><br><span class="line">    <span class="string">"今天晚上请你吃饭，我们一起吃火锅 明天晚上请你吃饭，我们一起吃烧烤"</span>,</span><br><span class="line">    <span class="string">"真实一只可爱的小猫 真是一只可爱的小猫"</span>,</span><br><span class="line">    <span class="string">"今天晚上去飞翔 今天晚上飞翔去我"</span>,</span><br><span class="line">    <span class="string">"洋葱奶昔来一杯 养乐多绿来一杯"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> need_compared:</span><br><span class="line">    s1,s2 =s.split()</span><br><span class="line">    p1,p2 = get_probablity(s1),get_probablity(s2)</span><br><span class="line">    </span><br><span class="line">    better = s1 <span class="keyword">if</span> p1 &gt; p2 <span class="keyword">else</span> s2</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'&#123;&#125; is more possible'</span>.format(better))</span><br><span class="line">    print(<span class="string">'-'</span>*<span class="number">4</span> + <span class="string">' &#123;&#125; with probablity &#123;&#125;'</span>.format(s1,p1))</span><br><span class="line">    print(<span class="string">'-'</span>*<span class="number">4</span> + <span class="string">' &#123;&#125; with probablity &#123;&#125;'</span>.format(s2,p2))</span><br></pre></td></tr></table></figure>
<pre><code>今天晚上请你吃饭，我们一起吃火锅 is more possible
---- 今天晚上请你吃饭，我们一起吃火锅 with probablity 6.195089781185323e-58
---- 明天晚上请你吃饭，我们一起吃烧烤 with probablity 4.512004197519248e-64
真是一只可爱的小猫 is more possible
---- 真实一只可爱的小猫 with probablity 5.014470336000062e-27
---- 真是一只可爱的小猫 with probablity 5.014470336000062e-27
今天晚上去飞翔 is more possible
---- 今天晚上去飞翔 with probablity 1.1061245254231737e-24
---- 今天晚上飞翔去我 with probablity 2.3020387757829988e-31
养乐多绿来一杯 is more possible
---- 洋葱奶昔来一杯 with probablity 1.1160363145085983e-25
---- 养乐多绿来一杯 with probablity 1.9662609030163744e-18
</code></pre><h1 id="Data-Driven-1"><a href="#Data-Driven-1" class="headerlink" title="Data Driven"></a>Data Driven</h1><h2 id="假如你做了很久的数据预处理"><a href="#假如你做了很久的数据预处理" class="headerlink" title="假如你做了很久的数据预处理"></a>假如你做了很久的数据预处理</h2><h2 id="AI的问题里边，65-都在做数据预处理"><a href="#AI的问题里边，65-都在做数据预处理" class="headerlink" title="AI的问题里边，65%都在做数据预处理"></a>AI的问题里边，65%都在做数据预处理</h2><h2 id="我们养成一个习惯，把数据存到硬盘里"><a href="#我们养成一个习惯，把数据存到硬盘里" class="headerlink" title="我们养成一个习惯，把数据存到硬盘里"></a>我们养成一个习惯，把数据存到硬盘里</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Syntax-Tree</tag>
        <tag>Language Model</tag>
      </tags>
  </entry>
  <entry>
    <title>python BinarySearchTree</title>
    <url>/2019/11/29/python-BinarySearchTree/</url>
    <content><![CDATA[<h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p>&emsp;&emsp;最近在学python，断断续续的，感觉学的慢，就顺便写写代码，加强对python的感觉。<br>虽然学了半天还是啥都不会，但是还是要写写博客啥的，激励自己不老懒惰，不能因为眼前的</p>
<h2 id="困难而放弃进步。所以，下面我要贴代码了…"><a href="#困难而放弃进步。所以，下面我要贴代码了…" class="headerlink" title="困难而放弃进步。所以，下面我要贴代码了…."></a>困难而放弃进步。所以，下面我要贴代码了….</h2><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinarySearchTree</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.root.__iter__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self,key,val)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.root:</span><br><span class="line">            self._put(key,val,self.root)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.root = TreeNode(key,val)</span><br><span class="line">        self.size+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_put</span><span class="params">(self,key,val,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> key &lt;currentNode.key:</span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild():</span><br><span class="line">                self._put(key,val,currentNode.leftChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.leftChild = TreeNode(key,val,parent=currentNode)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> currentNode.hasRightChild():</span><br><span class="line">                self._put(key,val,currentNode.rightChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.rightChild = TreeNode(key,val, parent = currentNode)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self,k,v)</span>:</span></span><br><span class="line">        self.put(k,v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.root:</span><br><span class="line">            res = self._get(key,self.root)</span><br><span class="line">            <span class="keyword">if</span> res:</span><br><span class="line">                <span class="keyword">return</span> res.payload</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span>  <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get</span><span class="params">(self,key,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> currentNode:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> currentNode.key == key:</span><br><span class="line">            <span class="keyword">return</span> currentNode</span><br><span class="line">        <span class="keyword">elif</span> currentNode.key &lt; key:</span><br><span class="line">            <span class="keyword">return</span> self._get(key,currentNode.rightChild)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._get(key,currentNode.leftChild)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.get(key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self._get(key,self.root):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span>   <span class="comment">#if 'fff' in mytree:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.size &gt;<span class="number">1</span>:</span><br><span class="line">            nodeToRemove = self._get(key,self.root)</span><br><span class="line">            <span class="keyword">if</span> nodeToRemove:</span><br><span class="line">                self.remove(nodeToRemove)</span><br><span class="line">                self.size-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> KeyError(<span class="string">'Error,key not in tree'</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.size ==<span class="number">1</span> <span class="keyword">and</span> self.root.key == key:</span><br><span class="line">            self.root = <span class="literal">None</span></span><br><span class="line">            self.size =self.size<span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'Error,key not in tree'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__delitem__</span><span class="params">(self,key)</span>:</span></span><br><span class="line">        self.delete(key)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self,currentNode)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> currentNode.isLeaf():</span><br><span class="line">            <span class="keyword">if</span> currentNode == currentNode.parent.leftChild:</span><br><span class="line">                currentNode.parent.leftChild = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                currentNode.parent.rightChild = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> currentNode.hasBothChildren():</span><br><span class="line">            succ = currentNode.findSuccessor()</span><br><span class="line">            succ.spliceOut()</span><br><span class="line">            currentNode.key = succ.key</span><br><span class="line">            currentNode.payload = succ.payload</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#this node has one child</span></span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild():</span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild():</span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.leftChild = currentNode.leftChild</span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild():</span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.rightChild = currentNode.leftChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    currentNode.replaceNodeData(currentNode.leftChild.key,currentNode.leftChild.payload,</span><br><span class="line">                                               currentNode.leftChild.leftChild,</span><br><span class="line">                                               currentNode.leftChild.rightChild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild():</span><br><span class="line">                    currentNode.rightChild.parent =currentNode.parent</span><br><span class="line">                    currentNode.parent.leftChild=currentNode.rightChild</span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild():</span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent</span><br><span class="line">                    currentNode.parent.rightChild =currentNode.rightChild</span><br><span class="line"></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    currentNode.replaceNodeData(currentNode.rightChild.key,currentNode.rightChild.payload,</span><br><span class="line">                                               currentNode.rightChild.leftChild,</span><br><span class="line">                                               currentNode.rightChild.rightChild)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, key, val, left = None, right = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                        parent = None)</span>:</span></span><br><span class="line">        self.key = key</span><br><span class="line">        self.payload = val</span><br><span class="line">        self.leftChild = left</span><br><span class="line">        self.rightChild = right</span><br><span class="line">        self.parent = parent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasLeftChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.leftChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasRightChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isLeftChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.leftChild == self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isRightChild</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.rightChild == self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isRoot</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.parent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isLeaf</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> (self.rightChild <span class="keyword">or</span> self.leftChild)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasAnyChildren</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">or</span> self.leftChild</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasBothChildren</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">and</span> self.leftChild</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceNodeData</span><span class="params">(self,key,value,lc,rc)</span>:</span></span><br><span class="line">        self.key =key</span><br><span class="line">        self.payload = value</span><br><span class="line">        self.leftChild = lc</span><br><span class="line">        self.rightChild = rc</span><br><span class="line">        <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">            self.leftChild.parent =self</span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">            self.rightChild.parent = self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSuccessor</span><span class="params">(self)</span>:</span></span><br><span class="line">        succ = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">            succ =self.rightChild.findMin()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.parent:</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    succ = self.parent</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = <span class="literal">None</span></span><br><span class="line">                    succ =self.parent.findSuccessor()</span><br><span class="line">                    self.parent.rightChild = self</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> succ</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMin</span><span class="params">(self)</span>:</span></span><br><span class="line">        current = self</span><br><span class="line">        <span class="keyword">while</span> current.hasLeftChild():</span><br><span class="line">            current =current.leftChild</span><br><span class="line">        <span class="keyword">return</span> current</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spliceOut</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.isLeaf():</span><br><span class="line">            <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                self.parent.leftChild = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent.rightChild = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">elif</span> self.hasAnyChildren():</span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    self.parent.leftChild = self.leftChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = self.leftChild</span><br><span class="line">                self.leftChild.parent =self.parent</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild():</span><br><span class="line">                    self.parent.leftChild = self.rightChild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.parent.rightChild = self.rightChild</span><br><span class="line">                self.rightChild.parent =self.parent</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self:</span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild():</span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.leftChild:</span><br><span class="line">                    <span class="keyword">yield</span> elem</span><br><span class="line">            <span class="keyword">yield</span> self.key</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.hasRightChild():</span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.rightChild:</span><br><span class="line">                    <span class="keyword">yield</span> elem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">mytree = BinarySearchTree()</span><br><span class="line">mytree[<span class="number">3</span>]=<span class="string">"red"</span></span><br><span class="line">mytree[<span class="number">4</span>]=<span class="string">"blue"</span></span><br><span class="line">mytree[<span class="number">6</span>]=<span class="string">"yellow"</span></span><br><span class="line">mytree[<span class="number">2</span>]=<span class="string">"at"</span></span><br><span class="line"></span><br><span class="line">print(mytree[<span class="number">6</span>])</span><br><span class="line">print(mytree[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World ~</title>
    <url>/2019/11/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to  my own web,you can find me on github <a href="https://github.com/iiot-tbb" target="_blank" rel="noopener">iiot-tbb</a>! This is my very first post.</p>
<a id="more"></a>
<p><img src="/images/bac.jpg" alt=""></p>
]]></content>
      <tags>
        <tag>myBorn</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>论法的精神（节选）</title>
    <url>/2019/11/11/%E8%AE%BA%E6%B3%95%E7%9A%84%E7%B2%BE%E7%A5%9E%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/</url>
    <content><![CDATA[<h3 id="法与一切存在物的关系"><a href="#法与一切存在物的关系" class="headerlink" title="法与一切存在物的关系"></a>法与一切存在物的关系</h3><p>从最大限度的广义上说，法是源于客观事物性质的必然关系。从这个意义上推断，所有的</p>
<p>存在物都有属于自己的法；上帝有他的法；物质世界也有它的法；高于人类的“先知</p>
<p>圣人们”有着他们的法；畜类也有自己的法；人类拥有他们的法。</p>
<p>有些人说，世间我们看到的万物都是由一个盲目的命运所创造的，这种说法荒谬绝伦。因</p>
<p>为盲目的命运竟然创造“具有智能的创造物”，岂不是一件更为荒谬的事吗?</p>
<p>于是便有了一个最浅显的理性的存在。法就是这个浅显理性与各种存在物之间关系的总</p>
<p>和，同时也体现着所有客观存在物彼此之间的关系。</p>
<p>上帝与宇宙的关系体现在，它既是宇宙的创造者又是它的保管者：以此产生的规律，便是</p>
<p>保管时参照的规律。上帝遵循这些规律行事，因为他熟知这些规范；之所以他熟知这些规</p>
<p>范，因为正是他制定了这些规范；他之所以制定这些规范，因为这些规律与他的才智和能<br>量有着密不可分的关系。</p>
<p>如同我们看到的一样，我们所在的世界是由物质的运动而构成的，它在一个非智能的状态</p>
<p>中永恒地生存着。它的物质运动必然具有某种固定的规律。如果人们能够在自己所处的世</p>
<p>界之外再臆想出另一个世界的话，那个世界要么具有恒定的规律可循，要么便是毁灭。</p>
<p>创造本身似乎是某种随意的行为，然而其中必定蕴涵着恒定的规律，就如同无神论者的命</p>
<p>运一般。如果造世主没有这些规范就能统管世界的话，那么，这显然是荒谬的说法，因为</p>
<p>没有这些规范，世界将无法生存。<br><a id="more"></a></p>
<p>这些规律建立在恒定不变的关系之中。在两个运动的物体之间，遵循其重量和速度间的关</p>
<p>系，从而承受所有运动形式给予它们的作用力，增加、减力以及消失；每一次差别，都有</p>
<p>其均衡性；而每一次变化，都有其永恒性。</p>
<p>特殊的“智能存在物”能够拥有自己制定的法律，然而也有一些并非是他们创造的，却被</p>
<p>他们所拥有。在“智能存在物”产生之前，它们已有了存在的可能性；于是它们便有了存</p>
<p>在的关系，所以也就有了可能存在的法律。在法律制定之前，已有了产生公共关系的可能</p>
<p>性。如果在人为法限制或禁止的行为之外，就无公道可言的话，那么便意味着，当人们还</p>
<p>未画出圆圈时，所有的半径也并不相等。</p>
<p>因此，我们应该承认，在人为法确定之前，已存在着公正。例如：即使在纷繁的人类社会</p>
<p>中，遵守法律乃是天经地义；如果某些“智能存在物”从另一个“智能存在物”身上获取某</p>
<p>种利益的话，前者就应该怀有感激之情；如果某一个“智能存在物”创造了另一个“智能存</p>
<p>在物”，那么，被创造者则应该保持自己原有的依存关系；当一个“智能存在物”损害另一</p>
<p>个同类时，它自身也应该受到相应的损害；还能列出许多公正关系的例子。</p>
<p>然而却不能说智能世界和物质世界被管理得一样完备。因为，尽管智能世界有其自身的规</p>
<p>律，这些规律就性质而言是不可变化的，但是智能世界却不能像物质世界那样永恒地遵守</p>
<p>其规律，原因是某些特殊的“智能存在物”囿于其本性而导致错误；而且，从另一个方面</p>
<p>来说，他们按自己的本性我行我素。所以，他们并不是永恒地遵守自己最初级的规律；即</p>
<p>使那些规律是他们自己制定的，他们却总是不去遵守它们。</p>
<p>人们不知道，兽类是否受到一般规律的支配，还是受特殊动力的支配。无论如何，兽类与</p>
<p>上帝的关系绝不比其他的物质世界的关系更亲密；兽类的感官只限于它们彼此的关系，与</p>
<p>其他特殊存在物，或者是与它们自身的关系之中。</p>
<p>出于欲念的诱惑，兽类保留了自己的特殊的生灵特征；而且，由于同样的原因，它们还保</p>
<p>存了自身的种类。它们具有自然法，因为它们是由感官组合而成的；它们并没有自己制定</p>
<p>的法律，因为它们并不是由知识组合而成的。然而，兽类并非始终不渝地遵守它们的自然</p>
<p>法则。倒是那些看上去并非掌握知识和具备感官的植物，却较为忠实地遵守着自然法则。</p>
<p>兽类不具备我们人类所具有的最高级的智能，然而我们却不具备它们拥有的某些能力。他</p>
<p>们丝毫没有我们所拥有的种种欲望，当然，它们也没有我们所特有的种种担忧和恐惧；它</p>
<p>们和我们一样将经受死亡，但是它们却不了解死亡；它们中的大多数甚至比我们更完好地</p>
<p>保存自身，却并不像我们那样滥用情欲。</p>
<p>人，从 <strong> “物质存在” </strong> 的意义上而言，与其他物体别无二致，人受到永恒规律的支配。当人</p>
<p>作为“智能存在物”的时候，便时常诋毁上帝创造的规律，并且更改自己制定的法律。他</p>
<p>本应主宰自我；然而他毕竟是有其局限性的存在物；他与所有“有限的智能生物”一样，</p>
<p>无知与错误在所难免；他不但知之甚少，就连仅有的粗浅知识也会丧失殆尽。作为有感知</p>
<p>的生物，他受到无数情欲的支配。这样一种存在物随时都可能忘却他们的创造者；上帝便<br>以宗教的法规让他们回忆起这一切。这样的存在物随时也会忘却他自己；哲学家以道德规范警示他。为了在社会中生存，他必定要有所作为，但是，他可能将别人置之外；</p>
<p>立法者们以政治和民事法律促使他们尽其义务。</p>
]]></content>
      <tags>
        <tag>人文</tag>
        <tag>法律</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
