<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Bool_tbb" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
<meta property="og:type" content="website">
<meta property="og:title" content="Bool_tbb">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Bool_tbb">
<meta property="og:description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="bool_tbb">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Bool_tbb</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body itemscope itemtype="http://schema.org/WebPage">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bool_tbb</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">一枚NLPer小菜鸡</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/05/26/Grammar-Induction-for-Spoken-Dialogue-Systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/26/Grammar-Induction-for-Spoken-Dialogue-Systems/" class="post-title-link" itemprop="url">Grammar Induction for Spoken Dialogue Systems</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-26 00:38:31" itemprop="dateCreated datePublished" datetime="2022-05-26T00:38:31+08:00">2022-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-05 11:36:31" itemprop="dateModified" datetime="2022-06-05T11:36:31+08:00">2022-06-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>之前实习的时候做过一些语法推导(Grammar Induction)方面的工作，当时用的方法主要基于N-gram的一些推导，虽然工作量大，但是基本上属于baseline的方法，最近空下来，打算对这部分内容进行深入的研究和学习。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/26/Grammar-Induction-for-Spoken-Dialogue-Systems/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/05/21/Parameter-Efficient-Transfer-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/21/Parameter-Efficient-Transfer-Learning/" class="post-title-link" itemprop="url">Parameter Efficient Transfer Learning</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-21 19:38:07" itemprop="dateCreated datePublished" datetime="2022-05-21T19:38:07+08:00">2022-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-24 22:53:04" itemprop="dateModified" datetime="2022-05-24T22:53:04+08:00">2022-05-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这两天读了一些(两篇..)关于Parameter Efficient的文章，结构并不复杂，但是感觉还是有些内容可以分享。</p>
<h3 id="什么是参数高效的迁移学习呢？"><a href="#什么是参数高效的迁移学习呢？" class="headerlink" title="什么是参数高效的迁移学习呢？"></a>什么是参数高效的迁移学习呢？</h3><p>在自然语言处理的任务中，目前各个任务中常用的方法普遍基于Transformer模型，更具体来说，基于以Transformer的backbone模型的大规模预训练模型,以此为基础在各种任务上进行fine-tuning。通常情况下，<br>会对所有的模型参数进行微调，当然，通过这样的方式，可以更容易使得模型达到比较好的效果。因此也是大多数工作所进行的方式。</p>
<p>但是呢，这就会衍生出一个问题，首先，训练和部署效率的问题，先说训练，对于全局模型进行微调势必会消耗大量算力以及有一点点费电，那当然也可以通过frozen预训练模型在下有任务微调，这就和今天的主题有些相似了。另外呢，如果有很多很多的任务，你对每一个任务都进行fine-tuning，然后部署保存模型，对于现在动辄billion级别的模型参数来说，比如说你一个模型10GB，那你有100个任务，则需要1TB的存储量。</p>
<p>这时候，如果我们可以插入一小部分网络结构并且只对这一小部分进行微调就能达到一个比较好的效果，岂不美哉。</p>
<p>因此关于这部分的内容就应运而生。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/21/Parameter-Efficient-Transfer-Learning/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/" class="post-title-link" itemprop="url">Bertology:说一说那些不为人知的细节</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-01 16:16:22" itemprop="dateCreated datePublished" datetime="2022-05-01T16:16:22+08:00">2022-05-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-08 17:31:00" itemprop="dateModified" datetime="2022-05-08T17:31:00+08:00">2022-05-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>自 Transformer 模型的文章<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">《Attention is all you need》</a>这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕获能力，基于Transformer系列的backbone预训练模型在下游任务微调成一种常用的方法。</p>
<p>虽然每年有大量的前沿研究基于Bert这样的模型，但是很多文章—-表面是对模型进行了创新获得了不错的效果，究其本质，无非是扩大了模型的参数量，提高训练数据的质量和数量。因此，取得了更为理想的效果。关于为什么模型的效果好，以及每一层的神经网络到底是取得了如何的效果则没有系统的阐释。今天读到了一篇<a href="https://watermark.silverchair.com/tacl_a_00349.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsowggLGBgkqhkiG9w0BBwagggK3MIICswIBADCCAqwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM356qe6dtueuLMY59AgEQgIICfeMIeHCgVGdSvJu8D2XfjC5ElP1yXiIVEdIbGODaHM9mO1WisSSW5owwhoZNils1hOJUETd_3bZvcHmLZBMUwDPl90BJI5W5psceyCep9D5F3qW-dy31dxzM2bWVKBUjHL7WE7vcbad4Cvrzq_AlR36jArY0mhSc5ekJJRdbTNMHtskqingzrFuLz8eR1yUpWpq8kDtceTZV1zBJCM_NVinFFwwt1eh9cn7n8PVEm4em5fj912Oo77yMTPnr_6nPORwVjl9ZuboiKWTzV0XJz3TO-PxXcUD_mg-6DdbsS9hXibOZ3oz9bVpXVGKW6XYFOs7BzHT6A6M9lDDKDtAp2R9iBqajMyhukiB1bB6VA0ePRke64wdJ17CF9xibbH8FqlDlaYhieRLmc7DTWbIFA15hsM7Aq1iwB9mYQ49eur0aCzZSZJWRrxugqSs3MuNSnjzJvClzEQwKRBToWUFfQ0FlxG0vpOfswjZz2PnDBDhOZD6VvVF0Gs1I19RJsoaMAxfI5lMNk3tSsvxcsFbRms7EOTXG6O0qlkCsMF4GfaGp6wQq9rsAmgQsGHu1wecwwRrMveNbF7dd-0crKukSCUwZOCXM0_qn5BiOOflI4LXAjJZ6T5A1kseghSFTFHLNM1no4Vxa6tdLY-njxhK50ib5Cytrunc58Kc1qwQbahB3x4UVlKm8ww14pFjnLi2e1nJJi-_YWdu1JNuI6RbhsM_GS1iSxu6KmsH49DTy0urourShCVC-23Y6KM91pry926KPk2r_gGzmoIwC0FSGZ-FCIlMzUING1XRyVlPR9P414Ifv1_hSJKldyzljyMyte-HJhKunk9uuuWOa6f8" target="_blank" rel="noopener">《A Primer in BERTology: What We Know About How BERTWorks》</a>发表于Transactions of the Association for Computational Linguistics。这篇文章对于bert模型的一些细节进行了系统的论述，很全面，因此，读了一下，把它分享给大家。</p>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>众所周知，BERT已经在nlp领域大杀四方，但是对于“why it works well”相对来说没有那么多的研究。如此一来，想要更进一步做出更加solid的研究，需要对该模型有更多的了解。 相比较一些经典的backbone，比如说卷积神经网络(CNN)，在感性上的动机则没有特别明显。我们希望对其中的一些结构进行分析，使得模型中具体所做的事情更容易被人们所认识，但是，对于模型过于大的尺寸所造成的预训练不好做（大部分研究人员没有超多的显卡，以及预训练所耗费的经费十分多）且想要做消融实验并不容易。因此对于近些年来对解释bert的文章做一个综述，方便大家对于该模型的认识。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">bilstm+crf(batch)的实现</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-18 21:27:11 / 修改时间：21:40:45" itemprop="dateCreated datePublished" datetime="2022-04-18T21:27:11+08:00">2022-04-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="batch版本的条件随机场"><a href="#batch版本的条件随机场" class="headerlink" title="batch版本的条件随机场"></a>batch版本的条件随机场</h2><p>在上一篇文章中我们按照了 pytorch的官方教程复现了 简单版本的 条件随机场。基本了解了其中的代码实现。但是我们可以看到上一篇中的 crf需要反复的条件循环，同时也没有支持批处理的操作，如果实际应用的话，速度应该会慢很多，因此，在这里，我们实现了Batch版本的条件随机场。to be honest, batch版本相比较 傻瓜版本的实现有一丢丢的复杂，尤其是需要考虑大量的矩阵并行的操作，十分伤脑筋。还好，我参考了<a href="https://github.com/jtlin-sync/batch_bilstm_crf" target="_blank" rel="noopener">batch lstm+crf</a>代码并且认真的剖析其中的细节后，基本弄清了其中的实现细节。如果你对 这部分代码感兴趣，可以参考我下面的代码跑起来学一学。在这部分代码中，一些核心地方给出了注释，但是可能仍然不够清晰，希望你可以自己画画图搞清楚这些细节。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">pytorch版本的bilstm+crf的实现</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-15 01:12:00 / 修改时间：15:28:31" itemprop="dateCreated datePublished" datetime="2022-04-15T01:12:00+08:00">2022-04-15</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="CRF-代码实现"><a href="#CRF-代码实现" class="headerlink" title="CRF 代码实现"></a>CRF 代码实现</h2><p>最近没啥事情，想把CRF相关的内容再捋一捋，因此研究了Pytoch的CRF实现，代码如下展示，之后将会研究如何做batch版本的CRF。</p>
<p>关于CRF的实现，表面上和HMM模型基本一致，从我的角度来看，因为其观测矩阵的实现由模型给出，即 $P(Y|X)$，因此其展示的是判别模型。所以才认为其是CRF模型，<br>如果变成隐马尔可夫模型，需要建模$P(Y,X)$, 十分有趣，不同观测矩阵的给出方式决定了模型的类型。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">kmeans numpy实现</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-10 16:11:32 / 修改时间：16:24:33" itemprop="dateCreated datePublished" datetime="2022-04-10T16:11:32+08:00">2022-04-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="k-means算法"><a href="#k-means算法" class="headerlink" title="k-means算法"></a>k-means算法</h2><p>k-means作为基本的无监督机器学习算法，在一些面试场景下经常会被拉起来让做手动实现，算法本身其实不难，但在面试场景下复现的准确且简洁则十分重要，因此，本篇博文实现了k-means基本算法，希望大家都可以动手自己实现一遍以保证在需要手写的时候能够快速完成。关于k-means算法的基本原理，我相信大家都应该十分的清楚，因此在这个不多展开介绍，不懂的同学请自行百度或者Google。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">Pytorch Autograd 机制介绍</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-01 14:23:16" itemprop="dateCreated datePublished" datetime="2022-04-01T14:23:16+08:00">2022-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-02 21:20:33" itemprop="dateModified" datetime="2022-04-02T21:20:33+08:00">2022-04-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="什么是-Autograd"><a href="#什么是-Autograd" class="headerlink" title="什么是 Autograd?"></a>什么是 Autograd?</h2><p>Autograd是反向自动微分系统。从概念上讲，autograd 记录一个图结构，记录执行操作时创建数据的所有操作，一个有向无环图，其叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，可以使用链式法则自动计算梯度。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/" class="post-title-link" itemprop="url">transformer语言模型训练</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-31 18:59:27" itemprop="dateCreated datePublished" datetime="2020-07-31T18:59:27+08:00">2020-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-28 17:04:28" itemprop="dateModified" datetime="2022-03-28T17:04:28+08:00">2022-03-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText"><a href="#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText" class="headerlink" title="Sequence-to-Sequence Modeling with nn.Transformer and TorchText"></a>Sequence-to-Sequence Modeling with nn.Transformer and TorchText</h1><p>This is a tutorial on how to train a sequence-to-sequence model<br>that uses the<br><code>nn.Transformer &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&gt;</code> module.<br>PyTorch 1.2 release includes a standard transformer module based on the<br>paper <code>Attention is All You
Need &lt;https://arxiv.org/pdf/1706.03762.pdf&gt;</code><br>The transformer model<br>has been proved to be superior in quality for many sequence-to-sequence<br>problems while being more parallelizable. The <code>nn.Transformer</code> module<br>relies entirely on an attention mechanism (another module recently<br>implemented as<br><code>nn.MultiheadAttention &lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&gt;</code>)<br>to draw global dependencies between input and output. The <code>nn.Transformer</code> module is now highly modularized such that a single component (like <code>nn.TransformerEncoder &lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&gt;</code>in this tutorial) can be easily adapted/composed.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/29/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/29/Transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-29 16:13:35" itemprop="dateCreated datePublished" datetime="2020-07-29T16:13:35+08:00">2020-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-28 17:04:28" itemprop="dateModified" datetime="2022-03-28T17:04:28+08:00">2022-03-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<h4 id="注意力计算公式"><a href="#注意力计算公式" class="headerlink" title="注意力计算公式"></a>注意力计算公式</h4><script type="math/tex; mode=display">A = Softmax(Q*K^T/\sqrt{d})*V</script>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/07/29/Transformer/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tb3.jpeg">
      <meta itemprop="name" content="bool_tbb">
      <meta itemprop="description" content="关于一些在NLP领域的学习总结，或者随便写点啥。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bool_tbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">NLP学习笔记</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-22 22:06:01" itemprop="dateCreated datePublished" datetime="2020-03-22T22:06:01+08:00">2020-03-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-28 17:04:28" itemprop="dateModified" datetime="2022-03-28T17:04:28+08:00">2022-03-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="NLP中如何发掘模型的可解释性"><a href="#NLP中如何发掘模型的可解释性" class="headerlink" title="NLP中如何发掘模型的可解释性"></a>NLP中如何发掘模型的可解释性</h2><p>可解释性在AI的模型设计中十分重要。需要防止模型存在偏见和缺陷带来的伦理问题，并且帮助决策者理解如何正确地使用我们的模型。越是严苛的场景，越需要模型提供证明它们是如何运作且避免错误的证据。如实时性较强的无人驾驶领域，黑盒模型无法让人们信服其工作的安全性。</p>
<p>通常深度学习模型就像一个黑匣子，它能预测出很好的结果，但是你并不知道它为什么会预测出这样的结果。想知道它是如何工作的，那么得尝试打开这个黑匣子，解释模型的意义十分必要。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bool_tbb"
      src="/images/tb3.jpeg">
  <p class="site-author-name" itemprop="name">bool_tbb</p>
  <div class="site-description" itemprop="description">关于一些在NLP领域的学习总结，或者随便写点啥。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/iiot-tbb" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;iiot-tbb" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/bool_tbb@sjtu.edu.cn" title="E-Mail → bool_tbb@sjtu.edu.cn"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/5313762851" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;5313762851" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/16817976/bool-tbb" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;16817976&#x2F;bool-tbb" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      推荐阅读
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://developer.apple.com/swift/" title="https:&#x2F;&#x2F;developer.apple.com&#x2F;swift&#x2F;" rel="noopener" target="_blank">Swift 4</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://developer.apple.com/documentation/objectivec" title="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;objectivec" rel="noopener" target="_blank">Objective -C</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

      
      <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
      <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
      <div class="widget-wrap">
          <h3 class="widget-title">Tag Cloud</h3>
          <div id="myCanvasContainer" class="widget tagcloud">
              <canvas width="250" height="250" id="resCanvas" style="width=100%">
                  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Autograd/" rel="tag">Autograd</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog/" rel="tag">Dialog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grammar-Inuduction/" rel="tag">Grammar Inuduction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Languaage-Model/" rel="tag">Languaage Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/" rel="tag">Language Model</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PDF%E8%A7%A3%E6%9E%90/" rel="tag">PDF解析</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parameter-Efficient/" rel="tag">Parameter Efficient</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Syntax-Tree/" rel="tag">Syntax-Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crf/" rel="tag">crf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/myBorn/" rel="tag">myBorn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/" rel="tag">nlp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/" rel="tag">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E6%96%87/" rel="tag">人文</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%95%E5%BE%8B/" rel="tag">法律</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag">自然语言处理</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">1</span></li></ul>
              </canvas>
          </div>
      </div>
      
      
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bool_tbb</span>
</div>
  <div class="powered-by">由 <a href="https://github.com/iiot-tbb" class="theme-link" rel="noopener" target="_blank">Tbb</a> 强力驱动 v1.0.0
  </div>
  <span class="post-meta-divider">|</span>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共48.1k字</span>
</div>


<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("11/11/2019 12:49:25");//在此处修改你的建站时间
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = " 运行了 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>














  

  

  

  
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
