<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bool_tbb</title>
  
  <subtitle>一枚NLPer小菜鸡</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-05-24T14:53:04.110Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>bool_tbb</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Parameter Efficient Transfer Learning</title>
    <link href="http://yoursite.com/2022/05/21/Parameter-Efficient-Transfer-Learning/"/>
    <id>http://yoursite.com/2022/05/21/Parameter-Efficient-Transfer-Learning/</id>
    <published>2022-05-21T11:38:07.000Z</published>
    <updated>2022-05-24T14:53:04.110Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这两天读了一些(两篇..)关于Parameter Efficient的文章，结构并不复杂，但是感觉还是有些内容可以分享。&lt;/p&gt;
&lt;h3 id=&quot;什么是参数高效的迁移学习呢？&quot;&gt;&lt;a href=&quot;#什么是参数高效的迁移学习呢？&quot; class=&quot;headerlink&quot; title=&quot;什么是参数高效的迁移学习呢？&quot;&gt;&lt;/a&gt;什么是参数高效的迁移学习呢？&lt;/h3&gt;&lt;p&gt;在自然语言处理的任务中，目前各个任务中常用的方法普遍基于Transformer模型，更具体来说，基于以Transformer的backbone模型的大规模预训练模型,以此为基础在各种任务上进行fine-tuning。通常情况下，&lt;br&gt;会对所有的模型参数进行微调，当然，通过这样的方式，可以更容易使得模型达到比较好的效果。因此也是大多数工作所进行的方式。&lt;/p&gt;
&lt;p&gt;但是呢，这就会衍生出一个问题，首先，训练和部署效率的问题，先说训练，对于全局模型进行微调势必会消耗大量算力以及有一点点费电，那当然也可以通过frozen预训练模型在下有任务微调，这就和今天的主题有些相似了。另外呢，如果有很多很多的任务，你对每一个任务都进行fine-tuning，然后部署保存模型，对于现在动辄billion级别的模型参数来说，比如说你一个模型10GB，那你有100个任务，则需要1TB的存储量。&lt;/p&gt;
&lt;p&gt;这时候，如果我们可以插入一小部分网络结构并且只对这一小部分进行微调就能达到一个比较好的效果，岂不美哉。&lt;/p&gt;
&lt;p&gt;因此关于这部分的内容就应运而生。&lt;br&gt;
    
    </summary>
    
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="Parameter Efficient" scheme="http://yoursite.com/tags/Parameter-Efficient/"/>
    
  </entry>
  
  <entry>
    <title>Bertology:说一说那些不为人知的细节</title>
    <link href="http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/"/>
    <id>http://yoursite.com/2022/05/01/Bertology-%E8%AF%B4%E4%B8%80%E8%AF%B4%E9%82%A3%E4%BA%9B%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E7%BB%86%E8%8A%82/</id>
    <published>2022-05-01T08:16:22.000Z</published>
    <updated>2022-05-08T09:31:00.301Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;序言&quot;&gt;&lt;a href=&quot;#序言&quot; class=&quot;headerlink&quot; title=&quot;序言&quot;&gt;&lt;/a&gt;序言&lt;/h1&gt;&lt;p&gt;自 Transformer 模型的文章&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Attention is all you need》&lt;/a&gt;这篇经典发表的时刻，整个自然语言处理界发生了天翻地覆的变化，强大的注意力机制使得模型能够摆脱RNN模型的长时依赖的问题，且在各种NLP下游任务中，效果吊打RNN系列模型。随后经典名作GPT，BERT这些预训练模型的paper相继发表，从此NLP进入预训练时代。由于其对基础的语意句法等nlp的特征超强的捕获能力，基于Transformer系列的backbone预训练模型在下游任务微调成一种常用的方法。&lt;/p&gt;
&lt;p&gt;虽然每年有大量的前沿研究基于Bert这样的模型，但是很多文章—-表面是对模型进行了创新获得了不错的效果，究其本质，无非是扩大了模型的参数量，提高训练数据的质量和数量。因此，取得了更为理想的效果。关于为什么模型的效果好，以及每一层的神经网络到底是取得了如何的效果则没有系统的阐释。今天读到了一篇&lt;a href=&quot;https://watermark.silverchair.com/tacl_a_00349.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsowggLGBgkqhkiG9w0BBwagggK3MIICswIBADCCAqwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM356qe6dtueuLMY59AgEQgIICfeMIeHCgVGdSvJu8D2XfjC5ElP1yXiIVEdIbGODaHM9mO1WisSSW5owwhoZNils1hOJUETd_3bZvcHmLZBMUwDPl90BJI5W5psceyCep9D5F3qW-dy31dxzM2bWVKBUjHL7WE7vcbad4Cvrzq_AlR36jArY0mhSc5ekJJRdbTNMHtskqingzrFuLz8eR1yUpWpq8kDtceTZV1zBJCM_NVinFFwwt1eh9cn7n8PVEm4em5fj912Oo77yMTPnr_6nPORwVjl9ZuboiKWTzV0XJz3TO-PxXcUD_mg-6DdbsS9hXibOZ3oz9bVpXVGKW6XYFOs7BzHT6A6M9lDDKDtAp2R9iBqajMyhukiB1bB6VA0ePRke64wdJ17CF9xibbH8FqlDlaYhieRLmc7DTWbIFA15hsM7Aq1iwB9mYQ49eur0aCzZSZJWRrxugqSs3MuNSnjzJvClzEQwKRBToWUFfQ0FlxG0vpOfswjZz2PnDBDhOZD6VvVF0Gs1I19RJsoaMAxfI5lMNk3tSsvxcsFbRms7EOTXG6O0qlkCsMF4GfaGp6wQq9rsAmgQsGHu1wecwwRrMveNbF7dd-0crKukSCUwZOCXM0_qn5BiOOflI4LXAjJZ6T5A1kseghSFTFHLNM1no4Vxa6tdLY-njxhK50ib5Cytrunc58Kc1qwQbahB3x4UVlKm8ww14pFjnLi2e1nJJi-_YWdu1JNuI6RbhsM_GS1iSxu6KmsH49DTy0urourShCVC-23Y6KM91pry926KPk2r_gGzmoIwC0FSGZ-FCIlMzUING1XRyVlPR9P414Ifv1_hSJKldyzljyMyte-HJhKunk9uuuWOa6f8&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《A Primer in BERTology: What We Know About How BERTWorks》&lt;/a&gt;发表于Transactions of the Association for Computational Linguistics。这篇文章对于bert模型的一些细节进行了系统的论述，很全面，因此，读了一下，把它分享给大家。&lt;/p&gt;
&lt;h1 id=&quot;动机&quot;&gt;&lt;a href=&quot;#动机&quot; class=&quot;headerlink&quot; title=&quot;动机&quot;&gt;&lt;/a&gt;动机&lt;/h1&gt;&lt;p&gt;众所周知，BERT已经在nlp领域大杀四方，但是对于“why it works well”相对来说没有那么多的研究。如此一来，想要更进一步做出更加solid的研究，需要对该模型有更多的了解。 相比较一些经典的backbone，比如说卷积神经网络(CNN)，在感性上的动机则没有特别明显。我们希望对其中的一些结构进行分析，使得模型中具体所做的事情更容易被人们所认识，但是，对于模型过于大的尺寸所造成的预训练不好做（大部分研究人员没有超多的显卡，以及预训练所耗费的经费十分多）且想要做消融实验并不容易。因此对于近些年来对解释bert的文章做一个综述，方便大家对于该模型的认识。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>bilstm+crf(batch)的实现</title>
    <link href="http://yoursite.com/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2022/04/18/bilstm-crf-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-18T13:27:11.000Z</published>
    <updated>2022-04-18T13:40:45.581Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;batch版本的条件随机场&quot;&gt;&lt;a href=&quot;#batch版本的条件随机场&quot; class=&quot;headerlink&quot; title=&quot;batch版本的条件随机场&quot;&gt;&lt;/a&gt;batch版本的条件随机场&lt;/h2&gt;&lt;p&gt;在上一篇文章中我们按照了 pytorch的官方教程复现了 简单版本的 条件随机场。基本了解了其中的代码实现。但是我们可以看到上一篇中的 crf需要反复的条件循环，同时也没有支持批处理的操作，如果实际应用的话，速度应该会慢很多，因此，在这里，我们实现了Batch版本的条件随机场。to be honest, batch版本相比较 傻瓜版本的实现有一丢丢的复杂，尤其是需要考虑大量的矩阵并行的操作，十分伤脑筋。还好，我参考了&lt;a href=&quot;https://github.com/jtlin-sync/batch_bilstm_crf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;batch lstm+crf&lt;/a&gt;代码并且认真的剖析其中的细节后，基本弄清了其中的实现细节。如果你对 这部分代码感兴趣，可以参考我下面的代码跑起来学一学。在这部分代码中，一些核心地方给出了注释，但是可能仍然不够清晰，希望你可以自己画画图搞清楚这些细节。&lt;br&gt;
    
    </summary>
    
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="crf" scheme="http://yoursite.com/tags/crf/"/>
    
  </entry>
  
  <entry>
    <title>pytorch版本的bilstm+crf的实现</title>
    <link href="http://yoursite.com/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2022/04/15/pytorch%E7%89%88%E6%9C%AC%E7%9A%84bilstm-crf%E7%9A%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-14T17:12:00.000Z</published>
    <updated>2022-04-15T07:28:31.310Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CRF-代码实现&quot;&gt;&lt;a href=&quot;#CRF-代码实现&quot; class=&quot;headerlink&quot; title=&quot;CRF 代码实现&quot;&gt;&lt;/a&gt;CRF 代码实现&lt;/h2&gt;&lt;p&gt;最近没啥事情，想把CRF相关的内容再捋一捋，因此研究了Pytoch的CRF实现，代码如下展示，之后将会研究如何做batch版本的CRF。&lt;/p&gt;
&lt;p&gt;关于CRF的实现，表面上和HMM模型基本一致，从我的角度来看，因为其观测矩阵的实现由模型给出，即 $P(Y|X)$，因此其展示的是判别模型。所以才认为其是CRF模型，&lt;br&gt;如果变成隐马尔可夫模型，需要建模$P(Y,X)$, 十分有趣，不同观测矩阵的给出方式决定了模型的类型。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>kmeans numpy实现</title>
    <link href="http://yoursite.com/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2022/04/10/kmeans-numpy%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-10T08:11:32.000Z</published>
    <updated>2022-04-10T08:24:33.851Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;k-means算法&quot;&gt;&lt;a href=&quot;#k-means算法&quot; class=&quot;headerlink&quot; title=&quot;k-means算法&quot;&gt;&lt;/a&gt;k-means算法&lt;/h2&gt;&lt;p&gt;k-means作为基本的无监督机器学习算法，在一些面试场景下经常会被拉起来让做手动实现，算法本身其实不难，但在面试场景下复现的准确且简洁则十分重要，因此，本篇博文实现了k-means基本算法，希望大家都可以动手自己实现一遍以保证在需要手写的时候能够快速完成。关于k-means算法的基本原理，我相信大家都应该十分的清楚，因此在这个不多展开介绍，不懂的同学请自行百度或者Google。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="numpy" scheme="http://yoursite.com/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Autograd 机制介绍</title>
    <link href="http://yoursite.com/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2022/04/01/Pytorch-Autograd-%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/</id>
    <published>2022-04-01T06:23:16.000Z</published>
    <updated>2022-04-02T13:20:33.665Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是-Autograd&quot;&gt;&lt;a href=&quot;#什么是-Autograd&quot; class=&quot;headerlink&quot; title=&quot;什么是 Autograd?&quot;&gt;&lt;/a&gt;什么是 Autograd?&lt;/h2&gt;&lt;p&gt;Autograd是反向自动微分系统。从概念上讲，autograd 记录一个图结构，记录执行操作时创建数据的所有操作，一个有向无环图，其叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，可以使用链式法则自动计算梯度。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="Autograd" scheme="http://yoursite.com/tags/Autograd/"/>
    
  </entry>
  
  <entry>
    <title>transformer语言模型训练</title>
    <link href="http://yoursite.com/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    <id>http://yoursite.com/2020/07/31/transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</id>
    <published>2020-07-31T10:59:27.000Z</published>
    <updated>2022-03-28T09:04:28.988Z</updated>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;%matplotlib inline&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h1 id=&quot;Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText&quot;&gt;&lt;a href=&quot;#Sequence-to-Sequence-Modeling-with-nn-Transformer-and-TorchText&quot; class=&quot;headerlink&quot; title=&quot;Sequence-to-Sequence Modeling with nn.Transformer and TorchText&quot;&gt;&lt;/a&gt;Sequence-to-Sequence Modeling with nn.Transformer and TorchText&lt;/h1&gt;&lt;p&gt;This is a tutorial on how to train a sequence-to-sequence model&lt;br&gt;that uses the&lt;br&gt;&lt;code&gt;nn.Transformer &amp;lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer&amp;gt;&lt;/code&gt; module.&lt;br&gt;PyTorch 1.2 release includes a standard transformer module based on the&lt;br&gt;paper &lt;code&gt;Attention is All You
Need &amp;lt;https://arxiv.org/pdf/1706.03762.pdf&amp;gt;&lt;/code&gt;&lt;br&gt;The transformer model&lt;br&gt;has been proved to be superior in quality for many sequence-to-sequence&lt;br&gt;problems while being more parallelizable. The &lt;code&gt;nn.Transformer&lt;/code&gt; module&lt;br&gt;relies entirely on an attention mechanism (another module recently&lt;br&gt;implemented as&lt;br&gt;&lt;code&gt;nn.MultiheadAttention &amp;lt;https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention&amp;gt;&lt;/code&gt;)&lt;br&gt;to draw global dependencies between input and output. The &lt;code&gt;nn.Transformer&lt;/code&gt; module is now highly modularized such that a single component (like &lt;code&gt;nn.TransformerEncoder &amp;lt;https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&amp;gt;&lt;/code&gt;in this tutorial) can be easily adapted/composed.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="Transformer" scheme="http://yoursite.com/tags/Transformer/"/>
    
      <category term="Languaage Model" scheme="http://yoursite.com/tags/Languaage-Model/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://yoursite.com/2020/07/29/Transformer/"/>
    <id>http://yoursite.com/2020/07/29/Transformer/</id>
    <published>2020-07-29T08:13:35.000Z</published>
    <updated>2022-03-28T09:04:28.988Z</updated>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.autograd &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Variable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; math&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn.functional &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; F&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;注意力计算公式&quot;&gt;&lt;a href=&quot;#注意力计算公式&quot; class=&quot;headerlink&quot; title=&quot;注意力计算公式&quot;&gt;&lt;/a&gt;注意力计算公式&lt;/h4&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;A = Softmax(Q*K^T/\sqrt{d})*V&lt;/script&gt;
    
    </summary>
    
    
    
      <category term="Language Model" scheme="http://yoursite.com/tags/Language-Model/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习笔记</title>
    <link href="http://yoursite.com/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/03/22/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-03-22T14:06:01.000Z</published>
    <updated>2022-03-28T09:04:28.980Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;NLP中如何发掘模型的可解释性&quot;&gt;&lt;a href=&quot;#NLP中如何发掘模型的可解释性&quot; class=&quot;headerlink&quot; title=&quot;NLP中如何发掘模型的可解释性&quot;&gt;&lt;/a&gt;NLP中如何发掘模型的可解释性&lt;/h2&gt;&lt;p&gt;可解释性在AI的模型设计中十分重要。需要防止模型存在偏见和缺陷带来的伦理问题，并且帮助决策者理解如何正确地使用我们的模型。越是严苛的场景，越需要模型提供证明它们是如何运作且避免错误的证据。如实时性较强的无人驾驶领域，黑盒模型无法让人们信服其工作的安全性。&lt;/p&gt;
&lt;p&gt;通常深度学习模型就像一个黑匣子，它能预测出很好的结果，但是你并不知道它为什么会预测出这样的结果。想知道它是如何工作的，那么得尝试打开这个黑匣子，解释模型的意义十分必要。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PdfMiner文档解析</title>
    <link href="http://yoursite.com/2020/02/27/PdfMiner%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2020/02/27/PdfMiner%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90/</id>
    <published>2020-02-27T09:51:57.000Z</published>
    <updated>2022-03-28T09:04:28.984Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;PDFMiner是一个可以从PDF文档中提取信息的工具。与其他PDF相关的工具不同，它注重的完全是获取和分析文本数据。PDFMiner允许你获取某一页中文本的准确位置和一些诸如字体、行数的信息。它包括一个PDF转换器，可以把PDF文件转换成HTML等格式。它还有一个扩展的PDF解析器，可以用于除文本分析以外的其他用途。&lt;a href=&quot;https://euske.github.io/pdfminer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方主页&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其特征有：1、完全使用python编写。（适用于2.4或更新版本）2、解析，分析，并转换成PDF文档。3、PDF-1.7规范的支持。（几乎）4、中日韩语言和垂直书写脚本支持。5、各种字体类型（Type1、TrueType、Type3，和CID）的支持。6、基本加密（RC4）的支持。7、PDF与HTML转换。8、纲要（TOC）的提取。9、标签内容提取。10、通过分组文本块重建原始的布局。&lt;br&gt;如果你的Python有安装pip模块，就可以通过命令“python pip install pdfminer”，自动安装pdfminer。&lt;br&gt;
    
    </summary>
    
    
    
      <category term="PDF解析" scheme="http://yoursite.com/tags/PDF%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>全连接神经网络-FNN</title>
    <link href="http://yoursite.com/2020/02/27/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/"/>
    <id>http://yoursite.com/2020/02/27/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/</id>
    <published>2020-02-27T09:34:02.000Z</published>
    <updated>2022-03-28T09:04:28.990Z</updated>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Machinglearing Model,the first step</title>
    <link href="http://yoursite.com/2020/01/31/Machinglearing-Model-the-first-step/"/>
    <id>http://yoursite.com/2020/01/31/Machinglearing-Model-the-first-step/</id>
    <published>2020-01-31T13:01:01.000Z</published>
    <updated>2022-03-28T09:04:28.979Z</updated>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;%matplotlib inline&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;random_data = np.random.random((&lt;span class=&quot;number&quot;&gt;20&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;random_data&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AI for NLP</title>
    <link href="http://yoursite.com/2020/01/16/AI%20for%20NLP/"/>
    <id>http://yoursite.com/2020/01/16/AI%20for%20NLP/</id>
    <published>2020-01-16T12:23:35.000Z</published>
    <updated>2022-03-28T09:04:28.978Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Build-Graph&quot;&gt;&lt;a href=&quot;#Build-Graph&quot; class=&quot;headerlink&quot; title=&quot;Build Graph&quot;&gt;&lt;/a&gt;Build Graph&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;coordination_source = &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;兰州&#39;, geoCoord:[103.73, 36.03]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;嘉峪关&#39;, geoCoord:[98.17, 39.47]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;西宁&#39;, geoCoord:[101.74, 36.56]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;成都&#39;, geoCoord:[104.06, 30.67]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;石家庄&#39;, geoCoord:[114.48, 38.03]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;拉萨&#39;, geoCoord:[102.73, 25.04]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;贵阳&#39;, geoCoord:[106.71, 26.57]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;武汉&#39;, geoCoord:[114.31, 30.52]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;郑州&#39;, geoCoord:[113.65, 34.76]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;济南&#39;, geoCoord:[117, 36.65]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;南京&#39;, geoCoord:[118.78, 32.04]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;合肥&#39;, geoCoord:[117.27, 31.86]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;杭州&#39;, geoCoord:[120.19, 30.26]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;南昌&#39;, geoCoord:[115.89, 28.68]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;福州&#39;, geoCoord:[119.3, 26.08]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;广州&#39;, geoCoord:[113.23, 23.16]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;长沙&#39;, geoCoord:[113, 28.21]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;//&amp;#123;name:&#39;海口&#39;, geoCoord:[110.35, 20.02]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;沈阳&#39;, geoCoord:[123.38, 41.8]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;长春&#39;, geoCoord:[125.35, 43.88]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;哈尔滨&#39;, geoCoord:[126.63, 45.75]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;太原&#39;, geoCoord:[112.53, 37.87]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;西安&#39;, geoCoord:[108.95, 34.27]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;//&amp;#123;name:&#39;台湾&#39;, geoCoord:[121.30, 25.03]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;北京&#39;, geoCoord:[116.46, 39.92]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;上海&#39;, geoCoord:[121.48, 31.22]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;重庆&#39;, geoCoord:[106.54, 29.59]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;天津&#39;, geoCoord:[117.2, 39.13]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;呼和浩特&#39;, geoCoord:[111.65, 40.82]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;南宁&#39;, geoCoord:[108.33, 22.84]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;//&amp;#123;name:&#39;西藏&#39;, geoCoord:[91.11, 29.97]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;银川&#39;, geoCoord:[106.27, 38.47]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;乌鲁木齐&#39;, geoCoord:[87.68, 43.77]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;香港&#39;, geoCoord:[114.17, 22.28]&amp;#125;,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&amp;#123;name:&#39;澳门&#39;, geoCoord:[113.54, 22.19]&amp;#125;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;re.findall(&lt;span class=&quot;string&quot;&gt;&quot;[\d\.]+&quot;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&quot;&amp;#123;name:&#39;澳门&#39;, geoCoord:[113.54, 22.19]&amp;#125;&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;113.54&amp;#39;, &amp;#39;22.19&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Get-data-from-source-using-regular-expression&quot;&gt;&lt;a href=&quot;#Get-data-from-source-using-regular-expression&quot; class=&quot;headerlink&quot; title=&quot;Get data from source using regular expression&quot;&gt;&lt;/a&gt;Get data from source using regular expression&lt;/h3&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Syntax-Tree</title>
    <link href="http://yoursite.com/2020/01/11/Syntax-Tree/"/>
    <id>http://yoursite.com/2020/01/11/Syntax-Tree/</id>
    <published>2020-01-11T11:46:35.000Z</published>
    <updated>2022-03-28T09:04:28.987Z</updated>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;simple_grammar =&lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;sentence =&amp;gt; noun_phrase verb_phrase&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;noun_phrase =&amp;gt; Article Adj* noun&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;Adj* =&amp;gt; null | Adj Adj*&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;verb_phrase =&amp;gt; verb noun_phrase&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;Article =&amp;gt; 一个 | 这个&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;noun =&amp;gt; 女人 | 篮球 | 桌子 | 小猫&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;verb =&amp;gt; 看着 | 坐着 | 听见 | 看着&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;Adj =&amp;gt; 蓝色的 | 好看的 | 小小的&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;another_grammar = &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;#&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; random&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;adj&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; random.choice(&lt;span class=&quot;string&quot;&gt;&#39;蓝色的|好看的|小小的&#39;&lt;/span&gt;.split(&lt;span class=&quot;string&quot;&gt;&#39;|&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;adj_star&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; random.choice([&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;,&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; : adj()+adj_star()])()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;adj_star()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;蓝色的好看的小小的好看的小小的蓝色的&amp;#39;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="Language Model" scheme="http://yoursite.com/tags/Language-Model/"/>
    
      <category term="Syntax-Tree" scheme="http://yoursite.com/tags/Syntax-Tree/"/>
    
  </entry>
  
  <entry>
    <title>python BinarySearchTree</title>
    <link href="http://yoursite.com/2019/11/29/python-BinarySearchTree/"/>
    <id>http://yoursite.com/2019/11/29/python-BinarySearchTree/</id>
    <published>2019-11-29T13:04:20.000Z</published>
    <updated>2022-03-28T09:04:28.988Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;python&quot;&gt;&lt;a href=&quot;#python&quot; class=&quot;headerlink&quot; title=&quot;python&quot;&gt;&lt;/a&gt;python&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;最近在学python，断断续续的，感觉学的慢，就顺便写写代码，加强对python的感觉。&lt;br&gt;虽然学了半天还是啥都不会，但是还是要写写博客啥的，激励自己不老懒惰，不能因为眼前的&lt;/p&gt;
&lt;h2 id=&quot;困难而放弃进步。所以，下面我要贴代码了…&quot;&gt;&lt;a href=&quot;#困难而放弃进步。所以，下面我要贴代码了…&quot; class=&quot;headerlink&quot; title=&quot;困难而放弃进步。所以，下面我要贴代码了….&quot;&gt;&lt;/a&gt;困难而放弃进步。所以，下面我要贴代码了….&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="二叉树" scheme="http://yoursite.com/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>Hello World ~</title>
    <link href="http://yoursite.com/2019/11/11/hello-world/"/>
    <id>http://yoursite.com/2019/11/11/hello-world/</id>
    <published>2019-11-11T13:04:20.000Z</published>
    <updated>2022-03-28T09:04:28.988Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to  my own web,you can find me on github &lt;a href=&quot;https://github.com/iiot-tbb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;iiot-tbb&lt;/a&gt;! This is my very first post.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="myBorn" scheme="http://yoursite.com/tags/myBorn/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>论法的精神（节选）</title>
    <link href="http://yoursite.com/2019/11/11/%E8%AE%BA%E6%B3%95%E7%9A%84%E7%B2%BE%E7%A5%9E%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/"/>
    <id>http://yoursite.com/2019/11/11/%E8%AE%BA%E6%B3%95%E7%9A%84%E7%B2%BE%E7%A5%9E%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/</id>
    <published>2019-11-11T08:21:35.000Z</published>
    <updated>2022-03-28T09:04:28.990Z</updated>
    
    <summary type="html">
    
      &lt;h3 id=&quot;法与一切存在物的关系&quot;&gt;&lt;a href=&quot;#法与一切存在物的关系&quot; class=&quot;headerlink&quot; title=&quot;法与一切存在物的关系&quot;&gt;&lt;/a&gt;法与一切存在物的关系&lt;/h3&gt;&lt;p&gt;从最大限度的广义上说，法是源于客观事物性质的必然关系。从这个意义上推断，所有的&lt;/p&gt;
&lt;p&gt;存在物都有属于自己的法；上帝有他的法；物质世界也有它的法；高于人类的“先知&lt;/p&gt;
&lt;p&gt;圣人们”有着他们的法；畜类也有自己的法；人类拥有他们的法。&lt;/p&gt;
&lt;p&gt;有些人说，世间我们看到的万物都是由一个盲目的命运所创造的，这种说法荒谬绝伦。因&lt;/p&gt;
&lt;p&gt;为盲目的命运竟然创造“具有智能的创造物”，岂不是一件更为荒谬的事吗?&lt;/p&gt;
&lt;p&gt;于是便有了一个最浅显的理性的存在。法就是这个浅显理性与各种存在物之间关系的总&lt;/p&gt;
&lt;p&gt;和，同时也体现着所有客观存在物彼此之间的关系。&lt;/p&gt;
&lt;p&gt;上帝与宇宙的关系体现在，它既是宇宙的创造者又是它的保管者：以此产生的规律，便是&lt;/p&gt;
&lt;p&gt;保管时参照的规律。上帝遵循这些规律行事，因为他熟知这些规范；之所以他熟知这些规&lt;/p&gt;
&lt;p&gt;范，因为正是他制定了这些规范；他之所以制定这些规范，因为这些规律与他的才智和能&lt;br&gt;量有着密不可分的关系。&lt;/p&gt;
&lt;p&gt;如同我们看到的一样，我们所在的世界是由物质的运动而构成的，它在一个非智能的状态&lt;/p&gt;
&lt;p&gt;中永恒地生存着。它的物质运动必然具有某种固定的规律。如果人们能够在自己所处的世&lt;/p&gt;
&lt;p&gt;界之外再臆想出另一个世界的话，那个世界要么具有恒定的规律可循，要么便是毁灭。&lt;/p&gt;
&lt;p&gt;创造本身似乎是某种随意的行为，然而其中必定蕴涵着恒定的规律，就如同无神论者的命&lt;/p&gt;
&lt;p&gt;运一般。如果造世主没有这些规范就能统管世界的话，那么，这显然是荒谬的说法，因为&lt;/p&gt;
&lt;p&gt;没有这些规范，世界将无法生存。&lt;br&gt;
    
    </summary>
    
    
    
      <category term="人文" scheme="http://yoursite.com/tags/%E4%BA%BA%E6%96%87/"/>
    
      <category term="法律" scheme="http://yoursite.com/tags/%E6%B3%95%E5%BE%8B/"/>
    
      <category term="随笔" scheme="http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
